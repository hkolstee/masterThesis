{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST of custom SAC agent on easy environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hkolstee/.local/lib/python3.10/site-packages/torch/cuda/__init__.py:138: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11040). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# from custom_agent.CTCE.sac_agent import Agent\n",
    "from custom_agents.CTCE_algorithms.single_agent_SAC_continuous import SAC\n",
    "\n",
    "env = gym.make(\"Pendulum-v1\")\n",
    "\n",
    "sac_agent = SAC(env, batch_size=256, buffer_max_size=10000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100]) torch.Size([1]) torch.Size([100])\n",
      "tensor(-1.)\n",
      "torch.Size([100]) torch.Size([1]) torch.Size([100])\n",
      "tensor(-1.)\n",
      "torch.Size([100]) torch.Size([1]) torch.Size([100])\n",
      "tensor(-1.)\n",
      "torch.Size([100]) torch.Size([1]) torch.Size([100])\n",
      "tensor(-1.)\n",
      "torch.Size([100]) torch.Size([1]) torch.Size([100])\n",
      "tensor(-1.)\n",
      "torch.Size([100]) torch.Size([1]) torch.Size([100])\n",
      "tensor(-1.)\n",
      "torch.Size([100]) torch.Size([1]) torch.Size([100])\n",
      "tensor(-1.)\n",
      "torch.Size([100]) torch.Size([1]) torch.Size([100])\n",
      "tensor(-1.)\n",
      "torch.Size([100]) torch.Size([1]) torch.Size([100])\n",
      "tensor(-1.)\n",
      "torch.Size([100]) torch.Size([1]) torch.Size([100])\n",
      "tensor(-1.)\n",
      "torch.Size([100]) torch.Size([1]) torch.Size([100])\n",
      "tensor(-1.)\n",
      "torch.Size([100]) torch.Size([1]) torch.Size([100])\n",
      "tensor(-1.)\n",
      "torch.Size([100]) torch.Size([1]) torch.Size([100])\n",
      "tensor(-1.)\n",
      "torch.Size([100]) torch.Size([1]) torch.Size([100])\n",
      "tensor(-1.)\n",
      "torch.Size([100]) torch.Size([1]) torch.Size([100])\n",
      "tensor(-1.)\n",
      "torch.Size([100]) torch.Size([1]) torch.Size([100])\n",
      "tensor(-1.)\n",
      "torch.Size([100]) torch.Size([1]) torch.Size([100])\n",
      "tensor(-1.)\n",
      "torch.Size([100]) torch.Size([1]) torch.Size([100])\n",
      "tensor(-1.)\n",
      "torch.Size([100]) torch.Size([1]) torch.Size([100])\n",
      "tensor(-1.)\n",
      "torch.Size([100]) torch.Size([1]) torch.Size([100])\n",
      "tensor(-1.)\n",
      "torch.Size([100]) torch.Size([1]) torch.Size([100])\n",
      "tensor(-1.)\n",
      "torch.Size([100]) torch.Size([1]) torch.Size([100])\n",
      "tensor(-1.)\n",
      "torch.Size([100]) torch.Size([1]) torch.Size([100])\n",
      "tensor(-1.)\n",
      "torch.Size([100]) torch.Size([1]) torch.Size([100])\n",
      "tensor(-1.)\n",
      "torch.Size([100]) torch.Size([1]) torch.Size([100])\n",
      "tensor(-1.)\n",
      "torch.Size([100]) torch.Size([1]) torch.Size([100])\n",
      "tensor(-1.)\n",
      "torch.Size([100]) torch.Size([1]) torch.Size([100])\n",
      "tensor(-1.)\n",
      "torch.Size([100]) torch.Size([1]) torch.Size([100])\n",
      "tensor(-1.)\n",
      "torch.Size([100]) torch.Size([1]) torch.Size([100])\n",
      "tensor(-1.)\n",
      "torch.Size([100]) torch.Size([1]) torch.Size([100])\n",
      "tensor(-1.)\n",
      "torch.Size([100]) torch.Size([1]) torch.Size([100])\n",
      "tensor(-1.)\n",
      "torch.Size([100]) torch.Size([1]) torch.Size([100])\n",
      "tensor(-1.)\n",
      "torch.Size([100]) torch.Size([1]) torch.Size([100])\n",
      "tensor(-1.)\n",
      "torch.Size([100]) torch.Size([1]) torch.Size([100])\n",
      "tensor(-1.)\n",
      "torch.Size([100]) torch.Size([1]) torch.Size([100])\n",
      "tensor(-1.)\n",
      "torch.Size([100]) torch.Size([1]) torch.Size([100])\n",
      "tensor(-1.)\n",
      "torch.Size([100]) torch.Size([1]) torch.Size([100])\n",
      "tensor(-1.)\n",
      "torch.Size([100]) torch.Size([1]) torch.Size([100])\n",
      "tensor(-1.)\n",
      "torch.Size([100]) torch.Size([1]) torch.Size([100])\n",
      "tensor(-1.)\n",
      "torch.Size([100]) torch.Size([1]) torch.Size([100])\n",
      "tensor(-1.)\n",
      "torch.Size([100]) torch.Size([1]) torch.Size([100])\n",
      "tensor(-1.)\n",
      "torch.Size([100]) torch.Size([1]) torch.Size([100])\n",
      "tensor(-1.)\n",
      "torch.Size([100]) torch.Size([1]) torch.Size([100])\n",
      "tensor(-1.)\n",
      "torch.Size([100]) torch.Size([1]) torch.Size([100])\n",
      "tensor(-1.)\n",
      "torch.Size([100]) torch.Size([1]) torch.Size([100])\n",
      "tensor(-1.)\n",
      "torch.Size([100]) torch.Size([1]) torch.Size([100])\n",
      "tensor(-1.)\n",
      "torch.Size([100]) torch.Size([1]) torch.Size([100])\n",
      "tensor(-1.)\n",
      "torch.Size([100]) torch.Size([1]) torch.Size([100])\n",
      "tensor(-1.)\n",
      "torch.Size([100]) torch.Size([1]) torch.Size([100])\n",
      "tensor(-1.)\n",
      "torch.Size([100]) torch.Size([1]) torch.Size([100])\n",
      "tensor(-1.)\n",
      "torch.Size([100]) torch.Size([1]) torch.Size([100])\n",
      "tensor(-1.)\n",
      "torch.Size([100]) torch.Size([1]) torch.Size([100])\n",
      "tensor(-1.)\n",
      "torch.Size([100]) torch.Size([1]) torch.Size([100])\n",
      "tensor(-1.)\n",
      "torch.Size([100]) torch.Size([1]) torch.Size([100])\n",
      "tensor(-1.)\n",
      "torch.Size([100]) torch.Size([1]) torch.Size([100])\n",
      "tensor(-1.)\n",
      "torch.Size([100]) torch.Size([1]) torch.Size([100])\n",
      "tensor(-1.)\n",
      "torch.Size([100]) torch.Size([1]) torch.Size([100])\n",
      "tensor(-1.)\n",
      "torch.Size([100]) torch.Size([1]) torch.Size([100])\n",
      "tensor(-1.)\n",
      "torch.Size([100]) torch.Size([1]) torch.Size([100])\n",
      "tensor(-1.)\n",
      "torch.Size([100]) torch.Size([1]) torch.Size([100])\n",
      "tensor(-1.)\n",
      "torch.Size([100]) torch.Size([1]) torch.Size([100])\n",
      "tensor(-1.)\n",
      "torch.Size([100]) torch.Size([1]) torch.Size([100])\n",
      "tensor(-1.)\n",
      "torch.Size([100]) torch.Size([1]) torch.Size([100])\n",
      "tensor(-1.)\n",
      "torch.Size([100]) torch.Size([1]) torch.Size([100])\n",
      "tensor(-1.)\n",
      "torch.Size([100]) torch.Size([1]) torch.Size([100])\n",
      "tensor(-1.)\n",
      "torch.Size([100]) torch.Size([1]) torch.Size([100])\n",
      "tensor(-1.)\n",
      "torch.Size([100]) torch.Size([1]) torch.Size([100])\n",
      "tensor(-1.)\n",
      "torch.Size([100]) torch.Size([1]) torch.Size([100])\n",
      "tensor(-1.)\n",
      "torch.Size([100]) torch.Size([1]) torch.Size([100])\n",
      "tensor(-1.)\n",
      "torch.Size([100]) torch.Size([1]) torch.Size([100])\n",
      "tensor(-1.)\n",
      "torch.Size([100]) torch.Size([1]) torch.Size([100])\n",
      "tensor(-1.)\n",
      "torch.Size([100]) torch.Size([1]) torch.Size([100])\n",
      "tensor(-1.)\n",
      "torch.Size([100]) torch.Size([1]) torch.Size([100])\n",
      "tensor(-1.)\n",
      "torch.Size([100]) torch.Size([1]) torch.Size([100])\n",
      "tensor(-1.)\n",
      "torch.Size([100]) torch.Size([1]) torch.Size([100])\n",
      "tensor(-1.)\n",
      "torch.Size([100]) torch.Size([1]) torch.Size([100])\n",
      "tensor(-1.)\n",
      "torch.Size([100]) torch.Size([1]) torch.Size([100])\n",
      "tensor(-1.)\n",
      "torch.Size([100]) torch.Size([1]) torch.Size([100])\n",
      "tensor(-1.)\n",
      "torch.Size([100]) torch.Size([1]) torch.Size([100])\n",
      "tensor(-1.)\n",
      "torch.Size([100]) torch.Size([1]) torch.Size([100])\n",
      "tensor(-1.)\n",
      "torch.Size([100]) torch.Size([1]) torch.Size([100])\n",
      "tensor(-1.)\n",
      "torch.Size([100]) torch.Size([1]) torch.Size([100])\n",
      "tensor(-1.)\n",
      "torch.Size([100]) torch.Size([1]) torch.Size([100])\n",
      "tensor(-1.)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m sac_agent \u001b[38;5;241m=\u001b[39m SAC(env, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, buffer_max_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100000\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# training run\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[43msac_agent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnr_steps\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m15000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarmup_steps\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearn_delay\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearn_freq\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearn_weight\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/masterThesis/masterThesis/custom_agents/core/SAC_single_agent_base.py:312\u001b[0m, in \u001b[0;36mSoftActorCriticCore.train\u001b[0;34m(self, nr_steps, max_episode_len, warmup_steps, learn_delay, learn_freq, learn_weight, checkpoint, save_dir)\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m step \u001b[38;5;241m>\u001b[39m learn_delay \u001b[38;5;129;01mand\u001b[39;00m step \u001b[38;5;241m%\u001b[39m learn_freq \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    310\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(learn_weight):\n\u001b[1;32m    311\u001b[0m         \u001b[38;5;66;03m# learning step\u001b[39;00m\n\u001b[0;32m--> 312\u001b[0m         status, loss_actor, loss_critic, alpha, loss_alpha \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    314\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m status:\n\u001b[1;32m    315\u001b[0m             \u001b[38;5;66;03m# keep track for logs\u001b[39;00m\n\u001b[1;32m    316\u001b[0m             ep_learn_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/masterThesis/masterThesis/custom_agents/core/SAC_single_agent_base.py:195\u001b[0m, in \u001b[0;36mSoftActorCriticCore.learn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m    194\u001b[0m loss_policy\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m--> 195\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# unfreeze critic gradient calculation to save computation\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munfreeze_network_grads(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcritic1)        \n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/optimizer.py:373\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    369\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    370\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    371\u001b[0m             )\n\u001b[0;32m--> 373\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    376\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/adam.py:163\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    152\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    155\u001b[0m         group,\n\u001b[1;32m    156\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    160\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    161\u001b[0m         state_steps)\n\u001b[0;32m--> 163\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/adam.py:311\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    309\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 311\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    324\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[43m     \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    326\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[43m     \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/adam.py:434\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    431\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    432\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (exp_avg_sq\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[0;32m--> 434\u001b[0m     \u001b[43mparam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maddcdiv_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexp_avg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdenom\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mstep_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    436\u001b[0m \u001b[38;5;66;03m# Lastly, switch back to complex view\u001b[39;00m\n\u001b[1;32m    437\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m amsgrad \u001b[38;5;129;01mand\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mis_complex(params[i]):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "logs_list = []\n",
    "\n",
    "for i in range(1):\n",
    "    # make agent\n",
    "    sac_agent = SAC(env, batch_size=100, buffer_max_size=100000)\n",
    "    \n",
    "    # training run\n",
    "    sac_agent.train(nr_steps = 15000, warmup_steps = 1000, learn_delay = 100, learn_freq = 1, learn_weight = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video /home/hkolstee/masterThesis/masterThesis/videos/custom_SAC_pendulum-episode-0.mp4.\n",
      "Moviepy - Writing video /home/hkolstee/masterThesis/masterThesis/videos/custom_SAC_pendulum-episode-0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /home/hkolstee/masterThesis/masterThesis/videos/custom_SAC_pendulum-episode-0.mp4\n"
     ]
    }
   ],
   "source": [
    "from gymnasium.utils.save_video import save_video\n",
    "env2 = gym.make(\"Pendulum-v1\", g=9.81, render_mode = \"rgb_array_list\")\n",
    "\n",
    "# test loop\n",
    "obs, info = env2.reset()\n",
    "done = 0\n",
    "truncated = 0\n",
    "for _ in range(100):\n",
    "    action = sac_agent.get_action(obs, False, True)\n",
    "\n",
    "    next_obs, reward, done, truncated, info = env2.step(action)\n",
    "\n",
    "    obs = next_obs\n",
    "\n",
    "save_video(env2.render(), \"videos\", name_prefix = \"custom_SAC_pendulum\", fps=env.metadata[\"render_fps\"], step_starting_index=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Somewhat more difficult env (still not very difficult): Half Cheetah V4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from custom_agent.CTCE.sac_agent_AE import Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"HalfCheetah-v4\")\n",
    "print(env.observation_space)\n",
    "print(env.observation_space.shape)\n",
    "print(env.action_space)\n",
    "print(env.action_space.high)\n",
    "print(env.action_space.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs_list = []\n",
    "\n",
    "for i in range(3):\n",
    "    # make agent\n",
    "    sac_agent = Agent(env, batch_size=100, buffer_max_size=1000000)\n",
    "    \n",
    "    # training run\n",
    "    logs = sac_agent.train(200000, warmup_steps=10000, learn_delay = 5000, learn_freq = 1, learn_weight = 1)\n",
    "    \n",
    "    # save training logs for this run\n",
    "    logs_list.append(logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs = pd.concat(logs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(logs[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_delay = 5000\n",
    "\n",
    "plt.figure()\n",
    "sns.set(style=\"darkgrid\", palette=\"rainbow\")\n",
    "plot = sns.lineplot(data = logs, x = \"step\", y = \"avg_reward\")\n",
    "plot.set(title=\"SAC for HalfCheetah-v4\", xlabel = \"step\", ylabel = \"Episode Mean Reward\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "sns.set(style=\"darkgrid\", palette=\"rainbow\")\n",
    "plot = sns.lineplot(data = logs[logs[\"step\"] > learn_delay], x = \"step\", y = \"avg_actor_loss\")\n",
    "plot.set(title=\"Actor Loss\", xlabel = \"Step\", ylabel = \"Loss\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "sns.set(style=\"darkgrid\", palette=\"rainbow\")\n",
    "plot = sns.lineplot(data = logs[logs[\"step\"] > learn_delay], x = \"step\", y = \"avg_critic_loss\")\n",
    "plot.set(title=\"Critic Loss\", xlabel = \"Step\", ylabel = \"Loss\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "sns.set(style=\"darkgrid\", palette=\"rainbow\")\n",
    "plot = sns.lineplot(data = logs[logs[\"step\"] > learn_delay], x = \"step\", y = \"avg_policy_entr\")\n",
    "plot.set(title=\"Policy Entropy\", xlabel = \"Step\", ylabel = \"Entropy\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "sns.set(style=\"darkgrid\", context=\"notebook\", palette=\"rainbow\")\n",
    "sns.lineplot(data = logs[logs[\"step\"] > learn_delay], x = \"step\", y = \"avg_alpha_loss\").set(title=\"Entropy Coefficient (alpha) Loss\", xlabel = \"Step\", ylabel = \"Loss\")\n",
    "\n",
    "plt.figure()\n",
    "sns.set(style=\"darkgrid\", context=\"notebook\", palette=\"rainbow\")\n",
    "sns.lineplot(data = logs[logs[\"step\"] > learn_delay], x = \"step\", y = \"avg_alpha\").set(title=\"Entropy Coefficient (alpha)\", xlabel = \"Step\", ylabel = \"Value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gymnasium.utils.save_video import save_video\n",
    "\n",
    "env2 = gym.make(\"HalfCheetah-v4\", render_mode = \"rgb_array_list\")\n",
    "\n",
    "# test loop\n",
    "obs, info = env2.reset()\n",
    "done = 0\n",
    "truncated = 0\n",
    "while not (done or truncated):\n",
    "    action = sac_agent.get_action(obs, False, True)\n",
    "\n",
    "    next_obs, reward, done, truncated, info = env2.step(action)\n",
    "\n",
    "    obs = next_obs\n",
    "\n",
    "save_video(env2.render(), \"videos\", name_prefix = \"custom_SAC_halfcheetahv4\", fps=env.metadata[\"render_fps\"], step_starting_index=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ant env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from custom_agent.CTCE.sac_agent_AE import Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Ant-v4\")\n",
    "print(env.observation_space)\n",
    "print(env.observation_space.shape)\n",
    "print(env.action_space)\n",
    "print(env.action_space.high)\n",
    "print(env.action_space.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs_list = []\n",
    "learn_delay = 5000\n",
    "\n",
    "for i in range(1):\n",
    "    # make agent\n",
    "    sac_agent = Agent(env, batch_size=256, buffer_max_size=1000000)\n",
    "    \n",
    "    # training run\n",
    "    logs = sac_agent.train(1000000, warmup_steps=10000, learn_delay = learn_delay, learn_freq = 1, learn_weight = 1)\n",
    "    0\n",
    "    # save training logs for this run\n",
    "    logs_list.append(logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs = pd.concat(logs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(logs[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_delay = 5000\n",
    "\n",
    "plt.figure()\n",
    "sns.set(style=\"darkgrid\", palette=\"rainbow\")\n",
    "plot = sns.lineplot(data = logs, x = \"step\", y = \"avg_reward\")\n",
    "plot.set(title=\"SAC for Ant-v4\", xlabel = \"step\", ylabel = \"Episode Mean Reward\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "sns.set(style=\"darkgrid\", palette=\"rainbow\")\n",
    "plot = sns.lineplot(data = logs[logs[\"step\"] > learn_delay], x = \"step\", y = \"avg_actor_loss\")\n",
    "plot.set(title=\"Actor Loss\", xlabel = \"Step\", ylabel = \"Loss\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "sns.set(style=\"darkgrid\", palette=\"rainbow\")\n",
    "plot = sns.lineplot(data = logs[logs[\"step\"] > learn_delay], x = \"step\", y = \"avg_critic_loss\")\n",
    "plot.set(title=\"Critic Loss\", xlabel = \"Step\", ylabel = \"Loss\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "sns.set(style=\"darkgrid\", palette=\"rainbow\")\n",
    "plot = sns.lineplot(data = logs[logs[\"step\"] > learn_delay], x = \"step\", y = \"avg_policy_entr\")\n",
    "plot.set(title=\"Policy Entropy\", xlabel = \"Step\", ylabel = \"Entropy\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "sns.set(style=\"darkgrid\", context=\"notebook\", palette=\"rainbow\")\n",
    "sns.lineplot(data = logs[logs[\"step\"] > learn_delay], x = \"step\", y = \"avg_alpha_loss\").set(title=\"Entropy Coefficient (alpha) Loss\", xlabel = \"Step\", ylabel = \"Loss\")\n",
    "\n",
    "plt.figure()\n",
    "sns.set(style=\"darkgrid\", context=\"notebook\", palette=\"rainbow\")\n",
    "sns.lineplot(data = logs[logs[\"step\"] > learn_delay], x = \"step\", y = \"avg_alpha\").set(title=\"Entropy Coefficient (alpha)\", xlabel = \"Step\", ylabel = \"Value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gymnasium.utils.save_video import save_video\n",
    "\n",
    "env2 = gym.make(\"Ant-v4\", render_mode = \"rgb_array_list\")\n",
    "\n",
    "# test loop\n",
    "obs, info = env2.reset()\n",
    "done = 0\n",
    "truncated = 0\n",
    "while not (done or truncated):\n",
    "    action = sac_agent.get_action(obs, False, True)\n",
    "\n",
    "    next_obs, reward, done, truncated, info = env2.step(action)\n",
    "\n",
    "    obs = next_obs\n",
    "\n",
    "save_video(env2.render(), \"videos\", name_prefix = \"antV4\", fps=env.metadata[\"render_fps\"], step_starting_index=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finally: CityLearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from custom_agent.CTCE.sac_agent import Agent\n",
    "from custom_agent.CTCE.citylearn_wrapper import CityLearnWrapper\n",
    "\n",
    "from citylearn.wrappers import NormalizedSpaceWrapper, NormalizedObservationWrapper\n",
    "from custom_reward.custom_reward import CustomReward\n",
    "\n",
    "from citylearn.citylearn import CityLearnEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WrapperEnv:\n",
    "    \"\"\"\n",
    "    Env to wrap provide Citylearn Env data without providing full env\n",
    "    Preventing attribute access outside of the available functions\n",
    "    \"\"\"\n",
    "    def __init__(self, env_data):\n",
    "        self.observation_names = env_data['observation_names']\n",
    "        self.action_names = env_data['action_names']\n",
    "        self.observation_space = env_data['observation_space']\n",
    "        self.action_space = env_data['action_space']\n",
    "        self.time_steps = env_data['time_steps']\n",
    "        self.seconds_per_time_step = env_data['seconds_per_time_step']\n",
    "        self.random_seed = env_data['random_seed']\n",
    "        self.buildings_metadata = env_data['buildings_metadata']\n",
    "        self.episode_tracker = env_data['episode_tracker']\n",
    "    \n",
    "    def get_metadata(self):\n",
    "        return {'buildings': self.buildings_metadata}\n",
    "    \n",
    "def makeEnv(schema_path, reward_function):\n",
    "    # create environment\n",
    "    env = CityLearnEnv(schema = schema_path, reward_function = reward_function, central_agent=True)\n",
    "\n",
    "    env_data = dict(\n",
    "        observation_names = env.observation_names,\n",
    "        action_names = env.action_names,\n",
    "        observation_space = env.observation_space,\n",
    "        action_space = env.action_space,\n",
    "        time_steps = env.time_steps,\n",
    "        random_seed = None,\n",
    "        episode_tracker = None,\n",
    "        seconds_per_time_step = None,\n",
    "        buildings_metadata = env.get_metadata()['buildings']\n",
    "    )\n",
    "\n",
    "    wrapper_env = WrapperEnv(env_data)\n",
    "    return env, wrapper_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_path = \"data/schema.json\"\n",
    "\n",
    "env, wrapper_env = makeEnv(schema_path, CustomReward)\n",
    "print(env.action_space)\n",
    "# env = NormalizedObservationWrapper(env)\n",
    "env = NormalizedSpaceWrapper(env)\n",
    "env = CityLearnWrapper(env)\n",
    "print(env.reset())\n",
    "\n",
    "print(isinstance(env.unwrapped.reward_function, CustomReward))\n",
    "print(env.unwrapped.reward_function.comfort[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs_list = []\n",
    "learn_delay = 100\n",
    "\n",
    "for i in range(1):\n",
    "    # make agent\n",
    "    sac_agent = Agent(env, batch_size=256, buffer_max_size=100000)\n",
    "    # sac_agent = SAC(\"MlpPolicy\", env)\n",
    "    \n",
    "    # training run\n",
    "    sac_agent.train(3950000, warmup_steps=100, learn_delay = 100, learn_freq = 1, learn_weight = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gymnasium.utils.save_video import save_video\n",
    "\n",
    "# test loop\n",
    "obs, info = env2.reset()\n",
    "done = 0\n",
    "truncated = 0\n",
    "while not (done or truncated):\n",
    "    action = sac_agent.get_action(obs, False, True)\n",
    "\n",
    "    next_obs, reward, done, truncated, info = env2.step(action)\n",
    "\n",
    "    obs = next_obs\n",
    "\n",
    "save_video(env2.render(), \"videos\", name_prefix = \"custom_SAC_halfcheetahv4\", fps=env.metadata[\"render_fps\"], step_starting_index=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
