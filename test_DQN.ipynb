{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shared_DQN import IndependentDQN\n",
    "from spider_fly_env.envs.grid_MA_pettingzoo_testing import SpiderFlyEnvMA\n",
    "from spider_fly_env.wrappers.pettingzoo_wrapper import PettingZooWrapper\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DQN (Shared parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = SpiderFlyEnvMA(render_mode = \"ascii\")\n",
    "env = PettingZooWrapper(env)\n",
    "\n",
    "env.observation_space[0].sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = SpiderFlyEnvMA(max_steps = 200)\n",
    "env = PettingZooWrapper(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDQN = IndependentDQN(env, eps_steps = 50000, layer_sizes = (64, 64), tau = 0.0025, buffer_max_size = 50000) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards, losses = IDQN.train(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = np.vstack(rewards)\n",
    "data2 = np.vstack(losses)\n",
    "\n",
    "df1 = pd.DataFrame(data1, columns = [\"agent_1\", \"agent_2\"])\n",
    "df1[\"Episode\"] = list(range(data1.shape[0]))\n",
    "\n",
    "df2 = pd.DataFrame(data2, columns = [\"agent_1\", \"agent_2\"])\n",
    "df2[\"Episode\"] = list(range(data1.shape[0]))\n",
    "\n",
    "df1 = df1.melt('Episode', var_name='Agent', value_name='Rewards')\n",
    "df2 = df2.melt('Episode', var_name='Agent', value_name='Rewards')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need 1.5.0 for rolling average of next step\n",
    "pd.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df1)\n",
    "display(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1[\"Avg_Reward\"] = df1[\"Rewards\"].rolling(window = 5, step = 5).mean()\n",
    "df1 = df1[df1.Episode > 5]\n",
    "df1.dropna()\n",
    "\n",
    "df2[\"Avg_Loss\"] = df2[\"Rewards\"].rolling(window = 5, step = 5).mean()\n",
    "df2 = df2[df2.Episode > 5]\n",
    "df2.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.title(\"Rewards\")\n",
    "sns.lineplot(data = df1, x = \"Episode\", y = \"Avg_Reward\")\n",
    "plt.title(\"Rewards\")\n",
    "plt.figure()\n",
    "sns.lineplot(data = df1, x = \"Episode\", y = \"Avg_Reward\", hue = \"Agent\")\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Losses\")\n",
    "sns.lineplot(data = df2, x = \"Episode\", y = \"Avg_Loss\")\n",
    "plt.figure()\n",
    "plt.title(\"Losses\")\n",
    "sns.lineplot(data = df2, x = \"Episode\", y = \"Avg_Loss\", hue = \"Agent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sequential Q-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tabular Q-learning:\n",
    "$$\n",
    "\\begin{align*}\n",
    "    & s = env.reset()\\\\\n",
    "    &\\text{while not } done:\\\\\n",
    "    & \\quad\\quad \\text{for } i \\text{ in } (1, \\ldots, m):\\\\\n",
    "    & \\quad\\quad\\quad\\quad a_i = \\argmax_{a_i} Q_i(s_i, a_i)\\\\\n",
    "    & \\quad\\quad s', r, d = env.step(a_1, \\ldots, a_m)\\\\\n",
    "    & \\quad\\quad \\text{for } i \\text{ in } (1, \\ldots, m):\\\\\n",
    "    & \\quad\\quad\\quad\\quad Q_i(s, a_i) = Q_i(s, a_i) + lr * ((mean(r) + \\gamma * \\max_{a'_i} Q_i(s', a'_i)) - Q_i(s, a_i))\\\\\n",
    "\\end{align*}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sequential Tabular Q-learning:\n",
    "$$\n",
    "\\begin{align*}\n",
    "    & s = env.reset()\\\\\n",
    "    &\\text{while not } done:\\\\\n",
    "    & \\quad\\quad \\text{for } i \\text{ in } (1, \\ldots, m):\\\\\n",
    "    & \\quad\\quad\\quad\\quad a_i = \\argmax_{a_i} Q_i(s, a_1, \\ldots, a_i)\\\\\n",
    "    & \\quad\\quad s', r, d = env.step(a_1, \\ldots, a_m)\\\\\n",
    "    & \\quad\\quad \\text{for } i \\text{ in } (1, \\ldots, m-1):\\\\\n",
    "    & \\quad\\quad\\quad\\quad Q_i(s, a_1, \\ldots, a_i) = Q_i(s, a_1, \\ldots, a_i) + (i/m) * lr * (\\max_{a_{i+1}} Q_{i+1}(s, a_1, \\ldots, a_{i+1}) - Q_i(s, a_1, \\ldots, a_i))\\\\\n",
    "    & \\quad\\quad Q_m(s, a_1, \\ldots, a_m) = Q_m(s, a_1, \\ldots, a_m) + lr * ((mean(r) + \\gamma * \\max_{a'_1} Q_1(s', a'_1)) - Q_m(s, a_1, \\ldots, a_m))\\\\\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data1 = np.vstack(rewards)\n",
    "# data2 = np.vstack(losses)\n",
    "\n",
    "# df1 = pd.DataFrame(data1, columns = [\"agent_1\", \"agent_2\"])\n",
    "# df1[\"Episode\"] = list(range(data1.shape[0]))\n",
    "\n",
    "# df2 = pd.DataFrame(data2, columns = [\"agent_1\", \"agent_2\"])\n",
    "# df2[\"Episode\"] = list(range(data1.shape[0]))\n",
    "\n",
    "# df1 = df1.melt('Episode', var_name='Agent', value_name='Rewards')\n",
    "# df2 = df2.melt('Episode', var_name='Agent', value_name='Rewards')\n",
    "# df1[\"Avg_Reward\"] = df1[\"Rewards\"].rolling(window = 10, step = 10).mean()\n",
    "# df1 = df1[df1.Episode > 10]\n",
    "# df1.dropna()\n",
    "\n",
    "# df2[\"Avg_Loss\"] = df2[\"Rewards\"].rolling(window = 10, step = 10).mean()\n",
    "# df2 = df2[df2.Episode > 10]\n",
    "# df2.dropna()\n",
    "\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.title(\"Rewards\")\n",
    "# sns.lineplot(data = df1, x = \"Episode\", y = \"Avg_Reward\")\n",
    "# plt.title(\"Rewards\")\n",
    "# plt.figure()\n",
    "# sns.lineplot(data = df1, x = \"Episode\", y = \"Avg_Reward\", hue = \"Agent\")\n",
    "\n",
    "# plt.figure()\n",
    "# plt.title(\"Losses\")\n",
    "# sns.lineplot(data = df2, x = \"Episode\", y = \"Avg_Loss\")\n",
    "# plt.figure()\n",
    "# plt.title(\"Losses\")\n",
    "# sns.lineplot(data = df2, x = \"Episode\", y = \"Avg_Loss\", hue = \"Agent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hkolstee/.local/lib/python3.10/site-packages/gymnasium/envs/registration.py:694: UserWarning: \u001b[33mWARN: Overriding environment SpiderFlyGrid-v0 already in registry.\u001b[0m\n",
      "  logger.warn(f\"Overriding environment {new_spec.id} already in registry.\")\n",
      "/home/hkolstee/.local/lib/python3.10/site-packages/gymnasium/envs/registration.py:694: UserWarning: \u001b[33mWARN: Overriding environment SpiderFlyGridMA-v0 already in registry.\u001b[0m\n",
      "  logger.warn(f\"Overriding environment {new_spec.id} already in registry.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['O' ' ' ' ' ' ']\n",
      " ['X' ' ' ' ' ' ']\n",
      " [' ' ' ' 'X' ' ']\n",
      " [' ' ' ' ' ' ' ']]\n"
     ]
    }
   ],
   "source": [
    "from shared_seq_double_DQN import seqDoubleDQN\n",
    "from shared_seq_DQN import seqDQN\n",
    "from shared_DQN import IndependentDQN\n",
    "from custom_spider_env.spider_fly_env.envs.grid_MA_pettingzoo import SpiderFlyEnvMA\n",
    "from spider_fly_env.wrappers.pettingzoo_wrapper import PettingZooWrapper\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "env = SpiderFlyEnvMA(size = 4, spiders = 2, max_timesteps = 100, render_mode = \"ascii\")\n",
    "env = PettingZooWrapper(env)\n",
    "\n",
    "env.observation_space[0].sample()\n",
    "\n",
    "env = SpiderFlyEnvMA(size = 4, spiders = 2, max_timesteps = 100)\n",
    "env = PettingZooWrapper(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hkolstee/.local/lib/python3.10/site-packages/torch/cuda/__init__.py:138: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11040). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n",
      "/home/hkolstee/masterThesis/masterThesis/shared_seq_DQN.py:353: RuntimeWarning: invalid value encountered in divide\n",
      "  avg_loss = loss_sum / learn_steps\n",
      "/home/hkolstee/masterThesis/masterThesis/shared_seq_DQN.py:355: RuntimeWarning: invalid value encountered in divide\n",
      "  loss_log.append(loss_sum / learn_steps)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0 - Reward:[1.825 1.888] - Avg loss (last ep): [nan nan]\n",
      "Episode: 10 - Reward:[0.838 0.883] - Avg loss (last ep): [0.00032585 0.00187178]\n",
      "Episode: 20 - Reward:[4.927 4.846] - Avg loss (last ep): [0.00075622 0.00281553]\n",
      "Episode: 30 - Reward:[3.868 3.856] - Avg loss (last ep): [0.0009669  0.00291591]\n",
      "Episode: 40 - Reward:[7.    6.823] - Avg loss (last ep): [0.00143454 0.00401261]\n",
      "Episode: 50 - Reward:[11.965 11.86 ] - Avg loss (last ep): [0.00189193 0.00633617]\n",
      "Episode: 60 - Reward:[14.944 14.866] - Avg loss (last ep): [0.0025698  0.00751691]\n",
      "Episode: 70 - Reward:[24.958 24.883] - Avg loss (last ep): [0.00299741 0.00939011]\n",
      "Episode: 80 - Reward:[26.908 26.914] - Avg loss (last ep): [0.00378357 0.01105038]\n",
      "Episode: 90 - Reward:[40.933 40.942] - Avg loss (last ep): [0.00431614 0.01386695]\n",
      "Episode: 100 - Reward:[35.956 35.908] - Avg loss (last ep): [0.00457349 0.01610767]\n",
      "Episode: 110 - Reward:[33.913 33.973] - Avg loss (last ep): [0.00505176 0.01920811]\n",
      "Episode: 120 - Reward:[28.96  28.897] - Avg loss (last ep): [0.00519203 0.02029506]\n",
      "Episode: 130 - Reward:[38.899 38.911] - Avg loss (last ep): [0.00559205 0.02135373]\n",
      "Episode: 140 - Reward:[38.905 38.929] - Avg loss (last ep): [0.00558731 0.02209106]\n",
      "Episode: 150 - Reward:[41.947 41.902] - Avg loss (last ep): [0.00565025 0.02235448]\n",
      "Episode: 160 - Reward:[40.912 40.924] - Avg loss (last ep): [0.00539846 0.02412153]\n",
      "Episode: 170 - Reward:[39.907 39.928] - Avg loss (last ep): [0.00522079 0.02521682]\n",
      "Episode: 180 - Reward:[39.943 39.91 ] - Avg loss (last ep): [0.00541343 0.02610719]\n",
      "Episode: 190 - Reward:[38.944 38.917] - Avg loss (last ep): [0.00545604 0.02784811]\n"
     ]
    }
   ],
   "source": [
    "sequential_DQN = seqDQN(env, eps_steps = 100 * 100, layer_sizes = (64, 64), tau = 0.0025, buffer_max_size = 100000, batch_size = 256, global_observations = True, log_dir = \"tensorboard_logs_seqDQN_hard2_eq_lr\") \n",
    "\n",
    "rewards, losses = sequential_DQN.train(200)\n",
    "\n",
    "# save model\n",
    "sequential_DQN.shared_DQN.save(\"models\", \"seqDQN_hard2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequential_DQN.shared_DQN.save(\"models\", \"seqDQN_hard2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "model = seqDQN(env, layer_sizes = (64,64), global_observations = True)\n",
    "model.shared_DQN.load(\"models/seqDQN_hard2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ']\n",
      " ['X' ' ' ' ' ' ']\n",
      " [' ' 'O' 'X' ' ']]\n",
      "start ----------------------------\n",
      "[[' ' ' ' ' ' ' ']\n",
      " [' ' ' ' 'X' ' ']\n",
      " [' ' ' ' ' ' ' ']\n",
      " [' ' 'O' ' ' 'X']]\n",
      "['left', 'left']\n",
      "[[' ' ' ' ' ' ' ']\n",
      " [' ' 'X' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ']\n",
      " [' ' 'O' 'X' ' ']]\n",
      "['nothing', 'down']\n",
      "[[' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ']\n",
      " [' ' 'X' ' ' ' ']\n",
      " [' ' 'O' 'X' ' ']]\n"
     ]
    }
   ],
   "source": [
    "env = SpiderFlyEnvMA(size = 4, spiders = 2, max_timesteps = 100, render_mode = \"ascii\")\n",
    "env = PettingZooWrapper(env)\n",
    "\n",
    "terminal = False\n",
    "print(\"start ----------------------------\")\n",
    "obs, _ = env.reset()\n",
    "while not terminal:\n",
    "    actions = []\n",
    "    actions = model.get_actions(obs, deterministic = True)\n",
    "    print([env.action_to_direction_string[act] for act in actions])\n",
    "\n",
    "    obs, rewards, terminals, truncations, infos = env.step(actions)\n",
    "    if rewards[0] == 1:\n",
    "        break\n",
    "    # print(obs, rewards, terminals, truncations, infos, actions)\n",
    "    # print(\"--------------------\")\n",
    "    \n",
    "    terminal = terminals[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 Spiders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hkolstee/.local/lib/python3.10/site-packages/gymnasium/envs/registration.py:694: UserWarning: \u001b[33mWARN: Overriding environment SpiderFlyGrid-v0 already in registry.\u001b[0m\n",
      "  logger.warn(f\"Overriding environment {new_spec.id} already in registry.\")\n",
      "/home/hkolstee/.local/lib/python3.10/site-packages/gymnasium/envs/registration.py:694: UserWarning: \u001b[33mWARN: Overriding environment SpiderFlyGridMA-v0 already in registry.\u001b[0m\n",
      "  logger.warn(f\"Overriding environment {new_spec.id} already in registry.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['X' ' ' ' ' ' ']\n",
      " ['O' 'X' ' ' ' ']\n",
      " ['X' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ']]\n"
     ]
    }
   ],
   "source": [
    "from shared_seq_double_DQN import seqDoubleDQN\n",
    "from shared_seq_DQN import seqDQN\n",
    "from shared_DQN import IndependentDQN\n",
    "from custom_spider_env.spider_fly_env.envs.grid_MA_pettingzoo import SpiderFlyEnvMA\n",
    "from spider_fly_env.wrappers.pettingzoo_wrapper import PettingZooWrapper\n",
    "\n",
    "from pettingzoo.test import parallel_api_test\n",
    "from supersuit import normalize_obs_v0, dtype_v0\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "env = SpiderFlyEnvMA(size = 4, spiders = 3, max_timesteps = 100, render_mode = \"ascii\")\n",
    "env = PettingZooWrapper(env, normalize = False)\n",
    "\n",
    "\n",
    "env = SpiderFlyEnvMA(size = 4, spiders = 3, max_timesteps = 100)\n",
    "env = PettingZooWrapper(env, normalize = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GET ACTIONS -------\n",
      "last action sampled:  1\n",
      "last action sampled:  1\n",
      "last action sampled:  0\n",
      "Transition added:  [[0, 3, 2, 1, 1, 3, 3, 1], [0, 3, 2, 1, 1, 3, 3, 1], [0, 3, 2, 1, 1, 3, 3, 1]] [1, 1, 0] [-0.002, 0.001, -0.002] [[0, 3, 2, 1, 1, 3, 3, 1], [0, 3, 2, 1, 1, 3, 3, 1], [0, 3, 2, 1, 1, 3, 3, 1]] [False, False, False]\n",
      "GET ACTIONS -------\n",
      "last action sampled:  3\n",
      "last action sampled:  3\n",
      "last action sampled:  1\n",
      "Transition added:  [[0, 3, 2, 1, 1, 3, 3, 1], [0, 3, 2, 1, 1, 3, 3, 1], [0, 3, 2, 1, 1, 3, 3, 1]] [3, 3, 1] [-0.002, -0.002, -0.002] [[0, 3, 2, 2, 2, 3, 3, 1], [0, 3, 2, 2, 2, 3, 3, 1], [0, 3, 2, 2, 2, 3, 3, 1]] [False, False, False]\n",
      "TRAINING STEP --------------\n",
      "---------------\n",
      "input tensor agent  0\n",
      "tensor([[0., 3., 2., 1., 1., 3., 3., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0.],\n",
      "        [0., 3., 2., 1., 1., 3., 3., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0.]])\n",
      "Qvals\n",
      "tensor([[ 0.0692, -0.2229,  0.0311,  0.1646, -0.1517],\n",
      "        [ 0.0692, -0.2229,  0.0311,  0.1646, -0.1517]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "taken action\n",
      "tensor([[1],\n",
      "        [3]])\n",
      "Taken Q_val\n",
      "tensor([[-0.2229],\n",
      "        [ 0.1646]], grad_fn=<GatherBackward0>)\n",
      "-----------------\n",
      "target input tensor\n",
      "tensor([[0., 3., 2., 1., 1., 3., 3., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0.],\n",
      "        [0., 3., 2., 1., 1., 3., 3., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0.]])\n",
      "target Q vals\n",
      "tensor([[ 0.0972, -0.1697, -0.0072,  0.1094, -0.1419],\n",
      "        [ 0.0871, -0.1735, -0.0199,  0.1086, -0.1183]])\n",
      "Max target Q vals\n",
      "tensor([0.1094, 0.1086])\n",
      "---------------\n",
      "input tensor agent  1\n",
      "tensor([[0., 3., 2., 1., 1., 3., 3., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0.],\n",
      "        [0., 3., 2., 1., 1., 3., 3., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0.]])\n",
      "Qvals\n",
      "tensor([[ 0.1118, -0.1067, -0.0073,  0.0943, -0.1347],\n",
      "        [ 0.0986, -0.1126, -0.0195,  0.0919, -0.1094]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "taken action\n",
      "tensor([[1],\n",
      "        [3]])\n",
      "Taken Q_val\n",
      "tensor([[-0.1067],\n",
      "        [ 0.0919]], grad_fn=<GatherBackward0>)\n",
      "-----------------\n",
      "target input tensor\n",
      "tensor([[0., 3., 2., 1., 1., 3., 3., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         0., 0., 1.],\n",
      "        [0., 3., 2., 1., 1., 3., 3., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0.,\n",
      "         0., 0., 1.]])\n",
      "target Q vals\n",
      "tensor([[ 0.0780, -0.1634,  0.0060,  0.1353, -0.1178],\n",
      "        [ 0.0786, -0.1628, -0.0364,  0.0978, -0.0852]])\n",
      "Max target Q vals\n",
      "tensor([0.1353, 0.0978])\n",
      "---------------\n",
      "input tensor agent  2\n",
      "tensor([[0., 3., 2., 1., 1., 3., 3., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         0., 0., 1.],\n",
      "        [0., 3., 2., 1., 1., 3., 3., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0.,\n",
      "         0., 0., 1.]])\n",
      "Qvals\n",
      "tensor([[ 0.1022, -0.0562,  0.0045,  0.1131, -0.1139],\n",
      "        [ 0.0968, -0.0671, -0.0325,  0.0760, -0.0797]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "taken action\n",
      "tensor([[0],\n",
      "        [1]])\n",
      "Taken Q_val\n",
      "tensor([[ 0.1022],\n",
      "        [-0.0671]], grad_fn=<GatherBackward0>)\n",
      "-----------------\n",
      "last target input tensor\n",
      "tensor([[0., 3., 2., 1., 1., 3., 3., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0.],\n",
      "        [0., 3., 2., 2., 2., 3., 3., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0.]])\n",
      "last target Q vals\n",
      "tensor([[ 0.0692, -0.2229,  0.0311,  0.1646, -0.1517],\n",
      "        [ 0.0619, -0.2362,  0.0067,  0.1502, -0.1222]])\n",
      "Max target Q vals\n",
      "tensor([0.1353, 0.0978])\n",
      "rewards, dones, gamma tensor([-0.0010, -0.0020]) tensor([0., 0.]) 0.99\n",
      "GET ACTIONS -------\n",
      "Input Tensor agent  0\n",
      "tensor([[0., 3., 2., 2., 2., 3., 3., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0.]])\n",
      "last action sampled:  0\n",
      "last action sampled:  1\n",
      "last action sampled:  0\n",
      "Transition added:  [[0, 3, 2, 2, 2, 3, 3, 1], [0, 3, 2, 2, 2, 3, 3, 1], [0, 3, 2, 2, 2, 3, 3, 1]] [0, 1, 0] [-0.002, 0.001, -0.002] [[0, 3, 3, 2, 2, 3, 3, 1], [0, 3, 3, 2, 2, 3, 3, 1], [0, 3, 3, 2, 2, 3, 3, 1]] [False, False, False]\n",
      "TRAINING STEP --------------\n",
      "---------------\n",
      "input tensor agent  0\n",
      "tensor([[0., 3., 2., 1., 1., 3., 3., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0.],\n",
      "        [0., 3., 2., 2., 2., 3., 3., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0.]])\n",
      "Qvals\n",
      "tensor([[ 0.1148, -0.0456,  0.0269,  0.1285, -0.1313],\n",
      "        [ 0.1084, -0.0510,  0.0018,  0.1080, -0.1046]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "taken action\n",
      "tensor([[1],\n",
      "        [0]])\n",
      "Taken Q_val\n",
      "tensor([[-0.0456],\n",
      "        [ 0.1084]], grad_fn=<GatherBackward0>)\n",
      "-----------------\n",
      "target input tensor\n",
      "tensor([[0., 3., 2., 1., 1., 3., 3., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0.],\n",
      "        [0., 3., 2., 2., 2., 3., 3., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0.]])\n",
      "target Q vals\n",
      "tensor([[ 0.0973, -0.1692, -0.0071,  0.1093, -0.1419],\n",
      "        [ 0.0825, -0.2617, -0.0193,  0.1213, -0.1409]])\n",
      "Max target Q vals\n",
      "tensor([0.1093, 0.1213])\n",
      "---------------\n",
      "input tensor agent  1\n",
      "tensor([[0., 3., 2., 1., 1., 3., 3., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0.],\n",
      "        [0., 3., 2., 2., 2., 3., 3., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0.]])\n",
      "Qvals\n",
      "tensor([[ 0.1663,  0.0636, -0.0045,  0.0700, -0.1069],\n",
      "        [ 0.1435, -0.0176, -0.0254,  0.0647, -0.1166]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "taken action\n",
      "tensor([[1],\n",
      "        [1]])\n",
      "Taken Q_val\n",
      "tensor([[ 0.0636],\n",
      "        [-0.0176]], grad_fn=<GatherBackward0>)\n",
      "-----------------\n",
      "target input tensor\n",
      "tensor([[0., 3., 2., 1., 1., 3., 3., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         0., 0., 1.],\n",
      "        [0., 3., 2., 2., 2., 3., 3., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         0., 0., 1.]])\n",
      "target Q vals\n",
      "tensor([[ 7.8115e-02, -1.6305e-01,  6.0422e-03,  1.3521e-01, -1.1776e-01],\n",
      "        [ 6.0070e-02, -2.3116e-01,  7.3496e-05,  1.3579e-01, -1.2511e-01]])\n",
      "Max target Q vals\n",
      "tensor([0.1352, 0.1358])\n",
      "---------------\n",
      "input tensor agent  2\n",
      "tensor([[0., 3., 2., 1., 1., 3., 3., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         0., 0., 1.],\n",
      "        [0., 3., 2., 2., 2., 3., 3., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         0., 0., 1.]])\n",
      "Qvals\n",
      "tensor([[ 0.1580,  0.0806,  0.0034,  0.0982, -0.1000],\n",
      "        [ 0.1447,  0.0390, -0.0028,  0.0756, -0.1127]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "taken action\n",
      "tensor([[0],\n",
      "        [0]])\n",
      "Taken Q_val\n",
      "tensor([[0.1580],\n",
      "        [0.1447]], grad_fn=<GatherBackward0>)\n",
      "-----------------\n",
      "last target input tensor\n",
      "tensor([[0., 3., 2., 1., 1., 3., 3., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0.],\n",
      "        [0., 3., 3., 2., 2., 3., 3., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0.]])\n",
      "last target Q vals\n",
      "tensor([[ 0.0693, -0.2225,  0.0311,  0.1646, -0.1516],\n",
      "        [ 0.0531, -0.2755,  0.0064,  0.1494, -0.1517]])\n",
      "Max target Q vals\n",
      "tensor([0.1352, 0.1358])\n",
      "rewards, dones, gamma tensor([-0.0010, -0.0010]) tensor([0., 0.]) 0.99\n",
      "GET ACTIONS -------\n",
      "last action sampled:  1\n",
      "last action sampled:  4\n",
      "last action sampled:  4\n",
      "Transition added:  [[0, 3, 3, 2, 2, 3, 3, 1], [0, 3, 3, 2, 2, 3, 3, 1], [0, 3, 3, 2, 2, 3, 3, 1]] [1, 4, 4] [-0.002, 0.001, -0.002] [[1, 3, 3, 2, 2, 2, 3, 1], [1, 3, 3, 2, 2, 2, 3, 1], [1, 3, 3, 2, 2, 2, 3, 1]] [False, False, False]\n",
      "TRAINING STEP --------------\n",
      "---------------\n",
      "input tensor agent  0\n",
      "tensor([[0., 3., 2., 1., 1., 3., 3., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0.],\n",
      "        [0., 3., 2., 2., 2., 3., 3., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0.]])\n",
      "Qvals\n",
      "tensor([[ 1.6649e-01,  9.3471e-02,  2.4670e-02,  1.0677e-01, -1.1599e-01],\n",
      "        [ 1.6018e-01,  1.0528e-01, -5.9821e-05,  8.1041e-02, -8.3244e-02]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "taken action\n",
      "tensor([[1],\n",
      "        [0]])\n",
      "Taken Q_val\n",
      "tensor([[0.0935],\n",
      "        [0.1602]], grad_fn=<GatherBackward0>)\n",
      "-----------------\n",
      "target input tensor\n",
      "tensor([[0., 3., 2., 1., 1., 3., 3., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0.],\n",
      "        [0., 3., 2., 2., 2., 3., 3., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0.]])\n",
      "target Q vals\n",
      "tensor([[ 0.0976, -0.1683, -0.0071,  0.1091, -0.1418],\n",
      "        [ 0.0827, -0.2608, -0.0193,  0.1211, -0.1408]])\n",
      "Max target Q vals\n",
      "tensor([0.1091, 0.1211])\n",
      "---------------\n",
      "input tensor agent  1\n",
      "tensor([[0., 3., 2., 1., 1., 3., 3., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0.],\n",
      "        [0., 3., 2., 2., 2., 3., 3., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0.]])\n",
      "Qvals\n",
      "tensor([[ 0.1980,  0.1845, -0.0105,  0.0574, -0.0865],\n",
      "        [ 0.1778,  0.1350, -0.0330,  0.0389, -0.0975]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "taken action\n",
      "tensor([[1],\n",
      "        [1]])\n",
      "Taken Q_val\n",
      "tensor([[0.1845],\n",
      "        [0.1350]], grad_fn=<GatherBackward0>)\n",
      "-----------------\n",
      "target input tensor\n",
      "tensor([[0., 3., 2., 1., 1., 3., 3., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         0., 0., 1.],\n",
      "        [0., 3., 2., 2., 2., 3., 3., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         0., 0., 1.]])\n",
      "target Q vals\n",
      "tensor([[ 7.8358e-02, -1.6229e-01,  6.0399e-03,  1.3508e-01, -1.1771e-01],\n",
      "        [ 6.0361e-02, -2.3026e-01,  4.8473e-05,  1.3561e-01, -1.2501e-01]])\n",
      "Max target Q vals\n",
      "tensor([0.1351, 0.1356])\n",
      "---------------\n",
      "input tensor agent  2\n",
      "tensor([[0., 3., 2., 1., 1., 3., 3., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         0., 0., 1.],\n",
      "        [0., 3., 2., 2., 2., 3., 3., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         0., 0., 1.]])\n",
      "Qvals\n",
      "tensor([[ 0.1797,  0.1701, -0.0005,  0.0915, -0.0939],\n",
      "        [ 0.1720,  0.1482, -0.0021,  0.0518, -0.1083]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "taken action\n",
      "tensor([[0],\n",
      "        [0]])\n",
      "Taken Q_val\n",
      "tensor([[0.1797],\n",
      "        [0.1720]], grad_fn=<GatherBackward0>)\n",
      "-----------------\n",
      "last target input tensor\n",
      "tensor([[0., 3., 2., 1., 1., 3., 3., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0.],\n",
      "        [0., 3., 3., 2., 2., 3., 3., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0.]])\n",
      "last target Q vals\n",
      "tensor([[ 0.0696, -0.2216,  0.0311,  0.1644, -0.1515],\n",
      "        [ 0.0533, -0.2745,  0.0064,  0.1492, -0.1516]])\n",
      "Max target Q vals\n",
      "tensor([0.1351, 0.1356])\n",
      "rewards, dones, gamma tensor([-0.0010, -0.0010]) tensor([0., 0.]) 0.99\n",
      "GET ACTIONS -------\n",
      "last action sampled:  0\n",
      "last action sampled:  2\n",
      "last action sampled:  0\n",
      "Transition added:  [[1, 3, 3, 2, 2, 2, 3, 1], [1, 3, 3, 2, 2, 2, 3, 1], [1, 3, 3, 2, 2, 2, 3, 1]] [0, 2, 0] [-0.002, 0.001, -0.002] [[1, 3, 3, 2, 2, 2, 3, 1], [1, 3, 3, 2, 2, 2, 3, 1], [1, 3, 3, 2, 2, 2, 3, 1]] [False, False, False]\n",
      "TRAINING STEP --------------\n",
      "---------------\n",
      "input tensor agent  0\n",
      "tensor([[0., 3., 2., 1., 1., 3., 3., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0.],\n",
      "        [0., 3., 2., 1., 1., 3., 3., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0.]])\n",
      "Qvals\n",
      "tensor([[ 0.1744,  0.1752,  0.0167,  0.0939, -0.0965],\n",
      "        [ 0.1744,  0.1752,  0.0167,  0.0939, -0.0965]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "taken action\n",
      "tensor([[3],\n",
      "        [1]])\n",
      "Taken Q_val\n",
      "tensor([[0.0939],\n",
      "        [0.1752]], grad_fn=<GatherBackward0>)\n",
      "-----------------\n",
      "target input tensor\n",
      "tensor([[0., 3., 2., 1., 1., 3., 3., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0.],\n",
      "        [0., 3., 2., 1., 1., 3., 3., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0.]])\n",
      "target Q vals\n",
      "tensor([[ 0.0877, -0.1710, -0.0199,  0.1082, -0.1180],\n",
      "        [ 0.0980, -0.1671, -0.0072,  0.1090, -0.1416]])\n",
      "Max target Q vals\n",
      "tensor([0.1082, 0.1090])\n",
      "---------------\n",
      "input tensor agent  1\n",
      "tensor([[0., 3., 2., 1., 1., 3., 3., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0.],\n",
      "        [0., 3., 2., 1., 1., 3., 3., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0.]])\n",
      "Qvals\n",
      "tensor([[ 0.1715,  0.2484, -0.0275,  0.0457, -0.0620],\n",
      "        [ 0.1992,  0.2477, -0.0196,  0.0533, -0.0750]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "taken action\n",
      "tensor([[3],\n",
      "        [1]])\n",
      "Taken Q_val\n",
      "tensor([[0.0457],\n",
      "        [0.2477]], grad_fn=<GatherBackward0>)\n",
      "-----------------\n",
      "target input tensor\n",
      "tensor([[0., 3., 2., 1., 1., 3., 3., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0.,\n",
      "         0., 0., 1.],\n",
      "        [0., 3., 2., 1., 1., 3., 3., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         0., 0., 1.]])\n",
      "target Q vals\n",
      "tensor([[ 0.0791, -0.1607, -0.0364,  0.0974, -0.0849],\n",
      "        [ 0.0786, -0.1613,  0.0060,  0.1349, -0.1176]])\n",
      "Max target Q vals\n",
      "tensor([0.0974, 0.1349])\n",
      "---------------\n",
      "input tensor agent  2\n",
      "tensor([[0., 3., 2., 1., 1., 3., 3., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0.,\n",
      "         0., 0., 1.],\n",
      "        [0., 3., 2., 1., 1., 3., 3., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         0., 0., 1.]])\n",
      "Qvals\n",
      "tensor([[ 0.1470,  0.2053, -0.0353,  0.0460, -0.0573],\n",
      "        [ 0.1717,  0.2122, -0.0030,  0.0888, -0.0910]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "taken action\n",
      "tensor([[1],\n",
      "        [0]])\n",
      "Taken Q_val\n",
      "tensor([[0.2053],\n",
      "        [0.1717]], grad_fn=<GatherBackward0>)\n",
      "-----------------\n",
      "last target input tensor\n",
      "tensor([[0., 3., 2., 2., 2., 3., 3., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0.],\n",
      "        [0., 3., 2., 1., 1., 3., 3., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0.]])\n",
      "last target Q vals\n",
      "tensor([[ 0.0626, -0.2336,  0.0067,  0.1496, -0.1219],\n",
      "        [ 0.0698, -0.2205,  0.0311,  0.1642, -0.1514]])\n",
      "Max target Q vals\n",
      "tensor([0.0974, 0.1349])\n",
      "rewards, dones, gamma tensor([-0.0020, -0.0010]) tensor([0., 0.]) 0.99\n",
      "GET ACTIONS -------\n",
      "last action sampled:  0\n",
      "last action sampled:  1\n",
      "last action sampled:  3\n",
      "Transition added:  [[1, 3, 3, 2, 2, 2, 3, 1], [1, 3, 3, 2, 2, 2, 3, 1], [1, 3, 3, 2, 2, 2, 3, 1]] [0, 1, 3] [-0.002, 0.001, -0.002] [[1, 3, 3, 2, 2, 3, 3, 1], [1, 3, 3, 2, 2, 3, 3, 1], [1, 3, 3, 2, 2, 3, 3, 1]] [False, False, False]\n",
      "TRAINING STEP --------------\n",
      "---------------\n",
      "input tensor agent  0\n",
      "tensor([[0., 3., 2., 1., 1., 3., 3., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0.],\n",
      "        [0., 3., 3., 2., 2., 3., 3., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0.]])\n",
      "Qvals\n",
      "tensor([[ 0.1691,  0.2077,  0.0146,  0.0996, -0.0861],\n",
      "        [ 0.1679,  0.2521, -0.0143,  0.0605, -0.0568]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "taken action\n",
      "tensor([[3],\n",
      "        [1]])\n",
      "Taken Q_val\n",
      "tensor([[0.0996],\n",
      "        [0.2521]], grad_fn=<GatherBackward0>)\n",
      "-----------------\n",
      "target input tensor\n",
      "tensor([[0., 3., 2., 1., 1., 3., 3., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0.],\n",
      "        [0., 3., 3., 2., 2., 3., 3., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0.]])\n",
      "target Q vals\n",
      "tensor([[ 0.0880, -0.1697, -0.0199,  0.1080, -0.1177],\n",
      "        [ 0.0900, -0.2528, -0.0338,  0.1015, -0.1470]])\n",
      "Max target Q vals\n",
      "tensor([0.1080, 0.1015])\n",
      "---------------\n",
      "input tensor agent  1\n",
      "tensor([[0., 3., 2., 1., 1., 3., 3., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0.],\n",
      "        [0., 3., 3., 2., 2., 3., 3., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0.]])\n",
      "Qvals\n",
      "tensor([[ 0.1613,  0.2628, -0.0269,  0.0627, -0.0603],\n",
      "        [ 0.1903,  0.2793, -0.0505,  0.0384, -0.0591]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "taken action\n",
      "tensor([[3],\n",
      "        [4]])\n",
      "Taken Q_val\n",
      "tensor([[ 0.0627],\n",
      "        [-0.0591]], grad_fn=<GatherBackward0>)\n",
      "-----------------\n",
      "target input tensor\n",
      "tensor([[0., 3., 2., 1., 1., 3., 3., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0.,\n",
      "         0., 0., 1.],\n",
      "        [0., 3., 3., 2., 2., 3., 3., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "         0., 0., 1.]])\n",
      "target Q vals\n",
      "tensor([[ 0.0793, -0.1596, -0.0363,  0.0972, -0.0848],\n",
      "        [ 0.0697, -0.2497, -0.0498,  0.1129, -0.1012]])\n",
      "Max target Q vals\n",
      "tensor([0.0972, 0.1129])\n",
      "---------------\n",
      "input tensor agent  2\n",
      "tensor([[0., 3., 2., 1., 1., 3., 3., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0.,\n",
      "         0., 0., 1.],\n",
      "        [0., 3., 3., 2., 2., 3., 3., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "         0., 0., 1.]])\n",
      "Qvals\n",
      "tensor([[ 0.1359,  0.2106, -0.0319,  0.0619, -0.0459],\n",
      "        [ 0.1560,  0.2299, -0.0729,  0.0553, -0.0214]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "taken action\n",
      "tensor([[1],\n",
      "        [4]])\n",
      "Taken Q_val\n",
      "tensor([[ 0.2106],\n",
      "        [-0.0214]], grad_fn=<GatherBackward0>)\n",
      "-----------------\n",
      "last target input tensor\n",
      "tensor([[0., 3., 2., 2., 2., 3., 3., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0.],\n",
      "        [1., 3., 3., 2., 2., 2., 3., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0.]])\n",
      "last target Q vals\n",
      "tensor([[ 0.0629, -0.2322,  0.0067,  0.1494, -0.1218],\n",
      "        [ 0.0436, -0.2881,  0.0695,  0.1653, -0.1077]])\n",
      "Max target Q vals\n",
      "tensor([0.0972, 0.1129])\n",
      "rewards, dones, gamma tensor([-0.0020, -0.0010]) tensor([0., 0.]) 0.99\n",
      "GET ACTIONS -------\n",
      "last action sampled:  2\n",
      "last action sampled:  4\n",
      "last action sampled:  1\n",
      "Transition added:  [[1, 3, 3, 2, 2, 3, 3, 1], [1, 3, 3, 2, 2, 3, 3, 1], [1, 3, 3, 2, 2, 3, 3, 1]] [2, 4, 1] [-0.002, 0.001, -0.002] [[0, 3, 3, 2, 3, 3, 3, 1], [0, 3, 3, 2, 3, 3, 3, 1], [0, 3, 3, 2, 3, 3, 3, 1]] [False, False, False]\n",
      "TRAINING STEP --------------\n",
      "---------------\n",
      "input tensor agent  0\n",
      "tensor([[0., 3., 2., 2., 2., 3., 3., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0.],\n",
      "        [0., 3., 2., 2., 2., 3., 3., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0.]])\n",
      "Qvals\n",
      "tensor([[ 0.1439,  0.2332,  0.0024,  0.0855, -0.0246],\n",
      "        [ 0.1439,  0.2332,  0.0024,  0.0855, -0.0246]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "taken action\n",
      "tensor([[0],\n",
      "        [0]])\n",
      "Taken Q_val\n",
      "tensor([[0.1439],\n",
      "        [0.1439]], grad_fn=<GatherBackward0>)\n",
      "-----------------\n",
      "target input tensor\n",
      "tensor([[0., 3., 2., 2., 2., 3., 3., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0.],\n",
      "        [0., 3., 2., 2., 2., 3., 3., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0.]])\n",
      "target Q vals\n",
      "tensor([[ 0.0835, -0.2566, -0.0194,  0.1203, -0.1401],\n",
      "        [ 0.0835, -0.2566, -0.0194,  0.1203, -0.1401]])\n",
      "Max target Q vals\n",
      "tensor([0.1203, 0.1203])\n",
      "---------------\n",
      "input tensor agent  1\n",
      "tensor([[0., 3., 2., 2., 2., 3., 3., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0.],\n",
      "        [0., 3., 2., 2., 2., 3., 3., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0.]])\n",
      "Qvals\n",
      "tensor([[ 0.1451,  0.2489, -0.0217,  0.0393, -0.0225],\n",
      "        [ 0.1451,  0.2489, -0.0217,  0.0393, -0.0225]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "taken action\n",
      "tensor([[1],\n",
      "        [1]])\n",
      "Taken Q_val\n",
      "tensor([[0.2489],\n",
      "        [0.2489]], grad_fn=<GatherBackward0>)\n",
      "-----------------\n",
      "target input tensor\n",
      "tensor([[0., 3., 2., 2., 2., 3., 3., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         0., 0., 1.],\n",
      "        [0., 3., 2., 2., 2., 3., 3., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         0., 0., 1.]])\n",
      "target Q vals\n",
      "tensor([[ 6.1319e-02, -2.2631e-01, -1.6024e-05,  1.3488e-01, -1.2447e-01],\n",
      "        [ 6.1319e-02, -2.2631e-01, -1.6011e-05,  1.3488e-01, -1.2447e-01]])\n",
      "Max target Q vals\n",
      "tensor([0.1349, 0.1349])\n",
      "---------------\n",
      "input tensor agent  2\n",
      "tensor([[0., 3., 2., 2., 2., 3., 3., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         0., 0., 1.],\n",
      "        [0., 3., 2., 2., 2., 3., 3., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         0., 0., 1.]])\n",
      "Qvals\n",
      "tensor([[ 0.1215,  0.2004,  0.0073,  0.0726, -0.0420],\n",
      "        [ 0.1215,  0.2004,  0.0073,  0.0726, -0.0420]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "taken action\n",
      "tensor([[0],\n",
      "        [0]])\n",
      "Taken Q_val\n",
      "tensor([[0.1215],\n",
      "        [0.1215]], grad_fn=<GatherBackward0>)\n",
      "-----------------\n",
      "last target input tensor\n",
      "tensor([[0., 3., 3., 2., 2., 3., 3., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0.],\n",
      "        [0., 3., 3., 2., 2., 3., 3., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0.]])\n",
      "last target Q vals\n",
      "tensor([[ 0.0543, -0.2702,  0.0064,  0.1484, -0.1510],\n",
      "        [ 0.0543, -0.2702,  0.0064,  0.1484, -0.1510]])\n",
      "Max target Q vals\n",
      "tensor([0.1349, 0.1349])\n",
      "rewards, dones, gamma tensor([-0.0010, -0.0010]) tensor([0., 0.]) 0.99\n",
      "GET ACTIONS -------\n",
      "Input Tensor agent  0\n",
      "tensor([[0., 3., 3., 2., 3., 3., 3., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0.]])\n",
      "last action sampled:  1\n",
      "last action sampled:  3\n",
      "last action sampled:  1\n",
      "Transition added:  [[0, 3, 3, 2, 3, 3, 3, 1], [0, 3, 3, 2, 3, 3, 3, 1], [0, 3, 3, 2, 3, 3, 3, 1]] [1, 3, 1] [-0.002, 0.001, -0.002] [[1, 3, 3, 2, 3, 3, 3, 1], [1, 3, 3, 2, 3, 3, 3, 1], [1, 3, 3, 2, 3, 3, 3, 1]] [False, False, False]\n",
      "TRAINING STEP --------------\n",
      "---------------\n",
      "input tensor agent  0\n",
      "tensor([[0., 3., 3., 2., 3., 3., 3., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0.],\n",
      "        [0., 3., 2., 1., 1., 3., 3., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0.]])\n",
      "Qvals\n",
      "tensor([[ 0.1176,  0.2217, -0.0223,  0.0430,  0.0481],\n",
      "        [ 0.1362,  0.1923,  0.0283,  0.1319, -0.0202]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "taken action\n",
      "tensor([[1],\n",
      "        [1]])\n",
      "Taken Q_val\n",
      "tensor([[0.2217],\n",
      "        [0.1923]], grad_fn=<GatherBackward0>)\n",
      "-----------------\n",
      "target input tensor\n",
      "tensor([[0., 3., 3., 2., 3., 3., 3., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0.],\n",
      "        [0., 3., 2., 1., 1., 3., 3., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0.]])\n",
      "target Q vals\n",
      "tensor([[ 0.1251, -0.2468, -0.0299,  0.0693, -0.1381],\n",
      "        [ 0.0990, -0.1630, -0.0071,  0.1086, -0.1407]])\n",
      "Max target Q vals\n",
      "tensor([0.1251, 0.1086])\n",
      "---------------\n",
      "input tensor agent  1\n",
      "tensor([[0., 3., 3., 2., 3., 3., 3., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0.],\n",
      "        [0., 3., 2., 1., 1., 3., 3., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0.]])\n",
      "Qvals\n",
      "tensor([[ 0.1502,  0.2319, -0.0469,  0.0208,  0.0413],\n",
      "        [ 0.1608,  0.2312, -0.0036,  0.1081,  0.0124]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "taken action\n",
      "tensor([[3],\n",
      "        [1]])\n",
      "Taken Q_val\n",
      "tensor([[0.0208],\n",
      "        [0.2312]], grad_fn=<GatherBackward0>)\n",
      "-----------------\n",
      "target input tensor\n",
      "tensor([[0., 3., 3., 2., 3., 3., 3., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
      "         0., 0., 1.],\n",
      "        [0., 3., 2., 1., 1., 3., 3., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         0., 0., 1.]])\n",
      "target Q vals\n",
      "tensor([[ 0.1150, -0.2425, -0.0300,  0.0400, -0.1147],\n",
      "        [ 0.0794, -0.1579,  0.0061,  0.1345, -0.1171]])\n",
      "Max target Q vals\n",
      "tensor([0.1150, 0.1345])\n",
      "---------------\n",
      "input tensor agent  2\n",
      "tensor([[0., 3., 3., 2., 3., 3., 3., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
      "         0., 0., 1.],\n",
      "        [0., 3., 2., 1., 1., 3., 3., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         0., 0., 1.]])\n",
      "Qvals\n",
      "tensor([[ 0.1456,  0.1717, -0.0514,  0.0314,  0.0591],\n",
      "        [ 0.1392,  0.1772,  0.0120,  0.1313, -0.0123]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "taken action\n",
      "tensor([[1],\n",
      "        [0]])\n",
      "Taken Q_val\n",
      "tensor([[0.1717],\n",
      "        [0.1392]], grad_fn=<GatherBackward0>)\n",
      "-----------------\n",
      "last target input tensor\n",
      "tensor([[1., 3., 3., 2., 3., 3., 3., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0.],\n",
      "        [0., 3., 2., 1., 1., 3., 3., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0.]])\n",
      "last target Q vals\n",
      "tensor([[ 0.0724, -0.3030,  0.0185,  0.1262, -0.1210],\n",
      "        [ 0.0706, -0.2167,  0.0311,  0.1636, -0.1507]])\n",
      "Max target Q vals\n",
      "tensor([0.1150, 0.1345])\n",
      "rewards, dones, gamma tensor([-0.0010, -0.0010]) tensor([0., 0.]) 0.99\n",
      "GET ACTIONS -------\n",
      "last action sampled:  1\n",
      "last action sampled:  1\n",
      "last action sampled:  0\n",
      "Transition added:  [[1, 3, 3, 2, 3, 3, 3, 1], [1, 3, 3, 2, 3, 3, 3, 1], [1, 3, 3, 2, 3, 3, 3, 1]] [1, 1, 0] [-0.002, 0.001, -0.002] [[2, 3, 3, 2, 3, 3, 3, 1], [2, 3, 3, 2, 3, 3, 3, 1], [2, 3, 3, 2, 3, 3, 3, 1]] [False, False, False]\n",
      "TRAINING STEP --------------\n",
      "---------------\n",
      "input tensor agent  0\n",
      "tensor([[0., 3., 3., 2., 2., 3., 3., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0.],\n",
      "        [0., 3., 3., 2., 2., 3., 3., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0.]])\n",
      "Qvals\n",
      "tensor([[0.1195, 0.1854, 0.0118, 0.1135, 0.0495],\n",
      "        [0.1195, 0.1854, 0.0118, 0.1135, 0.0495]], grad_fn=<AddmmBackward0>)\n",
      "taken action\n",
      "tensor([[1],\n",
      "        [1]])\n",
      "Taken Q_val\n",
      "tensor([[0.1854],\n",
      "        [0.1854]], grad_fn=<GatherBackward0>)\n",
      "-----------------\n",
      "target input tensor\n",
      "tensor([[0., 3., 3., 2., 2., 3., 3., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0.],\n",
      "        [0., 3., 3., 2., 2., 3., 3., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0.]])\n",
      "target Q vals\n",
      "tensor([[ 0.0909, -0.2481, -0.0337,  0.1010, -0.1455],\n",
      "        [ 0.0909, -0.2481, -0.0337,  0.1010, -0.1455]])\n",
      "Max target Q vals\n",
      "tensor([0.1010, 0.1010])\n",
      "---------------\n",
      "input tensor agent  1\n",
      "tensor([[0., 3., 3., 2., 2., 3., 3., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0.],\n",
      "        [0., 3., 3., 2., 2., 3., 3., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0.]])\n",
      "Qvals\n",
      "tensor([[ 0.1470,  0.1926, -0.0217,  0.0997,  0.0632],\n",
      "        [ 0.1470,  0.1926, -0.0217,  0.0997,  0.0632]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "taken action\n",
      "tensor([[4],\n",
      "        [4]])\n",
      "Taken Q_val\n",
      "tensor([[0.0632],\n",
      "        [0.0632]], grad_fn=<GatherBackward0>)\n",
      "-----------------\n",
      "target input tensor\n",
      "tensor([[0., 3., 3., 2., 2., 3., 3., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "         0., 0., 1.],\n",
      "        [0., 3., 3., 2., 2., 3., 3., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "         0., 0., 1.]])\n",
      "target Q vals\n",
      "tensor([[ 0.0705, -0.2456, -0.0498,  0.1124, -0.0998],\n",
      "        [ 0.0705, -0.2456, -0.0498,  0.1124, -0.0998]])\n",
      "Max target Q vals\n",
      "tensor([0.1124, 0.1124])\n",
      "---------------\n",
      "input tensor agent  2\n",
      "tensor([[0., 3., 3., 2., 2., 3., 3., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "         0., 0., 1.],\n",
      "        [0., 3., 3., 2., 2., 3., 3., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "         0., 0., 1.]])\n",
      "Qvals\n",
      "tensor([[ 0.1318,  0.1406, -0.0575,  0.0989,  0.0974],\n",
      "        [ 0.1318,  0.1406, -0.0575,  0.0989,  0.0974]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "taken action\n",
      "tensor([[4],\n",
      "        [4]])\n",
      "Taken Q_val\n",
      "tensor([[0.0974],\n",
      "        [0.0974]], grad_fn=<GatherBackward0>)\n",
      "-----------------\n",
      "last target input tensor\n",
      "tensor([[1., 3., 3., 2., 2., 2., 3., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0.],\n",
      "        [1., 3., 3., 2., 2., 2., 3., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0.]])\n",
      "last target Q vals\n",
      "tensor([[ 0.0443, -0.2840,  0.0696,  0.1646, -0.1065],\n",
      "        [ 0.0443, -0.2840,  0.0696,  0.1646, -0.1065]])\n",
      "Max target Q vals\n",
      "tensor([0.1124, 0.1124])\n",
      "rewards, dones, gamma tensor([-0.0010, -0.0010]) tensor([0., 0.]) 0.99\n",
      "GET ACTIONS -------\n",
      "last action sampled:  4\n",
      "last action sampled:  1\n",
      "last action sampled:  1\n",
      "Transition added:  [[2, 3, 3, 2, 3, 3, 3, 1], [2, 3, 3, 2, 3, 3, 3, 1], [2, 3, 3, 2, 3, 3, 3, 1]] [4, 1, 1] [-0.002, 0.001, -0.002] [[2, 2, 3, 2, 3, 3, 3, 1], [2, 2, 3, 2, 3, 3, 3, 1], [2, 2, 3, 2, 3, 3, 3, 1]] [False, False, False]\n",
      "TRAINING STEP --------------\n",
      "---------------\n",
      "input tensor agent  0\n",
      "tensor([[1., 3., 3., 2., 3., 3., 3., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0.],\n",
      "        [0., 3., 2., 1., 1., 3., 3., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0.]])\n",
      "Qvals\n",
      "tensor([[0.1007, 0.0843, 0.0200, 0.1143, 0.0951],\n",
      "        [0.1189, 0.1214, 0.0524, 0.1651, 0.0253]], grad_fn=<AddmmBackward0>)\n",
      "taken action\n",
      "tensor([[1],\n",
      "        [3]])\n",
      "Taken Q_val\n",
      "tensor([[0.0843],\n",
      "        [0.1651]], grad_fn=<GatherBackward0>)\n",
      "-----------------\n",
      "target input tensor\n",
      "tensor([[1., 3., 3., 2., 3., 3., 3., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0.],\n",
      "        [0., 3., 2., 1., 1., 3., 3., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0.]])\n",
      "target Q vals\n",
      "tensor([[ 0.1087, -0.2937, -0.0191,  0.0975, -0.1405],\n",
      "        [ 0.0889, -0.1646, -0.0198,  0.1076, -0.1160]])\n",
      "Max target Q vals\n",
      "tensor([0.1087, 0.1076])\n",
      "---------------\n",
      "input tensor agent  1\n",
      "tensor([[1., 3., 3., 2., 3., 3., 3., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0.],\n",
      "        [0., 3., 2., 1., 1., 3., 3., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0.]])\n",
      "Qvals\n",
      "tensor([[ 0.1419,  0.0908, -0.0028,  0.0998,  0.1095],\n",
      "        [ 0.1224,  0.1517,  0.0014,  0.1289,  0.0768]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "taken action\n",
      "tensor([[1],\n",
      "        [3]])\n",
      "Taken Q_val\n",
      "tensor([[0.0908],\n",
      "        [0.1289]], grad_fn=<GatherBackward0>)\n",
      "-----------------\n",
      "target input tensor\n",
      "tensor([[1., 3., 3., 2., 3., 3., 3., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         0., 0., 1.],\n",
      "        [0., 3., 2., 1., 1., 3., 3., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0.,\n",
      "         0., 0., 1.]])\n",
      "target Q vals\n",
      "tensor([[ 0.0608, -0.2522, -0.0061,  0.0766, -0.1483],\n",
      "        [ 0.0799, -0.1555, -0.0360,  0.0970, -0.0834]])\n",
      "Max target Q vals\n",
      "tensor([0.0766, 0.0970])\n",
      "---------------\n",
      "input tensor agent  2\n",
      "tensor([[1., 3., 3., 2., 3., 3., 3., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         0., 0., 1.],\n",
      "        [0., 3., 2., 1., 1., 3., 3., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0.,\n",
      "         0., 0., 1.]])\n",
      "Qvals\n",
      "tensor([[ 0.1128,  0.0780, -0.0088,  0.1061,  0.0927],\n",
      "        [ 0.1057,  0.0976, -0.0100,  0.1155,  0.0851]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "taken action\n",
      "tensor([[0],\n",
      "        [1]])\n",
      "Taken Q_val\n",
      "tensor([[0.1128],\n",
      "        [0.0976]], grad_fn=<GatherBackward0>)\n",
      "-----------------\n",
      "last target input tensor\n",
      "tensor([[2., 3., 3., 2., 3., 3., 3., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0.],\n",
      "        [0., 3., 2., 2., 2., 3., 3., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0.]])\n",
      "last target Q vals\n",
      "tensor([[ 0.0657, -0.3471,  0.0392,  0.1508, -0.1273],\n",
      "        [ 0.0639, -0.2269,  0.0068,  0.1487, -0.1201]])\n",
      "Max target Q vals\n",
      "tensor([0.0766, 0.0970])\n",
      "rewards, dones, gamma tensor([-0.0010, -0.0020]) tensor([0., 0.]) 0.99\n",
      "GET ACTIONS -------\n",
      "last action sampled:  3\n",
      "last action sampled:  2\n",
      "last action sampled:  0\n",
      "Transition added:  [[2, 2, 3, 2, 3, 3, 3, 1], [2, 2, 3, 2, 3, 3, 3, 1], [2, 2, 3, 2, 3, 3, 3, 1]] [3, 2, 0] [-0.002, -0.002, -0.002] [[2, 3, 2, 2, 3, 3, 3, 1], [2, 3, 2, 2, 3, 3, 3, 1], [2, 3, 2, 2, 3, 3, 3, 1]] [False, False, False]\n",
      "TRAINING STEP --------------\n",
      "---------------\n",
      "input tensor agent  0\n",
      "tensor([[2., 2., 3., 2., 3., 3., 3., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0.],\n",
      "        [0., 3., 2., 2., 2., 3., 3., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0.]])\n",
      "Qvals\n",
      "tensor([[0.1017, 0.0339, 0.0176, 0.1563, 0.1146],\n",
      "        [0.0997, 0.0900, 0.0237, 0.1468, 0.1033]], grad_fn=<AddmmBackward0>)\n",
      "taken action\n",
      "tensor([[3],\n",
      "        [0]])\n",
      "Taken Q_val\n",
      "tensor([[0.1563],\n",
      "        [0.0997]], grad_fn=<GatherBackward0>)\n",
      "-----------------\n",
      "target input tensor\n",
      "tensor([[2., 2., 3., 2., 3., 3., 3., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0.],\n",
      "        [0., 3., 2., 2., 2., 3., 3., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0.]])\n",
      "target Q vals\n",
      "tensor([[ 0.0457, -0.2779, -0.0507,  0.1012, -0.0985],\n",
      "        [ 0.0843, -0.2513, -0.0192,  0.1199, -0.1378]])\n",
      "Max target Q vals\n",
      "tensor([0.1012, 0.1199])\n",
      "---------------\n",
      "input tensor agent  1\n",
      "tensor([[2., 2., 3., 2., 3., 3., 3., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0.],\n",
      "        [0., 3., 2., 2., 2., 3., 3., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0.]])\n",
      "Qvals\n",
      "tensor([[ 0.0885,  0.0616, -0.0272,  0.1014,  0.1788],\n",
      "        [ 0.1235,  0.0844,  0.0117,  0.1088,  0.1113]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "taken action\n",
      "tensor([[2],\n",
      "        [1]])\n",
      "Taken Q_val\n",
      "tensor([[-0.0272],\n",
      "        [ 0.0844]], grad_fn=<GatherBackward0>)\n",
      "-----------------\n",
      "target input tensor\n",
      "tensor([[2., 2., 3., 2., 3., 3., 3., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 1.],\n",
      "        [0., 3., 2., 2., 2., 3., 3., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         0., 0., 1.]])\n",
      "target Q vals\n",
      "tensor([[ 6.7008e-02, -2.9319e-01, -5.4629e-02,  1.0699e-01, -1.0658e-01],\n",
      "        [ 6.2315e-02, -2.2136e-01,  1.5620e-04,  1.3435e-01, -1.2247e-01]])\n",
      "Max target Q vals\n",
      "tensor([0.1070, 0.1344])\n",
      "---------------\n",
      "input tensor agent  2\n",
      "tensor([[2., 2., 3., 2., 3., 3., 3., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 1.],\n",
      "        [0., 3., 2., 2., 2., 3., 3., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         0., 0., 1.]])\n",
      "Qvals\n",
      "tensor([[ 0.1126,  0.0146, -0.0151,  0.1199,  0.1621],\n",
      "        [ 0.1201,  0.0594,  0.0447,  0.1236,  0.0823]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "taken action\n",
      "tensor([[0],\n",
      "        [0]])\n",
      "Taken Q_val\n",
      "tensor([[0.1126],\n",
      "        [0.1201]], grad_fn=<GatherBackward0>)\n",
      "-----------------\n",
      "last target input tensor\n",
      "tensor([[2., 3., 2., 2., 3., 3., 3., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0.],\n",
      "        [0., 3., 3., 2., 2., 3., 3., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0.]])\n",
      "last target Q vals\n",
      "tensor([[ 0.0416, -0.3055,  0.0195,  0.1417, -0.1201],\n",
      "        [ 0.0552, -0.2648,  0.0066,  0.1479, -0.1488]])\n",
      "Max target Q vals\n",
      "tensor([0.1070, 0.1344])\n",
      "rewards, dones, gamma tensor([-0.0020, -0.0010]) tensor([0., 0.]) 0.99\n",
      "GET ACTIONS -------\n",
      "last action sampled:  4\n",
      "last action sampled:  3\n",
      "last action sampled:  2\n",
      "Transition added:  [[2, 3, 2, 2, 3, 3, 3, 1], [2, 3, 2, 2, 3, 3, 3, 1], [2, 3, 2, 2, 3, 3, 3, 1]] [4, 3, 2] [-0.002, -0.002, -0.002] [[2, 3, 2, 2, 3, 3, 3, 1], [2, 3, 2, 2, 3, 3, 3, 1], [2, 3, 2, 2, 3, 3, 3, 1]] [False, False, False]\n",
      "TRAINING STEP --------------\n",
      "---------------\n",
      "input tensor agent  0\n",
      "tensor([[2., 2., 3., 2., 3., 3., 3., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0.],\n",
      "        [0., 3., 2., 2., 2., 3., 3., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0.]])\n",
      "Qvals\n",
      "tensor([[0.1187, 0.0130, 0.0509, 0.1398, 0.1445],\n",
      "        [0.1111, 0.0724, 0.0410, 0.1424, 0.1317]], grad_fn=<AddmmBackward0>)\n",
      "taken action\n",
      "tensor([[3],\n",
      "        [0]])\n",
      "Taken Q_val\n",
      "tensor([[0.1398],\n",
      "        [0.1111]], grad_fn=<GatherBackward0>)\n",
      "-----------------\n",
      "target input tensor\n",
      "tensor([[2., 2., 3., 2., 3., 3., 3., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0.],\n",
      "        [0., 3., 2., 2., 2., 3., 3., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0.]])\n",
      "target Q vals\n",
      "tensor([[ 0.0458, -0.2770, -0.0506,  0.1011, -0.0976],\n",
      "        [ 0.0845, -0.2501, -0.0191,  0.1199, -0.1370]])\n",
      "Max target Q vals\n",
      "tensor([0.1011, 0.1199])\n",
      "---------------\n",
      "input tensor agent  1\n",
      "tensor([[2., 2., 3., 2., 3., 3., 3., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0.],\n",
      "        [0., 3., 2., 2., 2., 3., 3., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0.]])\n",
      "Qvals\n",
      "tensor([[0.1089, 0.0445, 0.0140, 0.0958, 0.2088],\n",
      "        [0.1429, 0.0711, 0.0413, 0.1035, 0.1407]], grad_fn=<AddmmBackward0>)\n",
      "taken action\n",
      "tensor([[2],\n",
      "        [1]])\n",
      "Taken Q_val\n",
      "tensor([[0.0140],\n",
      "        [0.0711]], grad_fn=<GatherBackward0>)\n",
      "-----------------\n",
      "target input tensor\n",
      "tensor([[2., 2., 3., 2., 3., 3., 3., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 1.],\n",
      "        [0., 3., 2., 2., 2., 3., 3., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         0., 0., 1.]])\n",
      "target Q vals\n",
      "tensor([[ 6.7278e-02, -2.9211e-01, -5.4538e-02,  1.0697e-01, -1.0576e-01],\n",
      "        [ 6.2605e-02, -2.2030e-01,  2.7090e-04,  1.3426e-01, -1.2174e-01]])\n",
      "Max target Q vals\n",
      "tensor([0.1070, 0.1343])\n",
      "---------------\n",
      "input tensor agent  2\n",
      "tensor([[2., 2., 3., 2., 3., 3., 3., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 1.],\n",
      "        [0., 3., 2., 2., 2., 3., 3., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         0., 0., 1.]])\n",
      "Qvals\n",
      "tensor([[ 0.1472, -0.0003,  0.0280,  0.1037,  0.1940],\n",
      "        [ 0.1415,  0.0490,  0.0723,  0.1188,  0.1083]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "taken action\n",
      "tensor([[0],\n",
      "        [0]])\n",
      "Taken Q_val\n",
      "tensor([[0.1472],\n",
      "        [0.1415]], grad_fn=<GatherBackward0>)\n",
      "-----------------\n",
      "last target input tensor\n",
      "tensor([[2., 3., 2., 2., 3., 3., 3., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0.],\n",
      "        [0., 3., 3., 2., 2., 3., 3., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0.]])\n",
      "last target Q vals\n",
      "tensor([[ 0.0418, -0.3045,  0.0195,  0.1416, -0.1193],\n",
      "        [ 0.0554, -0.2637,  0.0067,  0.1478, -0.1480]])\n",
      "Max target Q vals\n",
      "tensor([0.1070, 0.1343])\n",
      "rewards, dones, gamma tensor([-0.0020, -0.0010]) tensor([0., 0.]) 0.99\n",
      "GET ACTIONS -------\n",
      "Input Tensor agent  0\n",
      "tensor([[2., 3., 2., 2., 3., 3., 3., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0.]])\n",
      "last action sampled:  4\n",
      "last action sampled:  1\n",
      "last action sampled:  2\n",
      "Transition added:  [[2, 3, 2, 2, 3, 3, 3, 1], [2, 3, 2, 2, 3, 3, 3, 1], [2, 3, 2, 2, 3, 3, 3, 1]] [4, 1, 2] [-0.002, 0.001, -0.002] [[2, 3, 3, 2, 3, 3, 3, 1], [2, 3, 3, 2, 3, 3, 3, 1], [2, 3, 3, 2, 3, 3, 3, 1]] [False, False, False]\n",
      "TRAINING STEP --------------\n",
      "---------------\n",
      "input tensor agent  0\n",
      "tensor([[1., 3., 3., 2., 2., 3., 3., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0.],\n",
      "        [1., 3., 3., 2., 2., 2., 3., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0.]])\n",
      "Qvals\n",
      "tensor([[0.1294, 0.0318, 0.1062, 0.1476, 0.1600],\n",
      "        [0.1200, 0.0128, 0.1303, 0.1289, 0.1700]], grad_fn=<AddmmBackward0>)\n",
      "taken action\n",
      "tensor([[2],\n",
      "        [0]])\n",
      "Taken Q_val\n",
      "tensor([[0.1062],\n",
      "        [0.1200]], grad_fn=<GatherBackward0>)\n",
      "-----------------\n",
      "target input tensor\n",
      "tensor([[1., 3., 3., 2., 2., 3., 3., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0.],\n",
      "        [1., 3., 3., 2., 2., 2., 3., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0.]])\n",
      "target Q vals\n",
      "tensor([[ 0.0315, -0.3311,  0.0025,  0.1998, -0.1140],\n",
      "        [ 0.0658, -0.3324,  0.0342,  0.1155, -0.1175]])\n",
      "Max target Q vals\n",
      "tensor([0.1998, 0.1155])\n",
      "---------------\n",
      "input tensor agent  1\n",
      "tensor([[1., 3., 3., 2., 2., 3., 3., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0.],\n",
      "        [1., 3., 3., 2., 2., 2., 3., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0.]])\n",
      "Qvals\n",
      "tensor([[ 0.1344,  0.0053,  0.1153,  0.1605,  0.2202],\n",
      "        [ 0.1505, -0.0095,  0.1279,  0.0982,  0.1932]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "taken action\n",
      "tensor([[4],\n",
      "        [1]])\n",
      "Taken Q_val\n",
      "tensor([[ 0.2202],\n",
      "        [-0.0095]], grad_fn=<GatherBackward0>)\n",
      "-----------------\n",
      "target input tensor\n",
      "tensor([[1., 3., 3., 2., 2., 3., 3., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1.,\n",
      "         0., 0., 1.],\n",
      "        [1., 3., 3., 2., 2., 2., 3., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         0., 0., 1.]])\n",
      "target Q vals\n",
      "tensor([[ 0.0041, -0.3234, -0.0113,  0.1829, -0.0759],\n",
      "        [ 0.0080, -0.2731,  0.0399,  0.1197, -0.1292]])\n",
      "Max target Q vals\n",
      "tensor([0.1829, 0.1197])\n",
      "---------------\n",
      "input tensor agent  2\n",
      "tensor([[1., 3., 3., 2., 2., 3., 3., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1.,\n",
      "         0., 0., 1.],\n",
      "        [1., 3., 3., 2., 2., 2., 3., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         0., 0., 1.]])\n",
      "Qvals\n",
      "tensor([[ 0.1207, -0.0083,  0.0890,  0.1329,  0.2324],\n",
      "        [ 0.1185,  0.0136,  0.1460,  0.1200,  0.1344]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "taken action\n",
      "tensor([[1],\n",
      "        [3]])\n",
      "Taken Q_val\n",
      "tensor([[-0.0083],\n",
      "        [ 0.1200]], grad_fn=<GatherBackward0>)\n",
      "-----------------\n",
      "last target input tensor\n",
      "tensor([[0., 3., 3., 2., 3., 3., 3., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0.],\n",
      "        [1., 3., 3., 2., 2., 3., 3., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0.]])\n",
      "last target Q vals\n",
      "tensor([[ 0.0824, -0.2519, -0.0016,  0.0990, -0.1123],\n",
      "        [ 0.0450, -0.3066,  0.0337,  0.1835, -0.1368]])\n",
      "Max target Q vals\n",
      "tensor([0.1829, 0.1197])\n",
      "rewards, dones, gamma tensor([-0.0010, -0.0010]) tensor([0., 0.]) 0.99\n",
      "GET ACTIONS -------\n",
      "last action sampled:  3\n",
      "last action sampled:  0\n",
      "last action sampled:  2\n",
      "Transition added:  [[2, 3, 3, 2, 3, 3, 3, 1], [2, 3, 3, 2, 3, 3, 3, 1], [2, 3, 3, 2, 3, 3, 3, 1]] [3, 0, 2] [-0.002, 0.001, -0.002] [[2, 3, 3, 2, 3, 3, 3, 1], [2, 3, 3, 2, 3, 3, 3, 1], [2, 3, 3, 2, 3, 3, 3, 1]] [False, False, False]\n",
      "TRAINING STEP --------------\n",
      "---------------\n",
      "input tensor agent  0\n",
      "tensor([[2., 3., 2., 2., 3., 3., 3., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0.],\n",
      "        [2., 3., 3., 2., 3., 3., 3., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0.]])\n",
      "Qvals\n",
      "tensor([[ 0.1398,  0.0051,  0.1415,  0.1075,  0.1800],\n",
      "        [ 0.1587, -0.0073,  0.1778,  0.1055,  0.1916]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "taken action\n",
      "tensor([[4],\n",
      "        [3]])\n",
      "Taken Q_val\n",
      "tensor([[0.1800],\n",
      "        [0.1055]], grad_fn=<GatherBackward0>)\n",
      "-----------------\n",
      "target input tensor\n",
      "tensor([[2., 3., 2., 2., 3., 3., 3., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0.],\n",
      "        [2., 3., 3., 2., 3., 3., 3., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0.]])\n",
      "target Q vals\n",
      "tensor([[ 0.0553, -0.3115,  0.0050,  0.1318, -0.1498],\n",
      "        [ 0.0618, -0.3058,  0.0065,  0.1069, -0.1035]])\n",
      "Max target Q vals\n",
      "tensor([0.1318, 0.1069])\n",
      "---------------\n",
      "input tensor agent  1\n",
      "tensor([[2., 3., 2., 2., 3., 3., 3., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0.],\n",
      "        [2., 3., 3., 2., 3., 3., 3., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0.]])\n",
      "Qvals\n",
      "tensor([[0.1653, 0.0081, 0.1617, 0.1033, 0.1859],\n",
      "        [0.1516, 0.0208, 0.1573, 0.0899, 0.2418]], grad_fn=<AddmmBackward0>)\n",
      "taken action\n",
      "tensor([[1],\n",
      "        [0]])\n",
      "Taken Q_val\n",
      "tensor([[0.0081],\n",
      "        [0.1516]], grad_fn=<GatherBackward0>)\n",
      "-----------------\n",
      "target input tensor\n",
      "tensor([[2., 3., 2., 2., 3., 3., 3., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0.,\n",
      "         0., 0., 1.],\n",
      "        [2., 3., 3., 2., 3., 3., 3., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0.,\n",
      "         0., 0., 1.]])\n",
      "target Q vals\n",
      "tensor([[ 0.0333, -0.2798,  0.0550,  0.1081, -0.1500],\n",
      "        [-0.0068, -0.2832, -0.0116,  0.0737, -0.0972]])\n",
      "Max target Q vals\n",
      "tensor([0.1081, 0.0737])\n",
      "---------------\n",
      "input tensor agent  2\n",
      "tensor([[2., 3., 2., 2., 3., 3., 3., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0.,\n",
      "         0., 0., 1.],\n",
      "        [2., 3., 3., 2., 3., 3., 3., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0.,\n",
      "         0., 0., 1.]])\n",
      "Qvals\n",
      "tensor([[0.1447, 0.0097, 0.1776, 0.1142, 0.1395],\n",
      "        [0.1041, 0.0067, 0.1389, 0.0780, 0.2422]], grad_fn=<AddmmBackward0>)\n",
      "taken action\n",
      "tensor([[2],\n",
      "        [2]])\n",
      "Taken Q_val\n",
      "tensor([[0.1776],\n",
      "        [0.1389]], grad_fn=<GatherBackward0>)\n",
      "-----------------\n",
      "last target input tensor\n",
      "tensor([[2., 3., 3., 2., 3., 3., 3., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0.],\n",
      "        [2., 3., 3., 2., 3., 3., 3., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0.]])\n",
      "last target Q vals\n",
      "tensor([[ 0.0665, -0.3429,  0.0398,  0.1504, -0.1239],\n",
      "        [ 0.0665, -0.3429,  0.0398,  0.1504, -0.1239]])\n",
      "Max target Q vals\n",
      "tensor([0.1081, 0.0737])\n",
      "rewards, dones, gamma tensor([-0.0010, -0.0010]) tensor([0., 0.]) 0.99\n",
      "GET ACTIONS -------\n",
      "last action sampled:  0\n",
      "last action sampled:  1\n",
      "Input Tensor agent  2\n",
      "tensor([[2., 3., 3., 2., 3., 3., 3., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         0., 0., 1.]])\n",
      "last action sampled:  4\n",
      "Transition added:  [[2, 3, 3, 2, 3, 3, 3, 1], [2, 3, 3, 2, 3, 3, 3, 1], [2, 3, 3, 2, 3, 3, 3, 1]] [0, 1, 4] [-0.002, 0.001, -0.002] [[2, 3, 3, 2, 3, 3, 3, 1], [2, 3, 3, 2, 3, 3, 3, 1], [2, 3, 3, 2, 3, 3, 3, 1]] [False, False, False]\n",
      "TRAINING STEP --------------\n",
      "---------------\n",
      "input tensor agent  0\n",
      "tensor([[1., 3., 3., 2., 2., 2., 3., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0.],\n",
      "        [0., 3., 2., 1., 1., 3., 3., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0.]])\n",
      "Qvals\n",
      "tensor([[0.1239, 0.0390, 0.1990, 0.1210, 0.1816],\n",
      "        [0.1505, 0.0851, 0.1487, 0.1572, 0.1174]], grad_fn=<AddmmBackward0>)\n",
      "taken action\n",
      "tensor([[0],\n",
      "        [3]])\n",
      "Taken Q_val\n",
      "tensor([[0.1239],\n",
      "        [0.1572]], grad_fn=<GatherBackward0>)\n",
      "-----------------\n",
      "target input tensor\n",
      "tensor([[1., 3., 3., 2., 2., 2., 3., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0.],\n",
      "        [0., 3., 2., 1., 1., 3., 3., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0.]])\n",
      "target Q vals\n",
      "tensor([[ 0.0664, -0.3301,  0.0352,  0.1153, -0.1158],\n",
      "        [ 0.0902, -0.1593, -0.0188,  0.1075, -0.1122]])\n",
      "Max target Q vals\n",
      "tensor([0.1153, 0.1075])\n",
      "---------------\n",
      "input tensor agent  1\n",
      "tensor([[1., 3., 3., 2., 2., 2., 3., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0.],\n",
      "        [0., 3., 2., 1., 1., 3., 3., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0.]])\n",
      "Qvals\n",
      "tensor([[0.1529, 0.0391, 0.1960, 0.0948, 0.2040],\n",
      "        [0.1560, 0.1398, 0.1223, 0.1121, 0.1617]], grad_fn=<AddmmBackward0>)\n",
      "taken action\n",
      "tensor([[1],\n",
      "        [3]])\n",
      "Taken Q_val\n",
      "tensor([[0.0391],\n",
      "        [0.1121]], grad_fn=<GatherBackward0>)\n",
      "-----------------\n",
      "target input tensor\n",
      "tensor([[1., 3., 3., 2., 2., 2., 3., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         0., 0., 1.],\n",
      "        [0., 3., 2., 1., 1., 3., 3., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0.,\n",
      "         0., 0., 1.]])\n",
      "target Q vals\n",
      "tensor([[ 0.0087, -0.2710,  0.0406,  0.1197, -0.1277],\n",
      "        [ 0.0809, -0.1514, -0.0348,  0.0970, -0.0799]])\n",
      "Max target Q vals\n",
      "tensor([0.1197, 0.0970])\n",
      "---------------\n",
      "input tensor agent  2\n",
      "tensor([[1., 3., 3., 2., 2., 2., 3., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         0., 0., 1.],\n",
      "        [0., 3., 2., 1., 1., 3., 3., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0.,\n",
      "         0., 0., 1.]])\n",
      "Qvals\n",
      "tensor([[0.1130, 0.0616, 0.2048, 0.1225, 0.1499],\n",
      "        [0.1437, 0.1043, 0.1072, 0.1014, 0.1625]], grad_fn=<AddmmBackward0>)\n",
      "taken action\n",
      "tensor([[3],\n",
      "        [1]])\n",
      "Taken Q_val\n",
      "tensor([[0.1225],\n",
      "        [0.1043]], grad_fn=<GatherBackward0>)\n",
      "-----------------\n",
      "last target input tensor\n",
      "tensor([[1., 3., 3., 2., 2., 3., 3., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0.],\n",
      "        [0., 3., 2., 2., 2., 3., 3., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0.]])\n",
      "last target Q vals\n",
      "tensor([[ 0.0457, -0.3042,  0.0345,  0.1832, -0.1350],\n",
      "        [ 0.0653, -0.2215,  0.0078,  0.1482, -0.1163]])\n",
      "Max target Q vals\n",
      "tensor([0.1197, 0.0970])\n",
      "rewards, dones, gamma tensor([-0.0010, -0.0020]) tensor([0., 0.]) 0.99\n",
      "GET ACTIONS -------\n",
      "last action sampled:  4\n",
      "last action sampled:  3\n",
      "last action sampled:  1\n",
      "Transition added:  [[2, 3, 3, 2, 3, 3, 3, 1], [2, 3, 3, 2, 3, 3, 3, 1], [2, 3, 3, 2, 3, 3, 3, 1]] [4, 3, 1] [-0.002, 0.001, -0.002] [[2, 2, 3, 2, 3, 3, 3, 1], [2, 2, 3, 2, 3, 3, 3, 1], [2, 2, 3, 2, 3, 3, 3, 1]] [False, False, False]\n",
      "TRAINING STEP --------------\n",
      "---------------\n",
      "input tensor agent  0\n",
      "tensor([[0., 3., 2., 1., 1., 3., 3., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0.],\n",
      "        [2., 3., 2., 2., 3., 3., 3., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0.]])\n",
      "Qvals\n",
      "tensor([[0.1424, 0.0988, 0.1594, 0.1544, 0.1202],\n",
      "        [0.1150, 0.0561, 0.1928, 0.1025, 0.1752]], grad_fn=<AddmmBackward0>)\n",
      "taken action\n",
      "tensor([[3],\n",
      "        [4]])\n",
      "Taken Q_val\n",
      "tensor([[0.1544],\n",
      "        [0.1752]], grad_fn=<GatherBackward0>)\n",
      "-----------------\n",
      "target input tensor\n",
      "tensor([[0., 3., 2., 1., 1., 3., 3., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0.],\n",
      "        [2., 3., 2., 2., 3., 3., 3., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0.]])\n",
      "target Q vals\n",
      "tensor([[ 0.0905, -0.1581, -0.0184,  0.1074, -0.1113],\n",
      "        [ 0.0559, -0.3091,  0.0060,  0.1315, -0.1478]])\n",
      "Max target Q vals\n",
      "tensor([0.1074, 0.1315])\n",
      "---------------\n",
      "input tensor agent  1\n",
      "tensor([[0., 3., 2., 1., 1., 3., 3., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0.],\n",
      "        [2., 3., 2., 2., 3., 3., 3., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0.]])\n",
      "Qvals\n",
      "tensor([[0.1487, 0.1616, 0.1388, 0.1043, 0.1621],\n",
      "        [0.1310, 0.0672, 0.2030, 0.0971, 0.1825]], grad_fn=<AddmmBackward0>)\n",
      "taken action\n",
      "tensor([[3],\n",
      "        [3]])\n",
      "Taken Q_val\n",
      "tensor([[0.1043],\n",
      "        [0.0971]], grad_fn=<GatherBackward0>)\n",
      "-----------------\n",
      "target input tensor\n",
      "tensor([[0., 3., 2., 1., 1., 3., 3., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0.,\n",
      "         0., 0., 1.],\n",
      "        [2., 3., 2., 2., 3., 3., 3., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0.,\n",
      "         0., 0., 1.]])\n",
      "target Q vals\n",
      "tensor([[ 0.0811, -0.1505, -0.0343,  0.0970, -0.0791],\n",
      "        [ 0.0258, -0.2990,  0.0098,  0.0827, -0.1305]])\n",
      "Max target Q vals\n",
      "tensor([0.0970, 0.0827])\n",
      "---------------\n",
      "input tensor agent  2\n",
      "tensor([[0., 3., 2., 1., 1., 3., 3., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0.,\n",
      "         0., 0., 1.],\n",
      "        [2., 3., 2., 2., 3., 3., 3., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0.,\n",
      "         0., 0., 1.]])\n",
      "Qvals\n",
      "tensor([[0.1385, 0.1247, 0.1217, 0.0952, 0.1607],\n",
      "        [0.1009, 0.0514, 0.1867, 0.0776, 0.1644]], grad_fn=<AddmmBackward0>)\n",
      "taken action\n",
      "tensor([[1],\n",
      "        [2]])\n",
      "Taken Q_val\n",
      "tensor([[0.1247],\n",
      "        [0.1867]], grad_fn=<GatherBackward0>)\n",
      "-----------------\n",
      "last target input tensor\n",
      "tensor([[0., 3., 2., 2., 2., 3., 3., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0.],\n",
      "        [2., 3., 2., 2., 3., 3., 3., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0.]])\n",
      "last target Q vals\n",
      "tensor([[ 0.0656, -0.2203,  0.0083,  0.1481, -0.1154],\n",
      "        [ 0.0428, -0.3002,  0.0207,  0.1410, -0.1158]])\n",
      "Max target Q vals\n",
      "tensor([0.0970, 0.0827])\n",
      "rewards, dones, gamma tensor([-0.0020, -0.0020]) tensor([0., 0.]) 0.99\n",
      "GET ACTIONS -------\n",
      "last action sampled:  4\n",
      "last action sampled:  0\n",
      "Input Tensor agent  2\n",
      "tensor([[2., 2., 3., 2., 3., 3., 3., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0.,\n",
      "         0., 0., 1.]])\n",
      "last action sampled:  2\n",
      "Transition added:  [[2, 2, 3, 2, 3, 3, 3, 1], [2, 2, 3, 2, 3, 3, 3, 1], [2, 2, 3, 2, 3, 3, 3, 1]] [4, 0, 2] [0.001, 0.001, -0.002] [[2, 1, 3, 2, 2, 3, 3, 1], [2, 1, 3, 2, 2, 3, 3, 1], [2, 1, 3, 2, 2, 3, 3, 1]] [False, False, False]\n",
      "TRAINING STEP --------------\n",
      "---------------\n",
      "input tensor agent  0\n",
      "tensor([[0., 3., 2., 1., 1., 3., 3., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0.],\n",
      "        [2., 3., 2., 2., 3., 3., 3., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0.]])\n",
      "Qvals\n",
      "tensor([[0.1368, 0.1124, 0.1638, 0.1470, 0.1174],\n",
      "        [0.1019, 0.0787, 0.1999, 0.0961, 0.1658]], grad_fn=<AddmmBackward0>)\n",
      "taken action\n",
      "tensor([[3],\n",
      "        [4]])\n",
      "Taken Q_val\n",
      "tensor([[0.1470],\n",
      "        [0.1658]], grad_fn=<GatherBackward0>)\n",
      "-----------------\n",
      "target input tensor\n",
      "tensor([[0., 3., 2., 1., 1., 3., 3., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0.],\n",
      "        [2., 3., 2., 2., 3., 3., 3., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0.]])\n",
      "target Q vals\n",
      "tensor([[ 0.0908, -0.1569, -0.0180,  0.1073, -0.1105],\n",
      "        [ 0.0562, -0.3078,  0.0065,  0.1313, -0.1469]])\n",
      "Max target Q vals\n",
      "tensor([0.1073, 0.1313])\n",
      "---------------\n",
      "input tensor agent  1\n",
      "tensor([[0., 3., 2., 1., 1., 3., 3., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0.],\n",
      "        [2., 3., 2., 2., 3., 3., 3., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0.]])\n",
      "Qvals\n",
      "tensor([[0.1420, 0.1809, 0.1453, 0.0901, 0.1583],\n",
      "        [0.1180, 0.0902, 0.2018, 0.0856, 0.1737]], grad_fn=<AddmmBackward0>)\n",
      "taken action\n",
      "tensor([[3],\n",
      "        [1]])\n",
      "Taken Q_val\n",
      "tensor([[0.0901],\n",
      "        [0.0902]], grad_fn=<GatherBackward0>)\n",
      "-----------------\n",
      "target input tensor\n",
      "tensor([[0., 3., 2., 1., 1., 3., 3., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0.,\n",
      "         0., 0., 1.],\n",
      "        [2., 3., 2., 2., 3., 3., 3., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0.,\n",
      "         0., 0., 1.]])\n",
      "target Q vals\n",
      "tensor([[ 0.0813, -0.1495, -0.0338,  0.0969, -0.0783],\n",
      "        [ 0.0340, -0.2766,  0.0562,  0.1080, -0.1475]])\n",
      "Max target Q vals\n",
      "tensor([0.0969, 0.1080])\n",
      "---------------\n",
      "input tensor agent  2\n",
      "tensor([[0., 3., 2., 1., 1., 3., 3., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0.,\n",
      "         0., 0., 1.],\n",
      "        [2., 3., 2., 2., 3., 3., 3., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0.,\n",
      "         0., 0., 1.]])\n",
      "Qvals\n",
      "tensor([[0.1328, 0.1427, 0.1229, 0.0806, 0.1555],\n",
      "        [0.1048, 0.0815, 0.1962, 0.0989, 0.1229]], grad_fn=<AddmmBackward0>)\n",
      "taken action\n",
      "tensor([[1],\n",
      "        [2]])\n",
      "Taken Q_val\n",
      "tensor([[0.1427],\n",
      "        [0.1962]], grad_fn=<GatherBackward0>)\n",
      "-----------------\n",
      "last target input tensor\n",
      "tensor([[0., 3., 2., 2., 2., 3., 3., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0.],\n",
      "        [2., 3., 3., 2., 3., 3., 3., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0.]])\n",
      "last target Q vals\n",
      "tensor([[ 0.0659, -0.2191,  0.0088,  0.1479, -0.1146],\n",
      "        [ 0.0673, -0.3392,  0.0412,  0.1499, -0.1209]])\n",
      "Max target Q vals\n",
      "tensor([0.0969, 0.1080])\n",
      "rewards, dones, gamma tensor([-0.0020, -0.0010]) tensor([0., 0.]) 0.99\n",
      "GET ACTIONS -------\n",
      "Input Tensor agent  0\n",
      "tensor([[2., 1., 3., 2., 2., 3., 3., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0.]])\n",
      "last action sampled:  4\n",
      "last action sampled:  0\n",
      "last action sampled:  4\n",
      "Transition added:  [[2, 1, 3, 2, 2, 3, 3, 1], [2, 1, 3, 2, 2, 3, 3, 1], [2, 1, 3, 2, 2, 3, 3, 1]] [4, 0, 4] [-0.002, 0.001, -0.002] [[2, 0, 3, 2, 2, 2, 3, 1], [2, 0, 3, 2, 2, 2, 3, 1], [2, 0, 3, 2, 2, 2, 3, 1]] [False, False, False]\n",
      "TRAINING STEP --------------\n",
      "---------------\n",
      "input tensor agent  0\n",
      "tensor([[0., 3., 2., 1., 1., 3., 3., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0.],\n",
      "        [2., 3., 3., 2., 3., 3., 3., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0.]])\n",
      "Qvals\n",
      "tensor([[0.1321, 0.1256, 0.1605, 0.1323, 0.1112],\n",
      "        [0.1124, 0.0844, 0.2240, 0.0770, 0.1711]], grad_fn=<AddmmBackward0>)\n",
      "taken action\n",
      "tensor([[3],\n",
      "        [3]])\n",
      "Taken Q_val\n",
      "tensor([[0.1323],\n",
      "        [0.0770]], grad_fn=<GatherBackward0>)\n",
      "-----------------\n",
      "target input tensor\n",
      "tensor([[0., 3., 2., 1., 1., 3., 3., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0.],\n",
      "        [2., 3., 3., 2., 3., 3., 3., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0.]])\n",
      "target Q vals\n",
      "tensor([[ 0.0910, -0.1556, -0.0175,  0.1072, -0.1096],\n",
      "        [ 0.0625, -0.3010,  0.0088,  0.1063, -0.0994]])\n",
      "Max target Q vals\n",
      "tensor([0.1072, 0.1063])\n",
      "---------------\n",
      "input tensor agent  1\n",
      "tensor([[0., 3., 2., 1., 1., 3., 3., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0.],\n",
      "        [2., 3., 3., 2., 3., 3., 3., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0.]])\n",
      "Qvals\n",
      "tensor([[0.1373, 0.1974, 0.1426, 0.0761, 0.1537],\n",
      "        [0.0928, 0.1289, 0.1961, 0.0595, 0.2291]], grad_fn=<AddmmBackward0>)\n",
      "taken action\n",
      "tensor([[3],\n",
      "        [0]])\n",
      "Taken Q_val\n",
      "tensor([[0.0761],\n",
      "        [0.0928]], grad_fn=<GatherBackward0>)\n",
      "-----------------\n",
      "target input tensor\n",
      "tensor([[0., 3., 2., 1., 1., 3., 3., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0.,\n",
      "         0., 0., 1.],\n",
      "        [2., 3., 3., 2., 3., 3., 3., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0.,\n",
      "         0., 0., 1.]])\n",
      "target Q vals\n",
      "tensor([[ 0.0815, -0.1485, -0.0334,  0.0968, -0.0776],\n",
      "        [-0.0062, -0.2787, -0.0097,  0.0730, -0.0932]])\n",
      "Max target Q vals\n",
      "tensor([0.0968, 0.0730])\n",
      "---------------\n",
      "input tensor agent  2\n",
      "tensor([[0., 3., 2., 1., 1., 3., 3., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0.,\n",
      "         0., 0., 1.],\n",
      "        [2., 3., 3., 2., 3., 3., 3., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0.,\n",
      "         0., 0., 1.]])\n",
      "Qvals\n",
      "tensor([[0.1273, 0.1556, 0.1150, 0.0683, 0.1504],\n",
      "        [0.0435, 0.0977, 0.1608, 0.0500, 0.2099]], grad_fn=<AddmmBackward0>)\n",
      "taken action\n",
      "tensor([[1],\n",
      "        [2]])\n",
      "Taken Q_val\n",
      "tensor([[0.1556],\n",
      "        [0.1608]], grad_fn=<GatherBackward0>)\n",
      "-----------------\n",
      "last target input tensor\n",
      "tensor([[0., 3., 2., 2., 2., 3., 3., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0.],\n",
      "        [2., 3., 3., 2., 3., 3., 3., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0.]])\n",
      "last target Q vals\n",
      "tensor([[ 0.0662, -0.2178,  0.0092,  0.1477, -0.1138],\n",
      "        [ 0.0675, -0.3379,  0.0417,  0.1496, -0.1200]])\n",
      "Max target Q vals\n",
      "tensor([0.0968, 0.0730])\n",
      "rewards, dones, gamma tensor([-0.0020, -0.0010]) tensor([0., 0.]) 0.99\n",
      "GET ACTIONS -------\n",
      "last action sampled:  3\n",
      "last action sampled:  2\n",
      "last action sampled:  2\n",
      "Transition added:  [[2, 0, 3, 2, 2, 2, 3, 1], [2, 0, 3, 2, 2, 2, 3, 1], [2, 0, 3, 2, 2, 2, 3, 1]] [3, 2, 2] [0.001, 0.001, -0.002] [[2, 1, 3, 2, 1, 2, 3, 1], [2, 1, 3, 2, 1, 2, 3, 1], [2, 1, 3, 2, 1, 2, 3, 1]] [False, False, False]\n",
      "TRAINING STEP --------------\n",
      "---------------\n",
      "input tensor agent  0\n",
      "tensor([[2., 0., 3., 2., 2., 2., 3., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0.],\n",
      "        [1., 3., 3., 2., 3., 3., 3., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0.]])\n",
      "Qvals\n",
      "tensor([[0.1293, 0.0860, 0.0929, 0.0728, 0.1366],\n",
      "        [0.1007, 0.1110, 0.1978, 0.0528, 0.1783]], grad_fn=<AddmmBackward0>)\n",
      "taken action\n",
      "tensor([[3],\n",
      "        [1]])\n",
      "Taken Q_val\n",
      "tensor([[0.0728],\n",
      "        [0.1110]], grad_fn=<GatherBackward0>)\n",
      "-----------------\n",
      "target input tensor\n",
      "tensor([[2., 0., 3., 2., 2., 2., 3., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0.],\n",
      "        [1., 3., 3., 2., 3., 3., 3., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0.]])\n",
      "target Q vals\n",
      "tensor([[ 0.0469, -0.2131, -0.1061,  0.0650, -0.0467],\n",
      "        [ 0.1108, -0.2834, -0.0154,  0.0966, -0.1319]])\n",
      "Max target Q vals\n",
      "tensor([0.0650, 0.1108])\n",
      "---------------\n",
      "input tensor agent  1\n",
      "tensor([[2., 0., 3., 2., 2., 2., 3., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0.],\n",
      "        [1., 3., 3., 2., 3., 3., 3., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0.]])\n",
      "Qvals\n",
      "tensor([[0.1062, 0.1612, 0.0645, 0.0195, 0.2382],\n",
      "        [0.1573, 0.1417, 0.1774, 0.0472, 0.1945]], grad_fn=<AddmmBackward0>)\n",
      "taken action\n",
      "tensor([[2],\n",
      "        [1]])\n",
      "Taken Q_val\n",
      "tensor([[0.0645],\n",
      "        [0.1417]], grad_fn=<GatherBackward0>)\n",
      "-----------------\n",
      "target input tensor\n",
      "tensor([[2., 0., 3., 2., 2., 2., 3., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 1.],\n",
      "        [1., 3., 3., 2., 3., 3., 3., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         0., 0., 1.]])\n",
      "target Q vals\n",
      "tensor([[ 0.0446, -0.2055, -0.1049,  0.0980, -0.1008],\n",
      "        [ 0.0633, -0.2429, -0.0029,  0.0761, -0.1407]])\n",
      "Max target Q vals\n",
      "tensor([0.0980, 0.0761])\n",
      "---------------\n",
      "input tensor agent  2\n",
      "tensor([[2., 0., 3., 2., 2., 2., 3., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 1.],\n",
      "        [1., 3., 3., 2., 3., 3., 3., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         0., 0., 1.]])\n",
      "Qvals\n",
      "tensor([[0.0917, 0.1036, 0.0416, 0.0516, 0.1300],\n",
      "        [0.1448, 0.1399, 0.1446, 0.0695, 0.1627]], grad_fn=<AddmmBackward0>)\n",
      "taken action\n",
      "tensor([[2],\n",
      "        [0]])\n",
      "Taken Q_val\n",
      "tensor([[0.0416],\n",
      "        [0.1448]], grad_fn=<GatherBackward0>)\n",
      "-----------------\n",
      "last target input tensor\n",
      "tensor([[2., 1., 3., 2., 1., 2., 3., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0.],\n",
      "        [2., 3., 3., 2., 3., 3., 3., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0.]])\n",
      "last target Q vals\n",
      "tensor([[ 0.0463, -0.3030, -0.0189,  0.2043, -0.0927],\n",
      "        [ 0.0677, -0.3365,  0.0422,  0.1493, -0.1191]])\n",
      "Max target Q vals\n",
      "tensor([0.0980, 0.0761])\n",
      "rewards, dones, gamma tensor([ 0.0000, -0.0010]) tensor([0., 0.]) 0.99\n",
      "GET ACTIONS -------\n",
      "Input Tensor agent  0\n",
      "tensor([[2., 1., 3., 2., 1., 2., 3., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0.]])\n",
      "last action sampled:  2\n",
      "Input Tensor agent  1\n",
      "tensor([[2., 1., 3., 2., 1., 2., 3., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0.]])\n",
      "last action sampled:  4\n",
      "last action sampled:  2\n",
      "Transition added:  [[2, 1, 3, 2, 1, 2, 3, 1], [2, 1, 3, 2, 1, 2, 3, 1], [2, 1, 3, 2, 1, 2, 3, 1]] [2, 4, 2] [-0.002, 0.001, -0.002] [[1, 1, 3, 2, 0, 2, 3, 1], [1, 1, 3, 2, 0, 2, 3, 1], [1, 1, 3, 2, 0, 2, 3, 1]] [False, False, False]\n",
      "TRAINING STEP --------------\n",
      "---------------\n",
      "input tensor agent  0\n",
      "tensor([[2., 3., 3., 2., 3., 3., 3., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0.],\n",
      "        [2., 3., 3., 2., 3., 3., 3., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0.]])\n",
      "Qvals\n",
      "tensor([[0.0946, 0.1018, 0.2107, 0.0666, 0.1532],\n",
      "        [0.0946, 0.1018, 0.2107, 0.0666, 0.1532]], grad_fn=<AddmmBackward0>)\n",
      "taken action\n",
      "tensor([[0],\n",
      "        [0]])\n",
      "Taken Q_val\n",
      "tensor([[0.0946],\n",
      "        [0.0946]], grad_fn=<GatherBackward0>)\n",
      "-----------------\n",
      "target input tensor\n",
      "tensor([[2., 3., 3., 2., 3., 3., 3., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0.],\n",
      "        [2., 3., 3., 2., 3., 3., 3., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0.]])\n",
      "target Q vals\n",
      "tensor([[ 0.0679, -0.3792, -0.0117,  0.1228, -0.1408],\n",
      "        [ 0.0679, -0.3792, -0.0117,  0.1228, -0.1408]])\n",
      "Max target Q vals\n",
      "tensor([0.1228, 0.1228])\n",
      "---------------\n",
      "input tensor agent  1\n",
      "tensor([[2., 3., 3., 2., 3., 3., 3., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0.],\n",
      "        [2., 3., 3., 2., 3., 3., 3., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0.]])\n",
      "Qvals\n",
      "tensor([[0.0990, 0.1238, 0.2097, 0.0497, 0.2013],\n",
      "        [0.0990, 0.1238, 0.2097, 0.0497, 0.2013]], grad_fn=<AddmmBackward0>)\n",
      "taken action\n",
      "tensor([[1],\n",
      "        [1]])\n",
      "Taken Q_val\n",
      "tensor([[0.1238],\n",
      "        [0.1238]], grad_fn=<GatherBackward0>)\n",
      "-----------------\n",
      "target input tensor\n",
      "tensor([[2., 3., 3., 2., 3., 3., 3., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         0., 0., 1.],\n",
      "        [2., 3., 3., 2., 3., 3., 3., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         0., 0., 1.]])\n",
      "target Q vals\n",
      "tensor([[ 0.0247, -0.3247,  0.0184,  0.1068, -0.1430],\n",
      "        [ 0.0247, -0.3247,  0.0184,  0.1068, -0.1430]])\n",
      "Max target Q vals\n",
      "tensor([0.1068, 0.1068])\n",
      "---------------\n",
      "input tensor agent  2\n",
      "tensor([[2., 3., 3., 2., 3., 3., 3., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         0., 0., 1.],\n",
      "        [2., 3., 3., 2., 3., 3., 3., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         0., 0., 1.]])\n",
      "Qvals\n",
      "tensor([[0.1108, 0.1147, 0.2108, 0.0994, 0.1722],\n",
      "        [0.1108, 0.1147, 0.2108, 0.0994, 0.1722]], grad_fn=<AddmmBackward0>)\n",
      "taken action\n",
      "tensor([[4],\n",
      "        [4]])\n",
      "Taken Q_val\n",
      "tensor([[0.1722],\n",
      "        [0.1722]], grad_fn=<GatherBackward0>)\n",
      "-----------------\n",
      "last target input tensor\n",
      "tensor([[2., 3., 3., 2., 3., 3., 3., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0.],\n",
      "        [2., 3., 3., 2., 3., 3., 3., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0.]])\n",
      "last target Q vals\n",
      "tensor([[ 0.0678, -0.3351,  0.0427,  0.1490, -0.1182],\n",
      "        [ 0.0678, -0.3351,  0.0427,  0.1490, -0.1182]])\n",
      "Max target Q vals\n",
      "tensor([0.1068, 0.1068])\n",
      "rewards, dones, gamma tensor([-0.0010, -0.0010]) tensor([0., 0.]) 0.99\n",
      "GET ACTIONS -------\n",
      "last action sampled:  1\n",
      "last action sampled:  4\n",
      "last action sampled:  1\n",
      "Transition added:  [[1, 1, 3, 2, 0, 2, 3, 1], [1, 1, 3, 2, 0, 2, 3, 1], [1, 1, 3, 2, 0, 2, 3, 1]] [1, 4, 1] [0.001, 0.001, -0.002] [[2, 1, 3, 2, 1, 2, 3, 1], [2, 1, 3, 2, 1, 2, 3, 1], [2, 1, 3, 2, 1, 2, 3, 1]] [False, False, False]\n",
      "TRAINING STEP --------------\n",
      "---------------\n",
      "input tensor agent  0\n",
      "tensor([[0., 3., 2., 1., 1., 3., 3., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0.],\n",
      "        [2., 3., 3., 2., 3., 3., 3., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0.]])\n",
      "Qvals\n",
      "tensor([[0.1287, 0.1378, 0.1666, 0.1123, 0.0990],\n",
      "        [0.1112, 0.0986, 0.2243, 0.0693, 0.1455]], grad_fn=<AddmmBackward0>)\n",
      "taken action\n",
      "tensor([[1],\n",
      "        [4]])\n",
      "Taken Q_val\n",
      "tensor([[0.1378],\n",
      "        [0.1455]], grad_fn=<GatherBackward0>)\n",
      "-----------------\n",
      "target input tensor\n",
      "tensor([[0., 3., 2., 1., 1., 3., 3., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0.],\n",
      "        [2., 3., 3., 2., 3., 3., 3., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0.]])\n",
      "target Q vals\n",
      "tensor([[ 0.1031, -0.1471, -0.0028,  0.1080, -0.1302],\n",
      "        [ 0.0685, -0.3561,  0.0197,  0.1355, -0.1494]])\n",
      "Max target Q vals\n",
      "tensor([0.1080, 0.1355])\n",
      "---------------\n",
      "input tensor agent  1\n",
      "tensor([[0., 3., 2., 1., 1., 3., 3., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0.],\n",
      "        [2., 3., 3., 2., 3., 3., 3., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0.]])\n",
      "Qvals\n",
      "tensor([[0.1763, 0.1858, 0.1533, 0.0803, 0.1389],\n",
      "        [0.1244, 0.1017, 0.2190, 0.0654, 0.1580]], grad_fn=<AddmmBackward0>)\n",
      "taken action\n",
      "tensor([[1],\n",
      "        [1]])\n",
      "Taken Q_val\n",
      "tensor([[0.1858],\n",
      "        [0.1017]], grad_fn=<GatherBackward0>)\n",
      "-----------------\n",
      "target input tensor\n",
      "tensor([[0., 3., 2., 1., 1., 3., 3., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         0., 0., 1.],\n",
      "        [2., 3., 3., 2., 3., 3., 3., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0.,\n",
      "         0., 0., 1.]])\n",
      "target Q vals\n",
      "tensor([[ 0.0829, -0.1452,  0.0095,  0.1338, -0.1088],\n",
      "        [ 0.0310, -0.3082,  0.0550,  0.1074, -0.1543]])\n",
      "Max target Q vals\n",
      "tensor([0.1338, 0.1074])\n",
      "---------------\n",
      "input tensor agent  2\n",
      "tensor([[0., 3., 2., 1., 1., 3., 3., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         0., 0., 1.],\n",
      "        [2., 3., 3., 2., 3., 3., 3., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0.,\n",
      "         0., 0., 1.]])\n",
      "Qvals\n",
      "tensor([[0.1780, 0.1477, 0.1453, 0.1001, 0.1021],\n",
      "        [0.1270, 0.0901, 0.2100, 0.0877, 0.1041]], grad_fn=<AddmmBackward0>)\n",
      "taken action\n",
      "tensor([[0],\n",
      "        [1]])\n",
      "Taken Q_val\n",
      "tensor([[0.1780],\n",
      "        [0.0901]], grad_fn=<GatherBackward0>)\n",
      "-----------------\n",
      "last target input tensor\n",
      "tensor([[0., 3., 2., 1., 1., 3., 3., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0.],\n",
      "        [2., 2., 3., 2., 3., 3., 3., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0.]])\n",
      "last target Q vals\n",
      "tensor([[ 0.0738, -0.2022,  0.0346,  0.1618, -0.1420],\n",
      "        [ 0.0660, -0.3054,  0.0130,  0.1585, -0.1097]])\n",
      "Max target Q vals\n",
      "tensor([0.1338, 0.1074])\n",
      "rewards, dones, gamma tensor([-0.0010, -0.0010]) tensor([0., 0.]) 0.99\n",
      "GET ACTIONS -------\n",
      "last action sampled:  1\n",
      "Input Tensor agent  1\n",
      "tensor([[2., 1., 3., 2., 1., 2., 3., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0.]])\n",
      "last action sampled:  2\n",
      "last action sampled:  1\n",
      "Transition added:  [[2, 1, 3, 2, 1, 2, 3, 1], [2, 1, 3, 2, 1, 2, 3, 1], [2, 1, 3, 2, 1, 2, 3, 1]] [1, 2, 1] [0.001, -0.002, -0.002] [[2, 1, 2, 2, 1, 2, 3, 1], [2, 1, 2, 2, 1, 2, 3, 1], [2, 1, 2, 2, 1, 2, 3, 1]] [False, False, False]\n",
      "TRAINING STEP --------------\n",
      "---------------\n",
      "input tensor agent  0\n",
      "tensor([[2., 2., 3., 2., 3., 3., 3., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0.],\n",
      "        [1., 3., 3., 2., 2., 3., 3., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0.]])\n",
      "Qvals\n",
      "tensor([[0.1252, 0.0934, 0.2207, 0.0662, 0.1348],\n",
      "        [0.1153, 0.1156, 0.2337, 0.0967, 0.1318]], grad_fn=<AddmmBackward0>)\n",
      "taken action\n",
      "tensor([[3],\n",
      "        [2]])\n",
      "Taken Q_val\n",
      "tensor([[0.0662],\n",
      "        [0.2337]], grad_fn=<GatherBackward0>)\n",
      "-----------------\n",
      "target input tensor\n",
      "tensor([[2., 2., 3., 2., 3., 3., 3., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0.],\n",
      "        [1., 3., 3., 2., 2., 3., 3., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0.]])\n",
      "target Q vals\n",
      "tensor([[ 0.0475, -0.2653, -0.0445,  0.0997, -0.0881],\n",
      "        [ 0.0345, -0.3179,  0.0078,  0.1983, -0.1055]])\n",
      "Max target Q vals\n",
      "tensor([0.0997, 0.1983])\n",
      "---------------\n",
      "input tensor agent  1\n",
      "tensor([[2., 2., 3., 2., 3., 3., 3., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0.],\n",
      "        [1., 3., 3., 2., 2., 3., 3., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0.]])\n",
      "Qvals\n",
      "tensor([[0.1014, 0.1407, 0.1929, 0.0629, 0.1946],\n",
      "        [0.1136, 0.1128, 0.2261, 0.1413, 0.1688]], grad_fn=<AddmmBackward0>)\n",
      "taken action\n",
      "tensor([[2],\n",
      "        [4]])\n",
      "Taken Q_val\n",
      "tensor([[0.1929],\n",
      "        [0.1688]], grad_fn=<GatherBackward0>)\n",
      "-----------------\n",
      "target input tensor\n",
      "tensor([[2., 2., 3., 2., 3., 3., 3., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 1.],\n",
      "        [1., 3., 3., 2., 2., 3., 3., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1.,\n",
      "         0., 0., 1.]])\n",
      "target Q vals\n",
      "tensor([[ 0.0704, -0.2799, -0.0491,  0.1053, -0.0965],\n",
      "        [ 0.0067, -0.3118, -0.0069,  0.1811, -0.0678]])\n",
      "Max target Q vals\n",
      "tensor([0.1053, 0.1811])\n",
      "---------------\n",
      "input tensor agent  2\n",
      "tensor([[2., 2., 3., 2., 3., 3., 3., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 1.],\n",
      "        [1., 3., 3., 2., 2., 3., 3., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1.,\n",
      "         0., 0., 1.]])\n",
      "Qvals\n",
      "tensor([[0.1212, 0.0970, 0.1696, 0.0618, 0.1531],\n",
      "        [0.1017, 0.0942, 0.1747, 0.1327, 0.1872]], grad_fn=<AddmmBackward0>)\n",
      "taken action\n",
      "tensor([[0],\n",
      "        [1]])\n",
      "Taken Q_val\n",
      "tensor([[0.1212],\n",
      "        [0.0942]], grad_fn=<GatherBackward0>)\n",
      "-----------------\n",
      "last target input tensor\n",
      "tensor([[2., 3., 2., 2., 3., 3., 3., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0.],\n",
      "        [0., 3., 3., 2., 3., 3., 3., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0.]])\n",
      "last target Q vals\n",
      "tensor([[ 0.0438, -0.2922,  0.0232,  0.1394, -0.1109],\n",
      "        [ 0.0845, -0.2409,  0.0032,  0.0969, -0.1043]])\n",
      "Max target Q vals\n",
      "tensor([0.1053, 0.1811])\n",
      "rewards, dones, gamma tensor([-0.0020, -0.0010]) tensor([0., 0.]) 0.99\n",
      "GET ACTIONS -------\n",
      "Input Tensor agent  0\n",
      "tensor([[2., 1., 2., 2., 1., 2., 3., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0.]])\n",
      "last action sampled:  2\n",
      "last action sampled:  0\n",
      "Input Tensor agent  2\n",
      "tensor([[2., 1., 2., 2., 1., 2., 3., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0.,\n",
      "         0., 0., 1.]])\n",
      "last action sampled:  2\n",
      "Transition added:  [[2, 1, 2, 2, 1, 2, 3, 1], [2, 1, 2, 2, 1, 2, 3, 1], [2, 1, 2, 2, 1, 2, 3, 1]] [2, 0, 2] [-0.002, -0.002, -0.002] [[1, 1, 2, 2, 0, 2, 3, 1], [1, 1, 2, 2, 0, 2, 3, 1], [1, 1, 2, 2, 0, 2, 3, 1]] [False, False, False]\n",
      "TRAINING STEP --------------\n",
      "---------------\n",
      "input tensor agent  0\n",
      "tensor([[2., 1., 2., 2., 1., 2., 3., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0.],\n",
      "        [2., 3., 3., 2., 3., 3., 3., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0.]])\n",
      "Qvals\n",
      "tensor([[0.1261, 0.1020, 0.1917, 0.1087, 0.1112],\n",
      "        [0.1322, 0.0930, 0.2216, 0.0804, 0.1204]], grad_fn=<AddmmBackward0>)\n",
      "taken action\n",
      "tensor([[2],\n",
      "        [4]])\n",
      "Taken Q_val\n",
      "tensor([[0.1917],\n",
      "        [0.1204]], grad_fn=<GatherBackward0>)\n",
      "-----------------\n",
      "target input tensor\n",
      "tensor([[2., 1., 2., 2., 1., 2., 3., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0.],\n",
      "        [2., 3., 3., 2., 3., 3., 3., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0.]])\n",
      "target Q vals\n",
      "tensor([[ 0.0468, -0.3129, -0.0588,  0.1784, -0.0552],\n",
      "        [ 0.0689, -0.3532,  0.0210,  0.1350, -0.1476]])\n",
      "Max target Q vals\n",
      "tensor([0.1784, 0.1350])\n",
      "---------------\n",
      "input tensor agent  1\n",
      "tensor([[2., 1., 2., 2., 1., 2., 3., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0.],\n",
      "        [2., 3., 3., 2., 3., 3., 3., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0.]])\n",
      "Qvals\n",
      "tensor([[0.1311, 0.1040, 0.1797, 0.0924, 0.1538],\n",
      "        [0.1432, 0.1051, 0.2053, 0.0679, 0.1361]], grad_fn=<AddmmBackward0>)\n",
      "taken action\n",
      "tensor([[0],\n",
      "        [3]])\n",
      "Taken Q_val\n",
      "tensor([[0.1311],\n",
      "        [0.0679]], grad_fn=<GatherBackward0>)\n",
      "-----------------\n",
      "target input tensor\n",
      "tensor([[2., 1., 2., 2., 1., 2., 3., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0.,\n",
      "         0., 0., 1.],\n",
      "        [2., 3., 3., 2., 3., 3., 3., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0.,\n",
      "         0., 0., 1.]])\n",
      "target Q vals\n",
      "tensor([[-0.0273, -0.3101, -0.0747,  0.1444, -0.0593],\n",
      "        [ 0.0447, -0.3231,  0.0255,  0.0947, -0.1419]])\n",
      "Max target Q vals\n",
      "tensor([0.1444, 0.0947])\n",
      "---------------\n",
      "input tensor agent  2\n",
      "tensor([[2., 1., 2., 2., 1., 2., 3., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0.,\n",
      "         0., 0., 1.],\n",
      "        [2., 3., 3., 2., 3., 3., 3., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0.,\n",
      "         0., 0., 1.]])\n",
      "Qvals\n",
      "tensor([[0.0687, 0.0459, 0.1517, 0.0626, 0.1104],\n",
      "        [0.1257, 0.0869, 0.1824, 0.0758, 0.1036]], grad_fn=<AddmmBackward0>)\n",
      "taken action\n",
      "tensor([[2],\n",
      "        [1]])\n",
      "Taken Q_val\n",
      "tensor([[0.1517],\n",
      "        [0.0869]], grad_fn=<GatherBackward0>)\n",
      "-----------------\n",
      "last target input tensor\n",
      "tensor([[1., 1., 2., 2., 0., 2., 3., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0.],\n",
      "        [2., 2., 3., 2., 3., 3., 3., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0.]])\n",
      "last target Q vals\n",
      "tensor([[ 0.0280, -0.2308, -0.0393,  0.1899, -0.0993],\n",
      "        [ 0.0665, -0.3028,  0.0143,  0.1579, -0.1080]])\n",
      "Max target Q vals\n",
      "tensor([0.1444, 0.0947])\n",
      "rewards, dones, gamma tensor([-0.0020, -0.0010]) tensor([0., 0.]) 0.99\n",
      "GET ACTIONS -------\n",
      "last action sampled:  0\n",
      "last action sampled:  0\n",
      "last action sampled:  3\n",
      "Transition added:  [[1, 1, 2, 2, 0, 2, 3, 1], [1, 1, 2, 2, 0, 2, 3, 1], [1, 1, 2, 2, 0, 2, 3, 1]] [0, 0, 3] [-0.002, -0.002, -0.002] [[1, 1, 2, 2, 0, 3, 3, 1], [1, 1, 2, 2, 0, 3, 3, 1], [1, 1, 2, 2, 0, 3, 3, 1]] [False, False, False]\n",
      "TRAINING STEP --------------\n",
      "---------------\n",
      "input tensor agent  0\n",
      "tensor([[2., 3., 3., 2., 3., 3., 3., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0.],\n",
      "        [1., 1., 3., 2., 0., 2., 3., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0.]])\n",
      "Qvals\n",
      "tensor([[0.1468, 0.1003, 0.2072, 0.0900, 0.1187],\n",
      "        [0.1493, 0.1677, 0.1586, 0.1141, 0.1325]], grad_fn=<AddmmBackward0>)\n",
      "taken action\n",
      "tensor([[4],\n",
      "        [1]])\n",
      "Taken Q_val\n",
      "tensor([[0.1187],\n",
      "        [0.1677]], grad_fn=<GatherBackward0>)\n",
      "-----------------\n",
      "target input tensor\n",
      "tensor([[2., 3., 3., 2., 3., 3., 3., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0.],\n",
      "        [1., 1., 3., 2., 0., 2., 3., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0.]])\n",
      "target Q vals\n",
      "tensor([[ 0.0691, -0.3517,  0.0215,  0.1347, -0.1467],\n",
      "        [ 0.0723, -0.2283, -0.0713,  0.0688, -0.0824]])\n",
      "Max target Q vals\n",
      "tensor([0.1347, 0.0723])\n",
      "---------------\n",
      "input tensor agent  1\n",
      "tensor([[2., 3., 3., 2., 3., 3., 3., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0.],\n",
      "        [1., 1., 3., 2., 0., 2., 3., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0.]])\n",
      "Qvals\n",
      "tensor([[0.1578, 0.1146, 0.1927, 0.0799, 0.1366],\n",
      "        [0.2045, 0.1938, 0.1798, 0.0253, 0.1965]], grad_fn=<AddmmBackward0>)\n",
      "taken action\n",
      "tensor([[1],\n",
      "        [4]])\n",
      "Taken Q_val\n",
      "tensor([[0.1146],\n",
      "        [0.1965]], grad_fn=<GatherBackward0>)\n",
      "-----------------\n",
      "target input tensor\n",
      "tensor([[2., 3., 3., 2., 3., 3., 3., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0.,\n",
      "         0., 0., 1.],\n",
      "        [1., 1., 3., 2., 0., 2., 3., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "         0., 0., 1.]])\n",
      "target Q vals\n",
      "tensor([[ 0.0319, -0.3041,  0.0567,  0.1067, -0.1519],\n",
      "        [ 0.0320, -0.2148, -0.0955,  0.0945, -0.0729]])\n",
      "Max target Q vals\n",
      "tensor([0.1067, 0.0945])\n",
      "---------------\n",
      "input tensor agent  2\n",
      "tensor([[2., 3., 3., 2., 3., 3., 3., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0.,\n",
      "         0., 0., 1.],\n",
      "        [1., 1., 3., 2., 0., 2., 3., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "         0., 0., 1.]])\n",
      "Qvals\n",
      "tensor([[0.1556, 0.1115, 0.1797, 0.1015, 0.0858],\n",
      "        [0.1506, 0.1397, 0.1201, 0.0165, 0.1607]], grad_fn=<AddmmBackward0>)\n",
      "taken action\n",
      "tensor([[1],\n",
      "        [1]])\n",
      "Taken Q_val\n",
      "tensor([[0.1115],\n",
      "        [0.1397]], grad_fn=<GatherBackward0>)\n",
      "-----------------\n",
      "last target input tensor\n",
      "tensor([[2., 2., 3., 2., 3., 3., 3., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0.],\n",
      "        [2., 1., 3., 2., 1., 2., 3., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0.]])\n",
      "last target Q vals\n",
      "tensor([[ 0.0669, -0.3015,  0.0149,  0.1577, -0.1072],\n",
      "        [ 0.0480, -0.2966, -0.0161,  0.2033, -0.0889]])\n",
      "Max target Q vals\n",
      "tensor([0.1067, 0.0945])\n",
      "rewards, dones, gamma tensor([-0.0010,  0.0000]) tensor([0., 0.]) 0.99\n",
      "GET ACTIONS -------\n",
      "last action sampled:  4\n",
      "last action sampled:  0\n",
      "last action sampled:  4\n",
      "Transition added:  [[1, 1, 2, 2, 0, 3, 3, 1], [1, 1, 2, 2, 0, 3, 3, 1], [1, 1, 2, 2, 0, 3, 3, 1]] [4, 0, 4] [-0.002, -0.002, -0.002] [[1, 0, 2, 2, 0, 2, 3, 1], [1, 0, 2, 2, 0, 2, 3, 1], [1, 0, 2, 2, 0, 2, 3, 1]] [False, False, False]\n",
      "TRAINING STEP --------------\n",
      "---------------\n",
      "input tensor agent  0\n",
      "tensor([[1., 3., 3., 2., 3., 3., 3., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0.],\n",
      "        [1., 1., 2., 2., 0., 3., 3., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0.]])\n",
      "Qvals\n",
      "tensor([[0.1498, 0.1094, 0.1825, 0.0745, 0.1340],\n",
      "        [0.1632, 0.1493, 0.1586, 0.0837, 0.0782]], grad_fn=<AddmmBackward0>)\n",
      "taken action\n",
      "tensor([[1],\n",
      "        [4]])\n",
      "Taken Q_val\n",
      "tensor([[0.1094],\n",
      "        [0.0782]], grad_fn=<GatherBackward0>)\n",
      "-----------------\n",
      "target input tensor\n",
      "tensor([[1., 3., 3., 2., 3., 3., 3., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0.],\n",
      "        [1., 1., 2., 2., 0., 3., 3., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0.]])\n",
      "target Q vals\n",
      "tensor([[ 0.1121, -0.2756, -0.0115,  0.0956, -0.1263],\n",
      "        [ 0.0173, -0.2386, -0.0760,  0.1197, -0.1685]])\n",
      "Max target Q vals\n",
      "tensor([0.1121, 0.1197])\n",
      "---------------\n",
      "input tensor agent  1\n",
      "tensor([[1., 3., 3., 2., 3., 3., 3., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0.],\n",
      "        [1., 1., 2., 2., 0., 3., 3., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0.]])\n",
      "Qvals\n",
      "tensor([[0.2084, 0.1302, 0.1810, 0.0702, 0.1535],\n",
      "        [0.1679, 0.1880, 0.1574, 0.0496, 0.0512]], grad_fn=<AddmmBackward0>)\n",
      "taken action\n",
      "tensor([[1],\n",
      "        [0]])\n",
      "Taken Q_val\n",
      "tensor([[0.1302],\n",
      "        [0.1679]], grad_fn=<GatherBackward0>)\n",
      "-----------------\n",
      "target input tensor\n",
      "tensor([[1., 3., 3., 2., 3., 3., 3., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         0., 0., 1.],\n",
      "        [1., 1., 2., 2., 0., 3., 3., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0.,\n",
      "         0., 0., 1.]])\n",
      "target Q vals\n",
      "tensor([[ 0.0650, -0.2358,  0.0005,  0.0755, -0.1359],\n",
      "        [-0.0280, -0.2309, -0.0694,  0.1038, -0.1425]])\n",
      "Max target Q vals\n",
      "tensor([0.0755, 0.1038])\n",
      "---------------\n",
      "input tensor agent  2\n",
      "tensor([[1., 3., 3., 2., 3., 3., 3., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         0., 0., 1.],\n",
      "        [1., 1., 2., 2., 0., 3., 3., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0.,\n",
      "         0., 0., 1.]])\n",
      "Qvals\n",
      "tensor([[0.1978, 0.1397, 0.1577, 0.0886, 0.1251],\n",
      "        [0.0999, 0.1696, 0.1799, 0.0211, 0.0334]], grad_fn=<AddmmBackward0>)\n",
      "taken action\n",
      "tensor([[0],\n",
      "        [4]])\n",
      "Taken Q_val\n",
      "tensor([[0.1978],\n",
      "        [0.0334]], grad_fn=<GatherBackward0>)\n",
      "-----------------\n",
      "last target input tensor\n",
      "tensor([[2., 3., 3., 2., 3., 3., 3., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0.],\n",
      "        [1., 0., 2., 2., 0., 2., 3., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0.]])\n",
      "last target Q vals\n",
      "tensor([[ 0.0691, -0.3281,  0.0453,  0.1479, -0.1139],\n",
      "        [ 0.0588, -0.1847, -0.0679,  0.1368, -0.1027]])\n",
      "Max target Q vals\n",
      "tensor([0.0755, 0.1038])\n",
      "rewards, dones, gamma tensor([-0.0010, -0.0020]) tensor([0., 0.]) 0.99\n",
      "GET ACTIONS -------\n",
      "last action sampled:  1\n",
      "last action sampled:  0\n",
      "last action sampled:  3\n",
      "Transition added:  [[1, 0, 2, 2, 0, 2, 3, 1], [1, 0, 2, 2, 0, 2, 3, 1], [1, 0, 2, 2, 0, 2, 3, 1]] [1, 0, 3] [-0.002, -0.002, -0.002] [[2, 0, 2, 2, 0, 3, 3, 1], [2, 0, 2, 2, 0, 3, 3, 1], [2, 0, 2, 2, 0, 3, 3, 1]] [False, False, False]\n",
      "TRAINING STEP --------------\n",
      "---------------\n",
      "input tensor agent  0\n",
      "tensor([[1., 1., 3., 2., 0., 2., 3., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0.],\n",
      "        [0., 3., 2., 1., 1., 3., 3., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0.]])\n",
      "Qvals\n",
      "tensor([[0.1412, 0.1489, 0.1583, 0.1285, 0.1068],\n",
      "        [0.1360, 0.1279, 0.1543, 0.1242, 0.0717]], grad_fn=<AddmmBackward0>)\n",
      "taken action\n",
      "tensor([[1],\n",
      "        [1]])\n",
      "Taken Q_val\n",
      "tensor([[0.1489],\n",
      "        [0.1279]], grad_fn=<GatherBackward0>)\n",
      "-----------------\n",
      "target input tensor\n",
      "tensor([[1., 1., 3., 2., 0., 2., 3., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0.],\n",
      "        [0., 3., 2., 1., 1., 3., 3., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0.]])\n",
      "target Q vals\n",
      "tensor([[ 0.0729, -0.2257, -0.0699,  0.0685, -0.0806],\n",
      "        [ 0.1047, -0.1407, -0.0003,  0.1074, -0.1263]])\n",
      "Max target Q vals\n",
      "tensor([0.0729, 0.1074])\n",
      "---------------\n",
      "input tensor agent  1\n",
      "tensor([[1., 1., 3., 2., 0., 2., 3., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0.],\n",
      "        [0., 3., 2., 1., 1., 3., 3., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0.]])\n",
      "Qvals\n",
      "tensor([[0.1773, 0.1834, 0.1795, 0.0426, 0.1589],\n",
      "        [0.1777, 0.1643, 0.1371, 0.0919, 0.1123]], grad_fn=<AddmmBackward0>)\n",
      "taken action\n",
      "tensor([[4],\n",
      "        [1]])\n",
      "Taken Q_val\n",
      "tensor([[0.1589],\n",
      "        [0.1643]], grad_fn=<GatherBackward0>)\n",
      "-----------------\n",
      "target input tensor\n",
      "tensor([[1., 1., 3., 2., 0., 2., 3., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "         0., 0., 1.],\n",
      "        [0., 3., 2., 1., 1., 3., 3., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         0., 0., 1.]])\n",
      "target Q vals\n",
      "tensor([[ 0.0324, -0.2124, -0.0942,  0.0941, -0.0712],\n",
      "        [ 0.0843, -0.1401,  0.0116,  0.1335, -0.1057]])\n",
      "Max target Q vals\n",
      "tensor([0.0941, 0.1335])\n",
      "---------------\n",
      "input tensor agent  2\n",
      "tensor([[1., 1., 3., 2., 0., 2., 3., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "         0., 0., 1.],\n",
      "        [0., 3., 2., 1., 1., 3., 3., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         0., 0., 1.]])\n",
      "Qvals\n",
      "tensor([[0.1280, 0.1494, 0.1227, 0.0371, 0.1568],\n",
      "        [0.1701, 0.1323, 0.1239, 0.1111, 0.0775]], grad_fn=<AddmmBackward0>)\n",
      "taken action\n",
      "tensor([[1],\n",
      "        [0]])\n",
      "Taken Q_val\n",
      "tensor([[0.1494],\n",
      "        [0.1701]], grad_fn=<GatherBackward0>)\n",
      "-----------------\n",
      "last target input tensor\n",
      "tensor([[2., 1., 3., 2., 1., 2., 3., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0.],\n",
      "        [0., 3., 2., 1., 1., 3., 3., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0.]])\n",
      "last target Q vals\n",
      "tensor([[ 0.0488, -0.2941, -0.0148,  0.2030, -0.0876],\n",
      "        [ 0.0750, -0.1963,  0.0367,  0.1609, -0.1386]])\n",
      "Max target Q vals\n",
      "tensor([0.0941, 0.1335])\n",
      "rewards, dones, gamma tensor([ 0.0000, -0.0010]) tensor([0., 0.]) 0.99\n",
      "GET ACTIONS -------\n",
      "last action sampled:  4\n",
      "last action sampled:  0\n",
      "last action sampled:  4\n",
      "Transition added:  [[2, 0, 2, 2, 0, 3, 3, 1], [2, 0, 2, 2, 0, 3, 3, 1], [2, 0, 2, 2, 0, 3, 3, 1]] [4, 0, 4] [-0.002, -0.002, -0.002] [[2, 0, 2, 2, 0, 2, 3, 1], [2, 0, 2, 2, 0, 2, 3, 1], [2, 0, 2, 2, 0, 2, 3, 1]] [False, False, False]\n",
      "TRAINING STEP --------------\n",
      "---------------\n",
      "input tensor agent  0\n",
      "tensor([[1., 1., 2., 2., 0., 3., 3., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0.],\n",
      "        [2., 3., 3., 2., 3., 3., 3., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0.]])\n",
      "Qvals\n",
      "tensor([[0.1217, 0.1269, 0.1547, 0.0922, 0.0768],\n",
      "        [0.1312, 0.1022, 0.1849, 0.1035, 0.1124]], grad_fn=<AddmmBackward0>)\n",
      "taken action\n",
      "tensor([[4],\n",
      "        [4]])\n",
      "Taken Q_val\n",
      "tensor([[0.0768],\n",
      "        [0.1124]], grad_fn=<GatherBackward0>)\n",
      "-----------------\n",
      "target input tensor\n",
      "tensor([[1., 1., 2., 2., 0., 3., 3., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0.],\n",
      "        [2., 3., 3., 2., 3., 3., 3., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0.]])\n",
      "target Q vals\n",
      "tensor([[ 0.0179, -0.2357, -0.0744,  0.1192, -0.1670],\n",
      "        [ 0.0698, -0.3472,  0.0231,  0.1343, -0.1441]])\n",
      "Max target Q vals\n",
      "tensor([0.1192, 0.1343])\n",
      "---------------\n",
      "input tensor agent  1\n",
      "tensor([[1., 1., 2., 2., 0., 3., 3., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0.],\n",
      "        [2., 3., 3., 2., 3., 3., 3., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0.]])\n",
      "Qvals\n",
      "tensor([[0.1076, 0.1609, 0.1542, 0.0466, 0.0560],\n",
      "        [0.1215, 0.1163, 0.1605, 0.0927, 0.1262]], grad_fn=<AddmmBackward0>)\n",
      "taken action\n",
      "tensor([[0],\n",
      "        [3]])\n",
      "Taken Q_val\n",
      "tensor([[0.1076],\n",
      "        [0.0927]], grad_fn=<GatherBackward0>)\n",
      "-----------------\n",
      "target input tensor\n",
      "tensor([[1., 1., 2., 2., 0., 3., 3., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0.,\n",
      "         0., 0., 1.],\n",
      "        [2., 3., 3., 2., 3., 3., 3., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0.,\n",
      "         0., 0., 1.]])\n",
      "target Q vals\n",
      "tensor([[-0.0275, -0.2284, -0.0679,  0.1033, -0.1411],\n",
      "        [ 0.0458, -0.3173,  0.0276,  0.0940, -0.1385]])\n",
      "Max target Q vals\n",
      "tensor([0.1033, 0.0940])\n",
      "---------------\n",
      "input tensor agent  2\n",
      "tensor([[1., 1., 2., 2., 0., 3., 3., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0.,\n",
      "         0., 0., 1.],\n",
      "        [2., 3., 3., 2., 3., 3., 3., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0.,\n",
      "         0., 0., 1.]])\n",
      "Qvals\n",
      "tensor([[0.0565, 0.1581, 0.1845, 0.0183, 0.0704],\n",
      "        [0.1047, 0.1063, 0.1525, 0.0864, 0.1003]], grad_fn=<AddmmBackward0>)\n",
      "taken action\n",
      "tensor([[4],\n",
      "        [1]])\n",
      "Taken Q_val\n",
      "tensor([[0.0704],\n",
      "        [0.1063]], grad_fn=<GatherBackward0>)\n",
      "-----------------\n",
      "last target input tensor\n",
      "tensor([[1., 0., 2., 2., 0., 2., 3., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0.],\n",
      "        [2., 2., 3., 2., 3., 3., 3., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0.]])\n",
      "last target Q vals\n",
      "tensor([[ 0.0592, -0.1826, -0.0666,  0.1363, -0.1015],\n",
      "        [ 0.0678, -0.2977,  0.0165,  0.1571, -0.1048]])\n",
      "Max target Q vals\n",
      "tensor([0.1033, 0.0940])\n",
      "rewards, dones, gamma tensor([-0.0020, -0.0010]) tensor([0., 0.]) 0.99\n",
      "GET ACTIONS -------\n",
      "last action sampled:  3\n",
      "last action sampled:  1\n",
      "last action sampled:  0\n",
      "Transition added:  [[2, 0, 2, 2, 0, 2, 3, 1], [2, 0, 2, 2, 0, 2, 3, 1], [2, 0, 2, 2, 0, 2, 3, 1]] [3, 1, 0] [0.001, 0.001, -0.002] [[2, 1, 3, 2, 0, 2, 3, 1], [2, 1, 3, 2, 0, 2, 3, 1], [2, 1, 3, 2, 0, 2, 3, 1]] [False, False, False]\n",
      "TRAINING STEP --------------\n",
      "---------------\n",
      "input tensor agent  0\n",
      "tensor([[2., 3., 3., 2., 3., 3., 3., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0.],\n",
      "        [2., 3., 2., 2., 3., 3., 3., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0.]])\n",
      "Qvals\n",
      "tensor([[0.1154, 0.1016, 0.1808, 0.1024, 0.1216],\n",
      "        [0.0837, 0.1154, 0.1400, 0.1044, 0.1015]], grad_fn=<AddmmBackward0>)\n",
      "taken action\n",
      "tensor([[4],\n",
      "        [4]])\n",
      "Taken Q_val\n",
      "tensor([[0.1216],\n",
      "        [0.1015]], grad_fn=<GatherBackward0>)\n",
      "-----------------\n",
      "target input tensor\n",
      "tensor([[2., 3., 3., 2., 3., 3., 3., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0.],\n",
      "        [2., 3., 2., 2., 3., 3., 3., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0.]])\n",
      "target Q vals\n",
      "tensor([[ 0.0699, -0.3458,  0.0237,  0.1341, -0.1432],\n",
      "        [ 0.0583, -0.2921,  0.0116,  0.1291, -0.1377]])\n",
      "Max target Q vals\n",
      "tensor([0.1341, 0.1291])\n",
      "---------------\n",
      "input tensor agent  1\n",
      "tensor([[2., 3., 3., 2., 3., 3., 3., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0.],\n",
      "        [2., 3., 2., 2., 3., 3., 3., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0.]])\n",
      "Qvals\n",
      "tensor([[0.1072, 0.1206, 0.1553, 0.0907, 0.1399],\n",
      "        [0.0814, 0.1364, 0.1300, 0.0876, 0.1150]], grad_fn=<AddmmBackward0>)\n",
      "taken action\n",
      "tensor([[1],\n",
      "        [1]])\n",
      "Taken Q_val\n",
      "tensor([[0.1206],\n",
      "        [0.1364]], grad_fn=<GatherBackward0>)\n",
      "-----------------\n",
      "target input tensor\n",
      "tensor([[2., 3., 3., 2., 3., 3., 3., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0.,\n",
      "         0., 0., 1.],\n",
      "        [2., 3., 2., 2., 3., 3., 3., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0.,\n",
      "         0., 0., 1.]])\n",
      "target Q vals\n",
      "tensor([[ 0.0330, -0.2987,  0.0584,  0.1062, -0.1487],\n",
      "        [ 0.0360, -0.2629,  0.0597,  0.1069, -0.1397]])\n",
      "Max target Q vals\n",
      "tensor([0.1062, 0.1069])\n",
      "---------------\n",
      "input tensor agent  2\n",
      "tensor([[2., 3., 3., 2., 3., 3., 3., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0.,\n",
      "         0., 0., 1.],\n",
      "        [2., 3., 2., 2., 3., 3., 3., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0.,\n",
      "         0., 0., 1.]])\n",
      "Qvals\n",
      "tensor([[0.1030, 0.1225, 0.1482, 0.1044, 0.0990],\n",
      "        [0.0694, 0.1205, 0.1214, 0.0996, 0.0735]], grad_fn=<AddmmBackward0>)\n",
      "taken action\n",
      "tensor([[1],\n",
      "        [2]])\n",
      "Taken Q_val\n",
      "tensor([[0.1225],\n",
      "        [0.1214]], grad_fn=<GatherBackward0>)\n",
      "-----------------\n",
      "last target input tensor\n",
      "tensor([[2., 2., 3., 2., 3., 3., 3., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0.],\n",
      "        [2., 3., 3., 2., 3., 3., 3., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0.]])\n",
      "last target Q vals\n",
      "tensor([[ 0.0680, -0.2964,  0.0170,  0.1569, -0.1039],\n",
      "        [ 0.0698, -0.3240,  0.0468,  0.1474, -0.1115]])\n",
      "Max target Q vals\n",
      "tensor([0.1062, 0.1069])\n",
      "rewards, dones, gamma tensor([-0.0010, -0.0010]) tensor([0., 0.]) 0.99\n",
      "GET ACTIONS -------\n",
      "last action sampled:  2\n",
      "last action sampled:  2\n",
      "last action sampled:  2\n",
      "Transition added:  [[2, 1, 3, 2, 0, 2, 3, 1], [2, 1, 3, 2, 0, 2, 3, 1], [2, 1, 3, 2, 0, 2, 3, 1]] [2, 2, 2] [-0.002, -0.002, -0.002] [[1, 1, 2, 2, 0, 2, 3, 1], [1, 1, 2, 2, 0, 2, 3, 1], [1, 1, 2, 2, 0, 2, 3, 1]] [False, False, False]\n",
      "TRAINING STEP --------------\n",
      "---------------\n",
      "input tensor agent  0\n",
      "tensor([[2., 2., 3., 2., 3., 3., 3., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0.],\n",
      "        [1., 3., 3., 2., 2., 3., 3., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0.]])\n",
      "Qvals\n",
      "tensor([[0.1079, 0.1069, 0.1678, 0.0970, 0.1439],\n",
      "        [0.0924, 0.1146, 0.1922, 0.1186, 0.1414]], grad_fn=<AddmmBackward0>)\n",
      "taken action\n",
      "tensor([[4],\n",
      "        [2]])\n",
      "Taken Q_val\n",
      "tensor([[0.1439],\n",
      "        [0.1922]], grad_fn=<GatherBackward0>)\n",
      "-----------------\n",
      "target input tensor\n",
      "tensor([[2., 2., 3., 2., 3., 3., 3., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0.],\n",
      "        [1., 3., 3., 2., 2., 3., 3., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0.]])\n",
      "target Q vals\n",
      "tensor([[ 0.0674, -0.3237, -0.0185,  0.1196, -0.1284],\n",
      "        [ 0.0365, -0.3072,  0.0126,  0.1974, -0.1000]])\n",
      "Max target Q vals\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m sequential_DQN \u001b[38;5;241m=\u001b[39m seqDQN(env, eps_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m10\u001b[39m, layer_sizes \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m64\u001b[39m), tau \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0025\u001b[39m, buffer_max_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100000\u001b[39m, batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m, global_observations \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, log_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtensorboard_logs_seqDQN_hard3\u001b[39m\u001b[38;5;124m\"\u001b[39m) \n\u001b[0;32m----> 3\u001b[0m rewards, losses \u001b[38;5;241m=\u001b[39m \u001b[43msequential_DQN\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m25\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# save model\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# sequential_DQN.shared_DQN.save(\"models\", \"seqDQN_hard3\")\u001b[39;00m\n",
      "File \u001b[0;32m~/masterThesis/masterThesis/shared_seq_DQN.py:329\u001b[0m, in \u001b[0;36mseqDQN.train\u001b[0;34m(self, num_episodes)\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplay_buffer\u001b[38;5;241m.\u001b[39madd_transition(obs, actions, rewards, next_obs, terminals)\n\u001b[1;32m    328\u001b[0m \u001b[38;5;66;03m# learning step\u001b[39;00m\n\u001b[0;32m--> 329\u001b[0m status, losses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[38;5;66;03m# update state\u001b[39;00m\n\u001b[1;32m    332\u001b[0m obs \u001b[38;5;241m=\u001b[39m next_obs\n",
      "File \u001b[0;32m~/masterThesis/masterThesis/shared_seq_DQN.py:183\u001b[0m, in \u001b[0;36mseqDQN.learn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28mprint\u001b[39m(Q_vals)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMax target Q vals\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 183\u001b[0m max_Q_next_agent \u001b[38;5;241m=\u001b[39m \u001b[43mQ_vals\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mvalues\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28mprint\u001b[39m(max_Q_next_agent)\n\u001b[1;32m    185\u001b[0m \u001b[38;5;66;03m# Q_next_agent = self.shared_target_DQN(targ_input_tensor).max(1).values\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \n\u001b[1;32m    187\u001b[0m \u001b[38;5;66;03m# we learn from the next agent Qvals only, with diminished learning rate\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sequential_DQN = seqDQN(env, eps_steps = 100 * 10, layer_sizes = (64, 64), tau = 0.0025, buffer_max_size = 100000, batch_size = 2, global_observations = True, log_dir = \"tensorboard_logs_seqDQN_hard3\") \n",
    "\n",
    "rewards, losses = sequential_DQN.train(25)\n",
    "\n",
    "# save model\n",
    "# sequential_DQN.shared_DQN.save(\"models\", \"seqDQN_hard3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequential_DQN.shared_DQN.save(\"models\", \"seqDQN_hard3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "model = seqDQN(env, layer_sizes = (64,64), global_observations = True)\n",
    "model.shared_DQN.load(\"models/seqDQN_hard3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['O' ' ' ' ' 'X']\n",
      " [' ' ' ' ' ' ' ']\n",
      " ['X' ' ' 'X' ' ']\n",
      " [' ' ' ' ' ' ' ']]\n",
      "start ----------------------------\n",
      "[[' ' ' ' 'X' ' ']\n",
      " ['X' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ']\n",
      " [' ' ' ' 'X' 'O']]\n",
      "['left', 'left', 'down']\n",
      "[[' ' ' ' ' ' ' ']\n",
      " ['X' ' ' 'X' ' ']\n",
      " [' ' ' ' ' ' ' ']\n",
      " [' ' 'X' ' ' 'O']]\n",
      "['left', 'left', 'right']\n",
      "[[' ' ' ' ' ' ' ']\n",
      " ['X' ' ' ' ' 'X']\n",
      " [' ' ' ' ' ' ' ']\n",
      " ['X' ' ' ' ' 'O']]\n",
      "['down', 'down', 'down']\n",
      "[[' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ']\n",
      " ['X' ' ' ' ' 'X']\n",
      " ['X' ' ' ' ' 'O']]\n",
      "['right', 'right', 'right']\n",
      "[[' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ']\n",
      " [' ' 'X' ' ' 'X']\n",
      " [' ' 'X' ' ' 'O']]\n",
      "['right', 'right', 'down']\n",
      "[[' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ']\n",
      " [' ' ' ' 'X' 'X']\n",
      " [' ' ' ' 'X' 'O']]\n"
     ]
    }
   ],
   "source": [
    "env = SpiderFlyEnvMA(size = 4, spiders = 3, max_timesteps = 100, render_mode = \"ascii\")\n",
    "env = PettingZooWrapper(env)\n",
    "\n",
    "terminal = False\n",
    "print(\"start ----------------------------\")\n",
    "obs, _ = env.reset()\n",
    "while not terminal:\n",
    "    actions = []\n",
    "    actions = model.get_actions(obs, deterministic = True)\n",
    "    print([env.action_to_direction_string[act] for act in actions])\n",
    "\n",
    "    obs, rewards, terminals, truncations, infos = env.step(actions)\n",
    "    if rewards[0] == 1:\n",
    "        break\n",
    "    # print(obs, rewards, terminals, truncations, infos, actions)\n",
    "    # print(\"--------------------\")\n",
    "    \n",
    "    terminal = terminals[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
