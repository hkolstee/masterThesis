{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shared_DQN import SharedDQN\n",
    "from spider_fly_env.envs.grid_MA_pettingzoo_testing import SpiderFlyEnvMA\n",
    "from spider_fly_env.wrappers.pettingzoo_wrapper import PettingZooWrapper\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DQN (Shared parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ' ' ' 'X' 'X' ' ' 'O' 'O']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([2, 5, 6, 6])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = SpiderFlyEnvMA(render_mode = \"ascii\")\n",
    "env = PettingZooWrapper(env)\n",
    "\n",
    "env.observation_space[0].sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = SpiderFlyEnvMA(max_steps = 200)\n",
    "env = PettingZooWrapper(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DQN = SharedDQN(env, eps_steps = 50000, layer_sizes = (64, 64), tau = 0.0025, buffer_max_size = 100000) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 must have the same dtype, but got Double and Float",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m rewards, losses \u001b[38;5;241m=\u001b[39m \u001b[43mDQN\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/masterThesis/masterThesis/shared_DQN.py:142\u001b[0m, in \u001b[0;36mSharedDQN.train\u001b[0;34m(self, num_episodes)\u001b[0m\n\u001b[1;32m    140\u001b[0m actions \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m agent_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnr_agents):\n\u001b[0;32m--> 142\u001b[0m     actions\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m[\u001b[49m\u001b[43magent_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magent_idx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    143\u001b[0m \u001b[38;5;66;03m# take action\u001b[39;00m\n\u001b[1;32m    144\u001b[0m next_obs, rewards, terminals, truncations, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(actions)\n",
      "File \u001b[0;32m~/masterThesis/masterThesis/shared_DQN.py:80\u001b[0m, in \u001b[0;36mSharedDQN.get_action\u001b[0;34m(self, observation, agent_idx, deterministic)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;66;03m# clear potentially left-over gradients\u001b[39;00m\n\u001b[1;32m     79\u001b[0m one_hot_id\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 80\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshared_DQN\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mone_hot_id\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39margmax()\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/masterThesis/masterThesis/custom_agents/networks/critic_discrete.py:35\u001b[0m, in \u001b[0;36mCritic.forward\u001b[0;34m(self, obs)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, obs):\n\u001b[0;32m---> 35\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/masterThesis/masterThesis/custom_agents/networks/MLP.py:82\u001b[0m, in \u001b[0;36mMultiLayerPerceptron.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnr_layers \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (idx \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnr_layers):\n\u001b[1;32m     81\u001b[0m         \u001b[38;5;66;03m# pass through first and middle layers\u001b[39;00m\n\u001b[0;32m---> 82\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m    \n\u001b[1;32m     83\u001b[0m         data \u001b[38;5;241m=\u001b[39m functional\u001b[38;5;241m.\u001b[39mrelu(data)\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     85\u001b[0m         \u001b[38;5;66;03m# last layer(s)\u001b[39;00m\n\u001b[1;32m     86\u001b[0m         \u001b[38;5;66;03m# if multiple last layers\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 must have the same dtype, but got Double and Float"
     ]
    }
   ],
   "source": [
    "rewards, losses = DQN.train(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = np.vstack(rewards)\n",
    "data2 = np.vstack(losses)\n",
    "\n",
    "df1 = pd.DataFrame(data1, columns = [\"agent_1\", \"agent_2\"])\n",
    "df1[\"Episode\"] = list(range(data1.shape[0]))\n",
    "\n",
    "df2 = pd.DataFrame(data2, columns = [\"agent_1\", \"agent_2\"])\n",
    "df2[\"Episode\"] = list(range(data1.shape[0]))\n",
    "\n",
    "df1 = df1.melt('Episode', var_name='Agent', value_name='Rewards')\n",
    "df2 = df2.melt('Episode', var_name='Agent', value_name='Rewards')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need 1.5.0 for rolling average of next step\n",
    "pd.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df1)\n",
    "display(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1[\"Avg_Reward\"] = df1[\"Rewards\"].rolling(window = 5, step = 5).mean()\n",
    "df1 = df1[df1.Episode > 5]\n",
    "df1.dropna()\n",
    "\n",
    "df2[\"Avg_Loss\"] = df2[\"Rewards\"].rolling(window = 5, step = 5).mean()\n",
    "df2 = df2[df2.Episode > 5]\n",
    "df2.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.title(\"Rewards\")\n",
    "sns.lineplot(data = df1, x = \"Episode\", y = \"Avg_Reward\")\n",
    "plt.title(\"Rewards\")\n",
    "plt.figure()\n",
    "sns.lineplot(data = df1, x = \"Episode\", y = \"Avg_Reward\", hue = \"Agent\")\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Losses\")\n",
    "sns.lineplot(data = df2, x = \"Episode\", y = \"Avg_Loss\")\n",
    "plt.figure()\n",
    "plt.title(\"Losses\")\n",
    "sns.lineplot(data = df2, x = \"Episode\", y = \"Avg_Loss\", hue = \"Agent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sequential Q-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tabular Q-learning:\n",
    "$$\n",
    "\\begin{align*}\n",
    "    & s = env.reset()\\\\\n",
    "    &\\text{while not } done:\\\\\n",
    "    & \\quad\\quad \\text{for } i \\text{ in } (1, \\ldots, m):\\\\\n",
    "    & \\quad\\quad\\quad\\quad a_i = \\argmax_{a_i} Q_i(s_i, a_i)\\\\\n",
    "    & \\quad\\quad s', r, d = env.step(a_1, \\ldots, a_m)\\\\\n",
    "    & \\quad\\quad \\text{for } i \\text{ in } (1, \\ldots, m):\\\\\n",
    "    & \\quad\\quad\\quad\\quad Q_i(s, a_i) = Q_i(s, a_i) + lr * ((mean(r) + \\gamma * \\max_{a'_i} Q_i(s', a'_i)) - Q_i(s, a_i))\\\\\n",
    "\\end{align*}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sequential Tabular Q-learning:\n",
    "$$\n",
    "\\begin{align*}\n",
    "    & s = env.reset()\\\\\n",
    "    &\\text{while not } done:\\\\\n",
    "    & \\quad\\quad \\text{for } i \\text{ in } (1, \\ldots, m):\\\\\n",
    "    & \\quad\\quad\\quad\\quad a_i = \\argmax_{a_i} Q_i(s, a_1, \\ldots, a_i)\\\\\n",
    "    & \\quad\\quad s', r, d = env.step(a_1, \\ldots, a_m)\\\\\n",
    "    & \\quad\\quad \\text{for } i \\text{ in } (1, \\ldots, m-1):\\\\\n",
    "    & \\quad\\quad\\quad\\quad Q_i(s, a_1, \\ldots, a_i) = Q_i(s, a_1, \\ldots, a_i) + (i/m) * lr * (\\max_{a_{i+1}} Q_{i+1}(s, a_1, \\ldots, a_{i+1}) - Q_i(s, a_1, \\ldots, a_i))\\\\\n",
    "    & \\quad\\quad Q_m(s, a_1, \\ldots, a_m) = Q_m(s, a_1, \\ldots, a_m) + lr * ((mean(r) + \\gamma * \\max_{a'_1} Q_1(s', a'_1)) - Q_m(s, a_1, \\ldots, a_m))\\\\\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data1 = np.vstack(rewards)\n",
    "# data2 = np.vstack(losses)\n",
    "\n",
    "# df1 = pd.DataFrame(data1, columns = [\"agent_1\", \"agent_2\"])\n",
    "# df1[\"Episode\"] = list(range(data1.shape[0]))\n",
    "\n",
    "# df2 = pd.DataFrame(data2, columns = [\"agent_1\", \"agent_2\"])\n",
    "# df2[\"Episode\"] = list(range(data1.shape[0]))\n",
    "\n",
    "# df1 = df1.melt('Episode', var_name='Agent', value_name='Rewards')\n",
    "# df2 = df2.melt('Episode', var_name='Agent', value_name='Rewards')\n",
    "# df1[\"Avg_Reward\"] = df1[\"Rewards\"].rolling(window = 10, step = 10).mean()\n",
    "# df1 = df1[df1.Episode > 10]\n",
    "# df1.dropna()\n",
    "\n",
    "# df2[\"Avg_Loss\"] = df2[\"Rewards\"].rolling(window = 10, step = 10).mean()\n",
    "# df2 = df2[df2.Episode > 10]\n",
    "# df2.dropna()\n",
    "\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.title(\"Rewards\")\n",
    "# sns.lineplot(data = df1, x = \"Episode\", y = \"Avg_Reward\")\n",
    "# plt.title(\"Rewards\")\n",
    "# plt.figure()\n",
    "# sns.lineplot(data = df1, x = \"Episode\", y = \"Avg_Reward\", hue = \"Agent\")\n",
    "\n",
    "# plt.figure()\n",
    "# plt.title(\"Losses\")\n",
    "# sns.lineplot(data = df2, x = \"Episode\", y = \"Avg_Loss\")\n",
    "# plt.figure()\n",
    "# plt.title(\"Losses\")\n",
    "# sns.lineplot(data = df2, x = \"Episode\", y = \"Avg_Loss\", hue = \"Agent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-23 22:20:57.043854: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[' ' ' ' ' ']\n",
      " [' ' 'X' ' ']\n",
      " ['X' ' ' 'O']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hkolstee/.local/lib/python3.10/site-packages/gymnasium/envs/registration.py:694: UserWarning: \u001b[33mWARN: Overriding environment SpiderFlyGrid-v0 already in registry.\u001b[0m\n",
      "  logger.warn(f\"Overriding environment {new_spec.id} already in registry.\")\n",
      "/home/hkolstee/.local/lib/python3.10/site-packages/gymnasium/envs/registration.py:694: UserWarning: \u001b[33mWARN: Overriding environment SpiderFlyGridMA-v0 already in registry.\u001b[0m\n",
      "  logger.warn(f\"Overriding environment {new_spec.id} already in registry.\")\n"
     ]
    }
   ],
   "source": [
    "# from shared_seq_double_DQN import seqDoubleDQN\n",
    "from custom_agents.CTCE_algorithms.shared_seq_DQN import seqDQN\n",
    "from custom_spider_env.spider_fly_env.envs.grid_MA_pettingzoo import SpiderFlyEnvMA\n",
    "from spider_fly_env.wrappers.pettingzoo_wrapper import PettingZooWrapper\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "env = SpiderFlyEnvMA(size = 3, spiders = 2, max_timesteps = 100, render_mode = \"ascii\")\n",
    "env = SpiderFlyEnvMA(size = 3, spiders = 2, max_timesteps = 100)\n",
    "env = PettingZooWrapper(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0 - Reward:[33.526 33.493] - Avg loss (last ep): None\n",
      "Episode: 10 - Reward:[37.585 37.582] - Avg loss (last ep): [0.00392721 0.06947921]\n",
      "Episode: 20 - Reward:[51.73  51.748] - Avg loss (last ep): [0.00266665 0.06541117]\n",
      "Episode: 30 - Reward:[46.621 46.6  ] - Avg loss (last ep): [0.00247545 0.06225387]\n",
      "Episode: 40 - Reward:[54.694 54.688] - Avg loss (last ep): [0.00233783 0.0584311 ]\n",
      "Episode: 50 - Reward:[47.662 47.674] - Avg loss (last ep): [0.00217093 0.05514215]\n",
      "Episode: 60 - Reward:[64.861 64.87 ] - Avg loss (last ep): [0.00196297 0.05220565]\n",
      "Episode: 70 - Reward:[65.752 65.761] - Avg loss (last ep): [0.00168361 0.04912174]\n",
      "Episode: 80 - Reward:[78.874 78.892] - Avg loss (last ep): [0.00192604 0.04489959]\n",
      "Episode: 90 - Reward:[67.804 67.723] - Avg loss (last ep): [0.00209014 0.04134202]\n",
      "Episode: 100 - Reward:[89.947 89.944] - Avg loss (last ep): [0.00229457 0.03914194]\n",
      "Episode: 110 - Reward:[67.774 67.72 ] - Avg loss (last ep): [0.00250399 0.03749768]\n",
      "Episode: 120 - Reward:[88.936 88.927] - Avg loss (last ep): [0.00296692 0.03463938]\n",
      "Episode: 130 - Reward:[88.942 88.927] - Avg loss (last ep): [0.00329158 0.0343208 ]\n",
      "Episode: 140 - Reward:[90.955 90.952] - Avg loss (last ep): [0.00381482 0.03300931]\n",
      "Episode: 150 - Reward:[85.939 85.933] - Avg loss (last ep): [0.00418337 0.03206576]\n",
      "Episode: 160 - Reward:[93.964 93.958] - Avg loss (last ep): [0.00444703 0.0306288 ]\n",
      "Episode: 170 - Reward:[89.938 89.941] - Avg loss (last ep): [0.00480103 0.02927407]\n",
      "Episode: 180 - Reward:[91.957 91.96 ] - Avg loss (last ep): [0.00521165 0.02901994]\n",
      "Episode: 190 - Reward:[88.942 88.927] - Avg loss (last ep): [0.00529776 0.02753667]\n"
     ]
    }
   ],
   "source": [
    "sequential_DQN = seqDQN(env, eps_steps = 100 * 100, batch_size = 256, layer_sizes = (32, 32), tau = 0.0001, buffer_max_size = 100000, global_observations = True) \n",
    "\n",
    "sequential_DQN.train(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0 - Reward:35.477500000000006 - Avg loss (last ep): None\n",
      "Episode: 10 - Reward:42.639999999999944 - Avg loss (last ep): 0.06667044095695018\n",
      "Episode: 20 - Reward:56.708499999999944 - Avg loss (last ep): 0.07504755944013596\n",
      "Episode: 30 - Reward:59.72199999999996 - Avg loss (last ep): 0.08443371653556823\n",
      "Episode: 40 - Reward:55.747 - Avg loss (last ep): 0.08777935929596424\n",
      "Episode: 50 - Reward:55.76499999999993 - Avg loss (last ep): 0.08297601357102394\n",
      "Episode: 60 - Reward:63.86199999999997 - Avg loss (last ep): 0.0813641981035471\n",
      "Episode: 70 - Reward:74.8465 - Avg loss (last ep): 0.07984564282000065\n",
      "Episode: 80 - Reward:86.929 - Avg loss (last ep): 0.07651614494621754\n",
      "Episode: 90 - Reward:90.9565 - Avg loss (last ep): 0.06991568699479103\n",
      "Episode: 100 - Reward:94.966 - Avg loss (last ep): 0.06423957917839289\n",
      "Episode: 110 - Reward:88.94500000000001 - Avg loss (last ep): 0.05901901252567768\n",
      "Episode: 120 - Reward:90.93399999999998 - Avg loss (last ep): 0.05702653013169765\n",
      "Episode: 130 - Reward:96.979 - Avg loss (last ep): 0.05308480720967054\n",
      "Episode: 140 - Reward:94.97050000000002 - Avg loss (last ep): 0.04991007149219513\n",
      "Episode: 150 - Reward:90.93549999999999 - Avg loss (last ep): 0.04897012323141098\n",
      "Episode: 160 - Reward:95.977 - Avg loss (last ep): 0.04545017249882221\n",
      "Episode: 170 - Reward:97.993 - Avg loss (last ep): 0.04360923932865262\n",
      "Episode: 180 - Reward:97.99000000000001 - Avg loss (last ep): 0.0434474197588861\n",
      "Episode: 190 - Reward:89.92899999999999 - Avg loss (last ep): 0.042050597053021194\n"
     ]
    }
   ],
   "source": [
    "from custom_agents.CTCE_algorithms.centralized_DQN import DQN\n",
    "\n",
    "mdDQN = DQN(env, eps_steps = 100 * 100, batch_size = 256, layer_sizes = (32, 32), tau = 0.0025, buffer_max_size = 100000, global_observations = True) \n",
    "mdDQN.train(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequential_DQN.shared_DQN.save(\"models\", \"seqDQN_hard2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "model = seqDQN(env, layer_sizes = (64,64), global_observations = True)\n",
    "model.shared_DQN.load(\"models/seqDQN_hard2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = SpiderFlyEnvMA(size = 4, spiders = 2, max_timesteps = 100, render_mode = \"ascii\")\n",
    "env = PettingZooWrapper(env)\n",
    "\n",
    "terminal = False\n",
    "print(\"start ----------------------------\")\n",
    "obs, _ = env.reset()\n",
    "while not terminal:\n",
    "    actions = []\n",
    "    actions = model.get_actions(obs, deterministic = True)\n",
    "    print([env.action_to_direction_string[act] for act in actions])\n",
    "\n",
    "    obs, rewards, terminals, truncations, infos = env.step(actions)\n",
    "    if rewards[0] == 1:\n",
    "        break\n",
    "    # print(obs, rewards, terminals, truncations, infos, actions)\n",
    "    # print(\"--------------------\")\n",
    "    \n",
    "    terminal = terminals[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 Spiders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-20 19:02:35.615916: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[' ' ' ' ' ' 'X']\n",
      " [' ' ' ' ' ' ' ']\n",
      " ['O' ' ' ' ' ' ']\n",
      " [' ' 'X' 'X' ' ']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hkolstee/.local/lib/python3.10/site-packages/gymnasium/envs/registration.py:694: UserWarning: \u001b[33mWARN: Overriding environment SpiderFlyGrid-v0 already in registry.\u001b[0m\n",
      "  logger.warn(f\"Overriding environment {new_spec.id} already in registry.\")\n",
      "/home/hkolstee/.local/lib/python3.10/site-packages/gymnasium/envs/registration.py:694: UserWarning: \u001b[33mWARN: Overriding environment SpiderFlyGridMA-v0 already in registry.\u001b[0m\n",
      "  logger.warn(f\"Overriding environment {new_spec.id} already in registry.\")\n"
     ]
    }
   ],
   "source": [
    "from custom_agents.CTCE_algorithms.shared_seq_DQN import seqDQN\n",
    "from custom_spider_env.spider_fly_env.envs.grid_MA_pettingzoo2 import SpiderFlyEnvMA\n",
    "from spider_fly_env.wrappers.pettingzoo_wrapper import PettingZooWrapper\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "env = SpiderFlyEnvMA(size = 4, spiders = 3, max_timesteps = 100, render_mode = \"ascii\")\n",
    "env = PettingZooWrapper(env, normalize = True)\n",
    "\n",
    "\n",
    "env = SpiderFlyEnvMA(size = 4, spiders = 3, max_timesteps = 100)\n",
    "env = PettingZooWrapper(env, normalize = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0 - Reward:[-0.066 -0.024 -0.064] - Avg loss (last ep): None\n"
     ]
    }
   ],
   "source": [
    "sequential_DQN = seqDQN(env,eps_steps = 100 * 1000, layer_sizes = (128, 128), tau = 0.0025, buffer_max_size = 1000000, batch_size = 256, global_observations = True, log_dir = \"tensorboard_logs_seqDQN_hard3_LOW_LR\", lr = 0.0001) \n",
    "\n",
    "rewards, losses = sequential_DQN.train(100 * 100000)\n",
    "\n",
    "# save model\n",
    "# sequential_DQN.shared_DQN.save(\"models\", \"seqDQN_hard3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequential_DQN.shared_DQN.save(\"models\", \"seqDQN_hard3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "model = seqDQN(env, layer_sizes = (64,64), global_observations = True)\n",
    "model.shared_DQN.load(\"models/seqDQN_hard3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = SpiderFlyEnvMA(size = 4, spiders = 3, max_timesteps = 100, render_mode = \"ascii\")\n",
    "env = PettingZooWrapper(env)\n",
    "\n",
    "terminal = False\n",
    "print(\"start ----------------------------\")\n",
    "obs, _ = env.reset()\n",
    "while not terminal:\n",
    "    actions = []\n",
    "    actions = model.get_actions(obs, deterministic = True)\n",
    "    print([env.action_to_direction_string[act] for act in actions])\n",
    "\n",
    "    obs, rewards, terminals, truncations, infos = env.step(actions)\n",
    "    if rewards[0] == 1:\n",
    "        break\n",
    "    # print(obs, rewards, terminals, truncations, infos, actions)\n",
    "    # print(\"--------------------\")\n",
    "    \n",
    "    terminal = terminals[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
