{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shared_DQN import SharedDQN\n",
    "from spider_fly_env.envs.grid_MA_pettingzoo_testing import SpiderFlyEnvMA\n",
    "from spider_fly_env.wrappers.pettingzoo_wrapper import PettingZooWrapper\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DQN (Shared parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ' ' ' 'X' 'X' ' ' 'O' 'O']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([2, 5, 6, 6])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = SpiderFlyEnvMA(render_mode = \"ascii\")\n",
    "env = PettingZooWrapper(env)\n",
    "\n",
    "env.observation_space[0].sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = SpiderFlyEnvMA(max_steps = 200)\n",
    "env = PettingZooWrapper(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DQN = SharedDQN(env, eps_steps = 50000, layer_sizes = (64, 64), tau = 0.0025, buffer_max_size = 100000) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 must have the same dtype, but got Double and Float",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m rewards, losses \u001b[38;5;241m=\u001b[39m \u001b[43mDQN\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/masterThesis/masterThesis/shared_DQN.py:142\u001b[0m, in \u001b[0;36mSharedDQN.train\u001b[0;34m(self, num_episodes)\u001b[0m\n\u001b[1;32m    140\u001b[0m actions \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m agent_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnr_agents):\n\u001b[0;32m--> 142\u001b[0m     actions\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m[\u001b[49m\u001b[43magent_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magent_idx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    143\u001b[0m \u001b[38;5;66;03m# take action\u001b[39;00m\n\u001b[1;32m    144\u001b[0m next_obs, rewards, terminals, truncations, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(actions)\n",
      "File \u001b[0;32m~/masterThesis/masterThesis/shared_DQN.py:80\u001b[0m, in \u001b[0;36mSharedDQN.get_action\u001b[0;34m(self, observation, agent_idx, deterministic)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;66;03m# clear potentially left-over gradients\u001b[39;00m\n\u001b[1;32m     79\u001b[0m one_hot_id\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 80\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshared_DQN\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mone_hot_id\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39margmax()\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/masterThesis/masterThesis/custom_agents/networks/critic_discrete.py:35\u001b[0m, in \u001b[0;36mCritic.forward\u001b[0;34m(self, obs)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, obs):\n\u001b[0;32m---> 35\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/masterThesis/masterThesis/custom_agents/networks/MLP.py:82\u001b[0m, in \u001b[0;36mMultiLayerPerceptron.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnr_layers \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (idx \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnr_layers):\n\u001b[1;32m     81\u001b[0m         \u001b[38;5;66;03m# pass through first and middle layers\u001b[39;00m\n\u001b[0;32m---> 82\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m    \n\u001b[1;32m     83\u001b[0m         data \u001b[38;5;241m=\u001b[39m functional\u001b[38;5;241m.\u001b[39mrelu(data)\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     85\u001b[0m         \u001b[38;5;66;03m# last layer(s)\u001b[39;00m\n\u001b[1;32m     86\u001b[0m         \u001b[38;5;66;03m# if multiple last layers\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 must have the same dtype, but got Double and Float"
     ]
    }
   ],
   "source": [
    "rewards, losses = DQN.train(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = np.vstack(rewards)\n",
    "data2 = np.vstack(losses)\n",
    "\n",
    "df1 = pd.DataFrame(data1, columns = [\"agent_1\", \"agent_2\"])\n",
    "df1[\"Episode\"] = list(range(data1.shape[0]))\n",
    "\n",
    "df2 = pd.DataFrame(data2, columns = [\"agent_1\", \"agent_2\"])\n",
    "df2[\"Episode\"] = list(range(data1.shape[0]))\n",
    "\n",
    "df1 = df1.melt('Episode', var_name='Agent', value_name='Rewards')\n",
    "df2 = df2.melt('Episode', var_name='Agent', value_name='Rewards')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need 1.5.0 for rolling average of next step\n",
    "pd.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df1)\n",
    "display(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1[\"Avg_Reward\"] = df1[\"Rewards\"].rolling(window = 5, step = 5).mean()\n",
    "df1 = df1[df1.Episode > 5]\n",
    "df1.dropna()\n",
    "\n",
    "df2[\"Avg_Loss\"] = df2[\"Rewards\"].rolling(window = 5, step = 5).mean()\n",
    "df2 = df2[df2.Episode > 5]\n",
    "df2.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.title(\"Rewards\")\n",
    "sns.lineplot(data = df1, x = \"Episode\", y = \"Avg_Reward\")\n",
    "plt.title(\"Rewards\")\n",
    "plt.figure()\n",
    "sns.lineplot(data = df1, x = \"Episode\", y = \"Avg_Reward\", hue = \"Agent\")\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Losses\")\n",
    "sns.lineplot(data = df2, x = \"Episode\", y = \"Avg_Loss\")\n",
    "plt.figure()\n",
    "plt.title(\"Losses\")\n",
    "sns.lineplot(data = df2, x = \"Episode\", y = \"Avg_Loss\", hue = \"Agent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sequential Q-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tabular Q-learning:\n",
    "$$\n",
    "\\begin{align*}\n",
    "    & s = env.reset()\\\\\n",
    "    &\\text{while not } done:\\\\\n",
    "    & \\quad\\quad \\text{for } i \\text{ in } (1, \\ldots, m):\\\\\n",
    "    & \\quad\\quad\\quad\\quad a_i = \\argmax_{a_i} Q_i(s_i, a_i)\\\\\n",
    "    & \\quad\\quad s', r, d = env.step(a_1, \\ldots, a_m)\\\\\n",
    "    & \\quad\\quad \\text{for } i \\text{ in } (1, \\ldots, m):\\\\\n",
    "    & \\quad\\quad\\quad\\quad Q_i(s, a_i) = Q_i(s, a_i) + lr * ((mean(r) + \\gamma * \\max_{a'_i} Q_i(s', a'_i)) - Q_i(s, a_i))\\\\\n",
    "\\end{align*}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sequential Tabular Q-learning:\n",
    "$$\n",
    "\\begin{align*}\n",
    "    & s = env.reset()\\\\\n",
    "    &\\text{while not } done:\\\\\n",
    "    & \\quad\\quad \\text{for } i \\text{ in } (1, \\ldots, m):\\\\\n",
    "    & \\quad\\quad\\quad\\quad a_i = \\argmax_{a_i} Q_i(s, a_1, \\ldots, a_i)\\\\\n",
    "    & \\quad\\quad s', r, d = env.step(a_1, \\ldots, a_m)\\\\\n",
    "    & \\quad\\quad \\text{for } i \\text{ in } (1, \\ldots, m-1):\\\\\n",
    "    & \\quad\\quad\\quad\\quad Q_i(s, a_1, \\ldots, a_i) = Q_i(s, a_1, \\ldots, a_i) + (i/m) * lr * (\\max_{a_{i+1}} Q_{i+1}(s, a_1, \\ldots, a_{i+1}) - Q_i(s, a_1, \\ldots, a_i))\\\\\n",
    "    & \\quad\\quad Q_m(s, a_1, \\ldots, a_m) = Q_m(s, a_1, \\ldots, a_m) + lr * ((mean(r) + \\gamma * \\max_{a'_1} Q_1(s', a'_1)) - Q_m(s, a_1, \\ldots, a_m))\\\\\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data1 = np.vstack(rewards)\n",
    "# data2 = np.vstack(losses)\n",
    "\n",
    "# df1 = pd.DataFrame(data1, columns = [\"agent_1\", \"agent_2\"])\n",
    "# df1[\"Episode\"] = list(range(data1.shape[0]))\n",
    "\n",
    "# df2 = pd.DataFrame(data2, columns = [\"agent_1\", \"agent_2\"])\n",
    "# df2[\"Episode\"] = list(range(data1.shape[0]))\n",
    "\n",
    "# df1 = df1.melt('Episode', var_name='Agent', value_name='Rewards')\n",
    "# df2 = df2.melt('Episode', var_name='Agent', value_name='Rewards')\n",
    "# df1[\"Avg_Reward\"] = df1[\"Rewards\"].rolling(window = 10, step = 10).mean()\n",
    "# df1 = df1[df1.Episode > 10]\n",
    "# df1.dropna()\n",
    "\n",
    "# df2[\"Avg_Loss\"] = df2[\"Rewards\"].rolling(window = 10, step = 10).mean()\n",
    "# df2 = df2[df2.Episode > 10]\n",
    "# df2.dropna()\n",
    "\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.title(\"Rewards\")\n",
    "# sns.lineplot(data = df1, x = \"Episode\", y = \"Avg_Reward\")\n",
    "# plt.title(\"Rewards\")\n",
    "# plt.figure()\n",
    "# sns.lineplot(data = df1, x = \"Episode\", y = \"Avg_Reward\", hue = \"Agent\")\n",
    "\n",
    "# plt.figure()\n",
    "# plt.title(\"Losses\")\n",
    "# sns.lineplot(data = df2, x = \"Episode\", y = \"Avg_Loss\")\n",
    "# plt.figure()\n",
    "# plt.title(\"Losses\")\n",
    "# sns.lineplot(data = df2, x = \"Episode\", y = \"Avg_Loss\", hue = \"Agent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hkolstee/.local/lib/python3.10/site-packages/gymnasium/envs/registration.py:694: UserWarning: \u001b[33mWARN: Overriding environment SpiderFlyGrid-v0 already in registry.\u001b[0m\n",
      "  logger.warn(f\"Overriding environment {new_spec.id} already in registry.\")\n",
      "/home/hkolstee/.local/lib/python3.10/site-packages/gymnasium/envs/registration.py:694: UserWarning: \u001b[33mWARN: Overriding environment SpiderFlyGridMA-v0 already in registry.\u001b[0m\n",
      "  logger.warn(f\"Overriding environment {new_spec.id} already in registry.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['X' 'O' 'X']\n",
      " [' ' ' ' ' ']\n",
      " [' ' ' ' ' ']]\n"
     ]
    }
   ],
   "source": [
    "# from shared_seq_double_DQN import seqDoubleDQN\n",
    "from shared_seq_DQN import seqDQN\n",
    "from custom_spider_env.spider_fly_env.envs.grid_MA_pettingzoo import SpiderFlyEnvMA\n",
    "from spider_fly_env.wrappers.pettingzoo_wrapper import PettingZooWrapper\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "env = SpiderFlyEnvMA(size = 3, spiders = 2, max_timesteps = 100, render_mode = \"ascii\")\n",
    "env = SpiderFlyEnvMA(size = 3, spiders = 2, max_timesteps = 100)\n",
    "env = PettingZooWrapper(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hkolstee/.local/lib/python3.10/site-packages/torch/cuda/__init__.py:138: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11040). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0 - Reward:[37.543 37.594] - Avg loss (last ep): None\n",
      "Episode: 10 - Reward:[43.615 43.618] - Avg loss (last ep): [0.00345285 0.05578576]\n",
      "Episode: 20 - Reward:[43.666 43.696] - Avg loss (last ep): [0.00904051 0.06330872]\n",
      "Episode: 30 - Reward:[60.751 60.769] - Avg loss (last ep): [0.01184662 0.06815781]\n",
      "Episode: 40 - Reward:[57.784 57.778] - Avg loss (last ep): [0.01388992 0.0709038 ]\n",
      "Episode: 50 - Reward:[70.801 70.807] - Avg loss (last ep): [0.01422313 0.07126122]\n",
      "Episode: 60 - Reward:[67.813 67.81 ] - Avg loss (last ep): [0.01405068 0.07199683]\n",
      "Episode: 70 - Reward:[70.831 70.828] - Avg loss (last ep): [0.01397123 0.0714011 ]\n",
      "Episode: 80 - Reward:[85.942 85.936] - Avg loss (last ep): [0.01325756 0.0704667 ]\n",
      "Episode: 90 - Reward:[87.946 87.952] - Avg loss (last ep): [0.01273387 0.06721904]\n",
      "Episode: 100 - Reward:[94.987 94.984] - Avg loss (last ep): [0.01185828 0.06100376]\n",
      "Episode: 110 - Reward:[89.977 89.977] - Avg loss (last ep): [0.01124744 0.05574842]\n",
      "Episode: 120 - Reward:[97.99  97.987] - Avg loss (last ep): [0.01074647 0.05136119]\n",
      "Episode: 130 - Reward:[98.989 98.992] - Avg loss (last ep): [0.01053336 0.0475781 ]\n",
      "Episode: 140 - Reward:[97.987 97.984] - Avg loss (last ep): [0.01008095 0.04562063]\n",
      "Episode: 150 - Reward:[91.957 91.954] - Avg loss (last ep): [0.00986928 0.0432038 ]\n",
      "Episode: 160 - Reward:[96.985 96.988] - Avg loss (last ep): [0.00947235 0.04024681]\n",
      "Episode: 170 - Reward:[96.988 96.979] - Avg loss (last ep): [0.00928407 0.03818096]\n",
      "Episode: 180 - Reward:[89.935 89.944] - Avg loss (last ep): [0.00875479 0.03736833]\n",
      "Episode: 190 - Reward:[95.983 95.986] - Avg loss (last ep): [0.00812413 0.03589522]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m sequential_DQN \u001b[38;5;241m=\u001b[39m seqDQN(env, batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m256\u001b[39m, eps_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10000\u001b[39m, layer_sizes \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m32\u001b[39m), tau \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0025\u001b[39m, buffer_max_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100000\u001b[39m, global_observations \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m) \n\u001b[0;32m----> 3\u001b[0m rewards, losses \u001b[38;5;241m=\u001b[39m sequential_DQN\u001b[38;5;241m.\u001b[39mtrain(\u001b[38;5;241m200\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "sequential_DQN = seqDQN(env, eps_steps = 100 * 100, batch_size = 256, eps_steps = 10000, layer_sizes = (32, 32), tau = 0.0025, buffer_max_size = 100000, global_observations = True) \n",
    "\n",
    "sequential_DQN.train(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "keyword argument repeated: eps_steps (2258089495.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[5], line 3\u001b[0;36m\u001b[0m\n\u001b[0;31m    mdDQN = DQN(env, eps_steps = 100 * 100, batch_size = 256, eps_steps = 10000, layer_sizes = (32, 32), tau = 0.0025, buffer_max_size = 100000, global_observations = True)\u001b[0m\n\u001b[0m                                                              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m keyword argument repeated: eps_steps\n"
     ]
    }
   ],
   "source": [
    "from centralized_DQN import DQN\n",
    "\n",
    "mdDQN = DQN(env, eps_steps = 100 * 100, batch_size = 256, layer_sizes = (32, 32), tau = 0.0025, buffer_max_size = 100000, global_observations = True) \n",
    "mdDQN.train(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequential_DQN.shared_DQN.save(\"models\", \"seqDQN_hard2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "model = seqDQN(env, layer_sizes = (64,64), global_observations = True)\n",
    "model.shared_DQN.load(\"models/seqDQN_hard2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = SpiderFlyEnvMA(size = 4, spiders = 2, max_timesteps = 100, render_mode = \"ascii\")\n",
    "env = PettingZooWrapper(env)\n",
    "\n",
    "terminal = False\n",
    "print(\"start ----------------------------\")\n",
    "obs, _ = env.reset()\n",
    "while not terminal:\n",
    "    actions = []\n",
    "    actions = model.get_actions(obs, deterministic = True)\n",
    "    print([env.action_to_direction_string[act] for act in actions])\n",
    "\n",
    "    obs, rewards, terminals, truncations, infos = env.step(actions)\n",
    "    if rewards[0] == 1:\n",
    "        break\n",
    "    # print(obs, rewards, terminals, truncations, infos, actions)\n",
    "    # print(\"--------------------\")\n",
    "    \n",
    "    terminal = terminals[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 Spiders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-20 19:02:35.615916: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[' ' ' ' ' ' 'X']\n",
      " [' ' ' ' ' ' ' ']\n",
      " ['O' ' ' ' ' ' ']\n",
      " [' ' 'X' 'X' ' ']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hkolstee/.local/lib/python3.10/site-packages/gymnasium/envs/registration.py:694: UserWarning: \u001b[33mWARN: Overriding environment SpiderFlyGrid-v0 already in registry.\u001b[0m\n",
      "  logger.warn(f\"Overriding environment {new_spec.id} already in registry.\")\n",
      "/home/hkolstee/.local/lib/python3.10/site-packages/gymnasium/envs/registration.py:694: UserWarning: \u001b[33mWARN: Overriding environment SpiderFlyGridMA-v0 already in registry.\u001b[0m\n",
      "  logger.warn(f\"Overriding environment {new_spec.id} already in registry.\")\n"
     ]
    }
   ],
   "source": [
    "from custom_agents.CTCE_algorithms.shared_seq_DQN import seqDQN\n",
    "from custom_spider_env.spider_fly_env.envs.grid_MA_pettingzoo2 import SpiderFlyEnvMA\n",
    "from spider_fly_env.wrappers.pettingzoo_wrapper import PettingZooWrapper\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "env = SpiderFlyEnvMA(size = 4, spiders = 3, max_timesteps = 100, render_mode = \"ascii\")\n",
    "env = PettingZooWrapper(env, normalize = True)\n",
    "\n",
    "\n",
    "env = SpiderFlyEnvMA(size = 4, spiders = 3, max_timesteps = 100)\n",
    "env = PettingZooWrapper(env, normalize = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0 - Reward:[-0.066 -0.024 -0.064] - Avg loss (last ep): None\n"
     ]
    }
   ],
   "source": [
    "sequential_DQN = seqDQN(env,eps_steps = 100 * 1000, layer_sizes = (128, 128), tau = 0.0025, buffer_max_size = 1000000, batch_size = 256, global_observations = True, log_dir = \"tensorboard_logs_seqDQN_hard3_LOW_LR\", lr = 0.0001) \n",
    "\n",
    "rewards, losses = sequential_DQN.train(100 * 100000)\n",
    "\n",
    "# save model\n",
    "# sequential_DQN.shared_DQN.save(\"models\", \"seqDQN_hard3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequential_DQN.shared_DQN.save(\"models\", \"seqDQN_hard3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "model = seqDQN(env, layer_sizes = (64,64), global_observations = True)\n",
    "model.shared_DQN.load(\"models/seqDQN_hard3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = SpiderFlyEnvMA(size = 4, spiders = 3, max_timesteps = 100, render_mode = \"ascii\")\n",
    "env = PettingZooWrapper(env)\n",
    "\n",
    "terminal = False\n",
    "print(\"start ----------------------------\")\n",
    "obs, _ = env.reset()\n",
    "while not terminal:\n",
    "    actions = []\n",
    "    actions = model.get_actions(obs, deterministic = True)\n",
    "    print([env.action_to_direction_string[act] for act in actions])\n",
    "\n",
    "    obs, rewards, terminals, truncations, infos = env.step(actions)\n",
    "    if rewards[0] == 1:\n",
    "        break\n",
    "    # print(obs, rewards, terminals, truncations, infos, actions)\n",
    "    # print(\"--------------------\")\n",
    "    \n",
    "    terminal = terminals[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
