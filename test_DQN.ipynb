{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hkolstee/.local/lib/python3.10/site-packages/gymnasium/envs/registration.py:694: UserWarning: \u001b[33mWARN: Overriding environment SpiderFlyGrid-v0 already in registry.\u001b[0m\n",
      "  logger.warn(f\"Overriding environment {new_spec.id} already in registry.\")\n",
      "/home/hkolstee/.local/lib/python3.10/site-packages/gymnasium/envs/registration.py:694: UserWarning: \u001b[33mWARN: Overriding environment SpiderFlyGridMA-v0 already in registry.\u001b[0m\n",
      "  logger.warn(f\"Overriding environment {new_spec.id} already in registry.\")\n"
     ]
    }
   ],
   "source": [
    "from shared_DQN import SharedDQN\n",
    "from spider_fly_env.envs.grid_MA_pettingzoo_testing import SpiderFlyEnvMA\n",
    "from spider_fly_env.wrappers.pettingzoo_wrapper import PettingZooWrapper\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DQN (Shared parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['X' ' ' ' ' 'X' ' ' 'O' 'O']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([4, 4, 1, 3])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = SpiderFlyEnvMA(render_mode = \"ascii\")\n",
    "env = PettingZooWrapper(env)\n",
    "\n",
    "env.observation_space[0].sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = SpiderFlyEnvMA(max_steps = 200)\n",
    "env = PettingZooWrapper(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DQN = SharedDQN(env, eps_steps = 50000, layer_sizes = (64, 64), tau = 0.0025, buffer_max_size = 50000) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards, losses = DQN.train(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = np.vstack(rewards)\n",
    "data2 = np.vstack(losses)\n",
    "\n",
    "df1 = pd.DataFrame(data1, columns = [\"agent_1\", \"agent_2\"])\n",
    "df1[\"Episode\"] = list(range(data1.shape[0]))\n",
    "\n",
    "df2 = pd.DataFrame(data2, columns = [\"agent_1\", \"agent_2\"])\n",
    "df2[\"Episode\"] = list(range(data1.shape[0]))\n",
    "\n",
    "df1 = df1.melt('Episode', var_name='Agent', value_name='Rewards')\n",
    "df2 = df2.melt('Episode', var_name='Agent', value_name='Rewards')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need 1.5.0 for rolling average of next step\n",
    "pd.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df1)\n",
    "display(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1[\"Avg_Reward\"] = df1[\"Rewards\"].rolling(window = 5, step = 5).mean()\n",
    "df1 = df1[df1.Episode > 5]\n",
    "df1.dropna()\n",
    "\n",
    "df2[\"Avg_Loss\"] = df2[\"Rewards\"].rolling(window = 5, step = 5).mean()\n",
    "df2 = df2[df2.Episode > 5]\n",
    "df2.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.title(\"Rewards\")\n",
    "sns.lineplot(data = df1, x = \"Episode\", y = \"Avg_Reward\")\n",
    "plt.title(\"Rewards\")\n",
    "plt.figure()\n",
    "sns.lineplot(data = df1, x = \"Episode\", y = \"Avg_Reward\", hue = \"Agent\")\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Losses\")\n",
    "sns.lineplot(data = df2, x = \"Episode\", y = \"Avg_Loss\")\n",
    "plt.figure()\n",
    "plt.title(\"Losses\")\n",
    "sns.lineplot(data = df2, x = \"Episode\", y = \"Avg_Loss\", hue = \"Agent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sequential Q-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tabular Q-learning:\n",
    "$$\n",
    "\\begin{align*}\n",
    "    & s = env.reset()\\\\\n",
    "    &\\text{while not } done:\\\\\n",
    "    & \\quad\\quad \\text{for } i \\text{ in } (1, \\ldots, m):\\\\\n",
    "    & \\quad\\quad\\quad\\quad a_i = \\argmax_{a_i} Q_i(s_i, a_i)\\\\\n",
    "    & \\quad\\quad s', r, d = env.step(a_1, \\ldots, a_m)\\\\\n",
    "    & \\quad\\quad \\text{for } i \\text{ in } (1, \\ldots, m):\\\\\n",
    "    & \\quad\\quad\\quad\\quad Q_i(s, a_i) = Q_i(s, a_i) + lr * ((mean(r) + \\gamma * \\max_{a'_i} Q_i(s', a'_i)) - Q_i(s, a_i))\\\\\n",
    "\\end{align*}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sequential Tabular Q-learning:\n",
    "$$\n",
    "\\begin{align*}\n",
    "    & s = env.reset()\\\\\n",
    "    &\\text{while not } done:\\\\\n",
    "    & \\quad\\quad \\text{for } i \\text{ in } (1, \\ldots, m):\\\\\n",
    "    & \\quad\\quad\\quad\\quad a_i = \\argmax_{a_i} Q_i(s, a_1, \\ldots, a_i)\\\\\n",
    "    & \\quad\\quad s', r, d = env.step(a_1, \\ldots, a_m)\\\\\n",
    "    & \\quad\\quad \\text{for } i \\text{ in } (1, \\ldots, m-1):\\\\\n",
    "    & \\quad\\quad\\quad\\quad Q_i(s, a_1, \\ldots, a_i) = Q_i(s, a_1, \\ldots, a_i) + (i/m) * lr * (\\max_{a_{i+1}} Q_{i+1}(s, a_1, \\ldots, a_{i+1}) - Q_i(s, a_1, \\ldots, a_i))\\\\\n",
    "    & \\quad\\quad Q_m(s, a_1, \\ldots, a_m) = Q_m(s, a_1, \\ldots, a_m) + lr * ((mean(r) + \\gamma * \\max_{a'_1} Q_1(s', a'_1)) - Q_m(s, a_1, \\ldots, a_m))\\\\\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data1 = np.vstack(rewards)\n",
    "# data2 = np.vstack(losses)\n",
    "\n",
    "# df1 = pd.DataFrame(data1, columns = [\"agent_1\", \"agent_2\"])\n",
    "# df1[\"Episode\"] = list(range(data1.shape[0]))\n",
    "\n",
    "# df2 = pd.DataFrame(data2, columns = [\"agent_1\", \"agent_2\"])\n",
    "# df2[\"Episode\"] = list(range(data1.shape[0]))\n",
    "\n",
    "# df1 = df1.melt('Episode', var_name='Agent', value_name='Rewards')\n",
    "# df2 = df2.melt('Episode', var_name='Agent', value_name='Rewards')\n",
    "# df1[\"Avg_Reward\"] = df1[\"Rewards\"].rolling(window = 10, step = 10).mean()\n",
    "# df1 = df1[df1.Episode > 10]\n",
    "# df1.dropna()\n",
    "\n",
    "# df2[\"Avg_Loss\"] = df2[\"Rewards\"].rolling(window = 10, step = 10).mean()\n",
    "# df2 = df2[df2.Episode > 10]\n",
    "# df2.dropna()\n",
    "\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.title(\"Rewards\")\n",
    "# sns.lineplot(data = df1, x = \"Episode\", y = \"Avg_Reward\")\n",
    "# plt.title(\"Rewards\")\n",
    "# plt.figure()\n",
    "# sns.lineplot(data = df1, x = \"Episode\", y = \"Avg_Reward\", hue = \"Agent\")\n",
    "\n",
    "# plt.figure()\n",
    "# plt.title(\"Losses\")\n",
    "# sns.lineplot(data = df2, x = \"Episode\", y = \"Avg_Loss\")\n",
    "# plt.figure()\n",
    "# plt.title(\"Losses\")\n",
    "# sns.lineplot(data = df2, x = \"Episode\", y = \"Avg_Loss\", hue = \"Agent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hkolstee/.local/lib/python3.10/site-packages/gymnasium/envs/registration.py:694: UserWarning: \u001b[33mWARN: Overriding environment SpiderFlyGrid-v0 already in registry.\u001b[0m\n",
      "  logger.warn(f\"Overriding environment {new_spec.id} already in registry.\")\n",
      "/home/hkolstee/.local/lib/python3.10/site-packages/gymnasium/envs/registration.py:694: UserWarning: \u001b[33mWARN: Overriding environment SpiderFlyGridMA-v0 already in registry.\u001b[0m\n",
      "  logger.warn(f\"Overriding environment {new_spec.id} already in registry.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['O' ' ' ' ' ' ']\n",
      " ['X' ' ' ' ' ' ']\n",
      " [' ' ' ' 'X' ' ']\n",
      " [' ' ' ' ' ' ' ']]\n"
     ]
    }
   ],
   "source": [
    "from shared_seq_double_DQN import seqDoubleDQN\n",
    "from shared_seq_DQN import seqDQN\n",
    "from shared_DQN import IndependentDQN\n",
    "from custom_spider_env.spider_fly_env.envs.grid_MA_pettingzoo import SpiderFlyEnvMA\n",
    "from spider_fly_env.wrappers.pettingzoo_wrapper import PettingZooWrapper\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "env = SpiderFlyEnvMA(size = 4, spiders = 2, max_timesteps = 100, render_mode = \"ascii\")\n",
    "env = PettingZooWrapper(env)\n",
    "\n",
    "env.observation_space[0].sample()\n",
    "\n",
    "env = SpiderFlyEnvMA(size = 4, spiders = 2, max_timesteps = 100)\n",
    "env = PettingZooWrapper(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hkolstee/.local/lib/python3.10/site-packages/torch/cuda/__init__.py:138: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11040). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n",
      "/home/hkolstee/masterThesis/masterThesis/shared_seq_DQN.py:353: RuntimeWarning: invalid value encountered in divide\n",
      "  avg_loss = loss_sum / learn_steps\n",
      "/home/hkolstee/masterThesis/masterThesis/shared_seq_DQN.py:355: RuntimeWarning: invalid value encountered in divide\n",
      "  loss_log.append(loss_sum / learn_steps)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0 - Reward:[1.825 1.888] - Avg loss (last ep): [nan nan]\n",
      "Episode: 10 - Reward:[0.838 0.883] - Avg loss (last ep): [0.00032585 0.00187178]\n",
      "Episode: 20 - Reward:[4.927 4.846] - Avg loss (last ep): [0.00075622 0.00281553]\n",
      "Episode: 30 - Reward:[3.868 3.856] - Avg loss (last ep): [0.0009669  0.00291591]\n",
      "Episode: 40 - Reward:[7.    6.823] - Avg loss (last ep): [0.00143454 0.00401261]\n",
      "Episode: 50 - Reward:[11.965 11.86 ] - Avg loss (last ep): [0.00189193 0.00633617]\n",
      "Episode: 60 - Reward:[14.944 14.866] - Avg loss (last ep): [0.0025698  0.00751691]\n",
      "Episode: 70 - Reward:[24.958 24.883] - Avg loss (last ep): [0.00299741 0.00939011]\n",
      "Episode: 80 - Reward:[26.908 26.914] - Avg loss (last ep): [0.00378357 0.01105038]\n",
      "Episode: 90 - Reward:[40.933 40.942] - Avg loss (last ep): [0.00431614 0.01386695]\n",
      "Episode: 100 - Reward:[35.956 35.908] - Avg loss (last ep): [0.00457349 0.01610767]\n",
      "Episode: 110 - Reward:[33.913 33.973] - Avg loss (last ep): [0.00505176 0.01920811]\n",
      "Episode: 120 - Reward:[28.96  28.897] - Avg loss (last ep): [0.00519203 0.02029506]\n",
      "Episode: 130 - Reward:[38.899 38.911] - Avg loss (last ep): [0.00559205 0.02135373]\n",
      "Episode: 140 - Reward:[38.905 38.929] - Avg loss (last ep): [0.00558731 0.02209106]\n",
      "Episode: 150 - Reward:[41.947 41.902] - Avg loss (last ep): [0.00565025 0.02235448]\n",
      "Episode: 160 - Reward:[40.912 40.924] - Avg loss (last ep): [0.00539846 0.02412153]\n",
      "Episode: 170 - Reward:[39.907 39.928] - Avg loss (last ep): [0.00522079 0.02521682]\n",
      "Episode: 180 - Reward:[39.943 39.91 ] - Avg loss (last ep): [0.00541343 0.02610719]\n",
      "Episode: 190 - Reward:[38.944 38.917] - Avg loss (last ep): [0.00545604 0.02784811]\n"
     ]
    }
   ],
   "source": [
    "sequential_DQN = seqDQN(env, eps_steps = 100 * 100, layer_sizes = (64, 64), tau = 0.0025, buffer_max_size = 100000, batch_size = 256, global_observations = True, log_dir = \"tensorboard_logs_seqDQN_hard2_eq_lr\") \n",
    "\n",
    "rewards, losses = sequential_DQN.train(200)\n",
    "\n",
    "# save model\n",
    "sequential_DQN.shared_DQN.save(\"models\", \"seqDQN_hard2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequential_DQN.shared_DQN.save(\"models\", \"seqDQN_hard2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "model = seqDQN(env, layer_sizes = (64,64), global_observations = True)\n",
    "model.shared_DQN.load(\"models/seqDQN_hard2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ']\n",
      " ['X' ' ' ' ' ' ']\n",
      " [' ' 'O' 'X' ' ']]\n",
      "start ----------------------------\n",
      "[[' ' ' ' ' ' ' ']\n",
      " [' ' ' ' 'X' ' ']\n",
      " [' ' ' ' ' ' ' ']\n",
      " [' ' 'O' ' ' 'X']]\n",
      "['left', 'left']\n",
      "[[' ' ' ' ' ' ' ']\n",
      " [' ' 'X' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ']\n",
      " [' ' 'O' 'X' ' ']]\n",
      "['nothing', 'down']\n",
      "[[' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ']\n",
      " [' ' 'X' ' ' ' ']\n",
      " [' ' 'O' 'X' ' ']]\n"
     ]
    }
   ],
   "source": [
    "env = SpiderFlyEnvMA(size = 4, spiders = 2, max_timesteps = 100, render_mode = \"ascii\")\n",
    "env = PettingZooWrapper(env)\n",
    "\n",
    "terminal = False\n",
    "print(\"start ----------------------------\")\n",
    "obs, _ = env.reset()\n",
    "while not terminal:\n",
    "    actions = []\n",
    "    actions = model.get_actions(obs, deterministic = True)\n",
    "    print([env.action_to_direction_string[act] for act in actions])\n",
    "\n",
    "    obs, rewards, terminals, truncations, infos = env.step(actions)\n",
    "    if rewards[0] == 1:\n",
    "        break\n",
    "    # print(obs, rewards, terminals, truncations, infos, actions)\n",
    "    # print(\"--------------------\")\n",
    "    \n",
    "    terminal = terminals[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 Spiders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hkolstee/.local/lib/python3.10/site-packages/gymnasium/envs/registration.py:694: UserWarning: \u001b[33mWARN: Overriding environment SpiderFlyGrid-v0 already in registry.\u001b[0m\n",
      "  logger.warn(f\"Overriding environment {new_spec.id} already in registry.\")\n",
      "/home/hkolstee/.local/lib/python3.10/site-packages/gymnasium/envs/registration.py:694: UserWarning: \u001b[33mWARN: Overriding environment SpiderFlyGridMA-v0 already in registry.\u001b[0m\n",
      "  logger.warn(f\"Overriding environment {new_spec.id} already in registry.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['X' ' ' 'X' ' ']\n",
      " [' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ']\n",
      " [' ' 'X' 'O' ' ']]\n"
     ]
    }
   ],
   "source": [
    "from shared_seq_double_DQN import seqDoubleDQN\n",
    "from shared_seq_DQN import seqDQN\n",
    "from shared_DQN import IndependentDQN\n",
    "from custom_spider_env.spider_fly_env.envs.grid_MA_pettingzoo import SpiderFlyEnvMA\n",
    "from spider_fly_env.wrappers.pettingzoo_wrapper import PettingZooWrapper\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "env = SpiderFlyEnvMA(size = 4, spiders = 3, max_timesteps = 100, render_mode = \"ascii\")\n",
    "env = PettingZooWrapper(env, normalize = True)\n",
    "\n",
    "\n",
    "env = SpiderFlyEnvMA(size = 4, spiders = 3, max_timesteps = 100)\n",
    "env = PettingZooWrapper(env, normalize = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hkolstee/.local/lib/python3.10/site-packages/torch/cuda/__init__.py:138: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11040). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n",
      "/home/hkolstee/masterThesis/masterThesis/shared_seq_DQN.py:359: RuntimeWarning: invalid value encountered in divide\n",
      "  avg_loss = loss_sum / learn_steps\n",
      "/home/hkolstee/masterThesis/masterThesis/shared_seq_DQN.py:361: RuntimeWarning: invalid value encountered in divide\n",
      "  loss_log.append(loss_sum / learn_steps)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0 - Reward:[-0.149 -0.137 -0.116] - Avg loss (last ep): [nan nan nan]\n",
      "True\n",
      "True\n",
      "False None None\n",
      "True\n",
      "True\n",
      "False None None\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False None None\n",
      "True\n",
      "True\n",
      "False None None\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False None None\n",
      "True\n",
      "True\n",
      "False None None\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False None None\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m sequential_DQN \u001b[38;5;241m=\u001b[39m seqDQN(env,eps_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1250\u001b[39m, layer_sizes \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m256\u001b[39m, \u001b[38;5;241m256\u001b[39m), tau \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0025\u001b[39m, buffer_max_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100000\u001b[39m, batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m256\u001b[39m, global_observations \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, log_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtensorboard_logs_seqDQN_hard3\u001b[39m\u001b[38;5;124m\"\u001b[39m) \n\u001b[0;32m----> 3\u001b[0m rewards, losses \u001b[38;5;241m=\u001b[39m \u001b[43msequential_DQN\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2500\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# save model\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# sequential_DQN.shared_DQN.save(\"models\", \"seqDQN_hard3\")\u001b[39;00m\n",
      "File \u001b[0;32m~/masterThesis/masterThesis/shared_seq_DQN.py:333\u001b[0m, in \u001b[0;36mseqDQN.train\u001b[0;34m(self, num_episodes)\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplay_buffer\u001b[38;5;241m.\u001b[39madd_transition(obs, actions, rewards, next_obs, terminals)\n\u001b[1;32m    332\u001b[0m \u001b[38;5;66;03m# learning step\u001b[39;00m\n\u001b[0;32m--> 333\u001b[0m status, losses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    335\u001b[0m \u001b[38;5;66;03m# update state\u001b[39;00m\n\u001b[1;32m    336\u001b[0m obs \u001b[38;5;241m=\u001b[39m next_obs\n",
      "File \u001b[0;32m~/masterThesis/masterThesis/shared_seq_DQN.py:148\u001b[0m, in \u001b[0;36mseqDQN.learn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28mprint\u001b[39m(input_tensor\u001b[38;5;241m.\u001b[39mrequires_grad, input_tensor\u001b[38;5;241m.\u001b[39mgrad, input_tensor\u001b[38;5;241m.\u001b[39mgrad_fn)\n\u001b[1;32m    144\u001b[0m \u001b[38;5;66;03m# Q_i(s, a_1, ..., a_i)\u001b[39;00m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;66;03m# print(\"---------------\")\u001b[39;00m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;66;03m# print(\"input tensor agent \", agent_idx)\u001b[39;00m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;66;03m# print(input_tensor)\u001b[39;00m\n\u001b[0;32m--> 148\u001b[0m Q_vals \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshared_DQN\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28mprint\u001b[39m(Q_vals\u001b[38;5;241m.\u001b[39mrequires_grad)\n\u001b[1;32m    150\u001b[0m \u001b[38;5;66;03m# print(\"Qvals\")\u001b[39;00m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;66;03m# print(Q_vals)\u001b[39;00m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;66;03m# print(\"taken action\")\u001b[39;00m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;66;03m# print(replay_act[agent_idx].long())\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;66;03m# Q_taken_action = self.shared_DQN(input_tensor).gather(1, replay_act[agent_idx].long())\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/masterThesis/masterThesis/custom_agent/SAC_components/critic_discrete.py:35\u001b[0m, in \u001b[0;36mCritic.forward\u001b[0;34m(self, obs)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, obs):\n\u001b[0;32m---> 35\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/masterThesis/masterThesis/custom_agent/SAC_components/MLP.py:83\u001b[0m, in \u001b[0;36mMultiLayerPerceptron.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (idx \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnr_layers):\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;66;03m# pass through first and middle layers\u001b[39;00m\n\u001b[1;32m     82\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers[idx](data)    \n\u001b[0;32m---> 83\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;66;03m# last layer(s)\u001b[39;00m\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;66;03m# if multiple last layers\u001b[39;00m\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers[idx], nn\u001b[38;5;241m.\u001b[39mModuleList):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/functional.py:1471\u001b[0m, in \u001b[0;36mrelu\u001b[0;34m(input, inplace)\u001b[0m\n\u001b[1;32m   1469\u001b[0m     result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu_(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m   1470\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1471\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1472\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sequential_DQN = seqDQN(env,eps_steps = 100 * 1250, layer_sizes = (256, 256), tau = 0.0025, buffer_max_size = 100000, batch_size = 256, global_observations = True, log_dir = \"tensorboard_logs_seqDQN_hard3\") \n",
    "\n",
    "rewards, losses = sequential_DQN.train(2500)\n",
    "\n",
    "# save model\n",
    "# sequential_DQN.shared_DQN.save(\"models\", \"seqDQN_hard3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequential_DQN.shared_DQN.save(\"models\", \"seqDQN_hard3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "model = seqDQN(env, layer_sizes = (64,64), global_observations = True)\n",
    "model.shared_DQN.load(\"models/seqDQN_hard3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['O' ' ' ' ' 'X']\n",
      " [' ' ' ' ' ' ' ']\n",
      " ['X' ' ' 'X' ' ']\n",
      " [' ' ' ' ' ' ' ']]\n",
      "start ----------------------------\n",
      "[[' ' ' ' 'X' ' ']\n",
      " ['X' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ']\n",
      " [' ' ' ' 'X' 'O']]\n",
      "['left', 'left', 'down']\n",
      "[[' ' ' ' ' ' ' ']\n",
      " ['X' ' ' 'X' ' ']\n",
      " [' ' ' ' ' ' ' ']\n",
      " [' ' 'X' ' ' 'O']]\n",
      "['left', 'left', 'right']\n",
      "[[' ' ' ' ' ' ' ']\n",
      " ['X' ' ' ' ' 'X']\n",
      " [' ' ' ' ' ' ' ']\n",
      " ['X' ' ' ' ' 'O']]\n",
      "['down', 'down', 'down']\n",
      "[[' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ']\n",
      " ['X' ' ' ' ' 'X']\n",
      " ['X' ' ' ' ' 'O']]\n",
      "['right', 'right', 'right']\n",
      "[[' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ']\n",
      " [' ' 'X' ' ' 'X']\n",
      " [' ' 'X' ' ' 'O']]\n",
      "['right', 'right', 'down']\n",
      "[[' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ']\n",
      " [' ' ' ' 'X' 'X']\n",
      " [' ' ' ' 'X' 'O']]\n"
     ]
    }
   ],
   "source": [
    "env = SpiderFlyEnvMA(size = 4, spiders = 3, max_timesteps = 100, render_mode = \"ascii\")\n",
    "env = PettingZooWrapper(env)\n",
    "\n",
    "terminal = False\n",
    "print(\"start ----------------------------\")\n",
    "obs, _ = env.reset()\n",
    "while not terminal:\n",
    "    actions = []\n",
    "    actions = model.get_actions(obs, deterministic = True)\n",
    "    print([env.action_to_direction_string[act] for act in actions])\n",
    "\n",
    "    obs, rewards, terminals, truncations, infos = env.step(actions)\n",
    "    if rewards[0] == 1:\n",
    "        break\n",
    "    # print(obs, rewards, terminals, truncations, infos, actions)\n",
    "    # print(\"--------------------\")\n",
    "    \n",
    "    terminal = terminals[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
