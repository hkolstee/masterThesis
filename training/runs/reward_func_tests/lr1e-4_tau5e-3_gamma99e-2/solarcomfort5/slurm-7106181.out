The following modules were not unloaded:
  (Use "module --force purge" to unload all):

  1) 2023.01   2) StdEnv

The following have been reloaded with a version change:
  1) GCCcore/12.2.0 => GCCcore/11.3.0
  2) GMP/6.2.1-GCCcore-12.2.0 => GMP/6.2.1-GCCcore-11.3.0
  3) Python/3.10.8-GCCcore-12.2.0 => Python/3.10.4-GCCcore-11.3.0
  4) SQLite/3.39.4-GCCcore-12.2.0 => SQLite/3.38.3-GCCcore-11.3.0
  5) Tcl/8.6.12-GCCcore-12.2.0 => Tcl/8.6.12-GCCcore-11.3.0
  6) XZ/5.2.7-GCCcore-12.2.0 => XZ/5.2.5-GCCcore-11.3.0
  7) binutils/2.39-GCCcore-12.2.0 => binutils/2.38-GCCcore-11.3.0
  8) bzip2/1.0.8-GCCcore-12.2.0 => bzip2/1.0.8-GCCcore-11.3.0
  9) libffi/3.4.4-GCCcore-12.2.0 => libffi/3.4.2-GCCcore-11.3.0
 10) libreadline/8.2-GCCcore-12.2.0 => libreadline/8.1.2-GCCcore-11.3.0
 11) ncurses/6.3-GCCcore-12.2.0 => ncurses/6.3-GCCcore-11.3.0
 12) zlib/1.2.12-GCCcore-12.2.0 => zlib/1.2.12-GCCcore-11.3.0


The following have been reloaded with a version change:
  1) FFTW/3.3.10-GCC-11.3.0 => FFTW/3.3.10-GCC-12.3.0
  2) FlexiBLAS/3.2.0-GCC-11.3.0 => FlexiBLAS/3.3.1-GCC-12.3.0
  3) GCC/11.3.0 => GCC/12.3.0
  4) GCCcore/11.3.0 => GCCcore/12.3.0
  5) OpenBLAS/0.3.20-GCC-11.3.0 => OpenBLAS/0.3.23-GCC-12.3.0
  6) Python/3.10.4-GCCcore-11.3.0 => Python/3.11.3-GCCcore-12.3.0
  7) SQLite/3.38.3-GCCcore-11.3.0 => SQLite/3.42.0-GCCcore-12.3.0
  8) SciPy-bundle/2022.05-foss-2022a => SciPy-bundle/2023.07-gfbf-2023a
  9) Tcl/8.6.12-GCCcore-11.3.0 => Tcl/8.6.13-GCCcore-12.3.0
 10) XZ/5.2.5-GCCcore-11.3.0 => XZ/5.4.2-GCCcore-12.3.0
 11) binutils/2.38-GCCcore-11.3.0 => binutils/2.40-GCCcore-12.3.0
 12) bzip2/1.0.8-GCCcore-11.3.0 => bzip2/1.0.8-GCCcore-12.3.0
 13) libffi/3.4.2-GCCcore-11.3.0 => libffi/3.4.4-GCCcore-12.3.0
 14) libreadline/8.1.2-GCCcore-11.3.0 => libreadline/8.2-GCCcore-12.3.0
 15) ncurses/6.3-GCCcore-11.3.0 => ncurses/6.4-GCCcore-12.3.0
 16) pybind11/2.9.2-GCCcore-11.3.0 => pybind11/2.11.1-GCCcore-12.3.0
 17) zlib/1.2.12-GCCcore-11.3.0 => zlib/1.2.13-GCCcore-12.3.0


The following have been reloaded with a version change:
  1) GCCcore/12.3.0 => GCCcore/11.3.0
  2) Python/3.11.3-GCCcore-12.3.0 => Python/3.10.4-GCCcore-11.3.0
  3) SQLite/3.42.0-GCCcore-12.3.0 => SQLite/3.38.3-GCCcore-11.3.0
  4) SciPy-bundle/2023.07-gfbf-2023a => SciPy-bundle/2022.05-foss-2022a
  5) Tcl/8.6.13-GCCcore-12.3.0 => Tcl/8.6.12-GCCcore-11.3.0
  6) XZ/5.4.2-GCCcore-12.3.0 => XZ/5.2.5-GCCcore-11.3.0
  7) binutils/2.40-GCCcore-12.3.0 => binutils/2.38-GCCcore-11.3.0
  8) bzip2/1.0.8-GCCcore-12.3.0 => bzip2/1.0.8-GCCcore-11.3.0
  9) libffi/3.4.4-GCCcore-12.3.0 => libffi/3.4.2-GCCcore-11.3.0
 10) libreadline/8.2-GCCcore-12.3.0 => libreadline/8.1.2-GCCcore-11.3.0
 11) ncurses/6.4-GCCcore-12.3.0 => ncurses/6.3-GCCcore-11.3.0
 12) pybind11/2.11.1-GCCcore-12.3.0 => pybind11/2.9.2-GCCcore-11.3.0
 13) zlib/1.2.13-GCCcore-12.3.0 => zlib/1.2.12-GCCcore-11.3.0

/home2/s3515249/masterThesis/venv/lib/python3.10/site-packages/stable_baselines3/common/vec_env/patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.
  warnings.warn(
/home2/s3515249/masterThesis/venv/lib/python3.10/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.
  warnings.warn(
/home2/s3515249/masterThesis/venv/lib/python3.10/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.
  warnings.warn(
Eval num_timesteps=18000, episode_reward=-100624.84 +/- 0.00
Episode length: 719.00 +/- 0.00
New best mean reward!
Eval num_timesteps=36000, episode_reward=-113344.56 +/- 0.00
Episode length: 719.00 +/- 0.00
Eval num_timesteps=54000, episode_reward=-106512.57 +/- 0.00
Episode length: 719.00 +/- 0.00
Eval num_timesteps=72000, episode_reward=-128561.84 +/- 0.00
Episode length: 719.00 +/- 0.00
Eval num_timesteps=90000, episode_reward=-116520.65 +/- 0.00
Episode length: 719.00 +/- 0.00
Eval num_timesteps=108000, episode_reward=-104037.36 +/- 0.00
Episode length: 719.00 +/- 0.00
Eval num_timesteps=126000, episode_reward=-87453.39 +/- 0.00
Episode length: 719.00 +/- 0.00
New best mean reward!
Eval num_timesteps=144000, episode_reward=-89022.42 +/- 0.00
Episode length: 719.00 +/- 0.00
Eval num_timesteps=162000, episode_reward=-82606.68 +/- 0.00
Episode length: 719.00 +/- 0.00
New best mean reward!
Eval num_timesteps=180000, episode_reward=-57291.97 +/- 0.00
Episode length: 719.00 +/- 0.00
New best mean reward!
Eval num_timesteps=198000, episode_reward=-50323.74 +/- 0.00
Episode length: 719.00 +/- 0.00
New best mean reward!
Eval num_timesteps=216000, episode_reward=-46869.11 +/- 0.00
Episode length: 719.00 +/- 0.00
New best mean reward!
Eval num_timesteps=234000, episode_reward=-58291.92 +/- 0.00
Episode length: 719.00 +/- 0.00
Eval num_timesteps=252000, episode_reward=-57778.03 +/- 0.00
Episode length: 719.00 +/- 0.00
Eval num_timesteps=270000, episode_reward=-49652.76 +/- 0.00
Episode length: 719.00 +/- 0.00
Eval num_timesteps=288000, episode_reward=-41734.24 +/- 0.00
Episode length: 719.00 +/- 0.00
New best mean reward!
Eval num_timesteps=306000, episode_reward=-40884.30 +/- 0.00
Episode length: 719.00 +/- 0.00
New best mean reward!
Eval num_timesteps=324000, episode_reward=-41143.13 +/- 0.00
Episode length: 719.00 +/- 0.00
Eval num_timesteps=342000, episode_reward=-50493.09 +/- 0.00
Episode length: 719.00 +/- 0.00
Eval num_timesteps=360000, episode_reward=-49669.35 +/- 0.00
Episode length: 719.00 +/- 0.00
Eval num_timesteps=378000, episode_reward=-58498.42 +/- 0.00
Episode length: 719.00 +/- 0.00
Eval num_timesteps=396000, episode_reward=-47407.31 +/- 0.00
Episode length: 719.00 +/- 0.00
Eval num_timesteps=414000, episode_reward=-45131.52 +/- 0.00
Episode length: 719.00 +/- 0.00
Eval num_timesteps=432000, episode_reward=-42937.53 +/- 0.00
Episode length: 719.00 +/- 0.00
Eval num_timesteps=450000, episode_reward=-37599.98 +/- 0.00
Episode length: 719.00 +/- 0.00
New best mean reward!
Eval num_timesteps=468000, episode_reward=-32254.52 +/- 0.00
Episode length: 719.00 +/- 0.00
New best mean reward!
Eval num_timesteps=486000, episode_reward=-28558.24 +/- 0.00
Episode length: 719.00 +/- 0.00
New best mean reward!
Eval num_timesteps=504000, episode_reward=-28753.12 +/- 0.00
Episode length: 719.00 +/- 0.00
Eval num_timesteps=522000, episode_reward=-27678.09 +/- 0.00
Episode length: 719.00 +/- 0.00
New best mean reward!
Eval num_timesteps=540000, episode_reward=-26945.53 +/- 0.00
Episode length: 719.00 +/- 0.00
New best mean reward!
Eval num_timesteps=558000, episode_reward=-25121.42 +/- 0.00
Episode length: 719.00 +/- 0.00
New best mean reward!
Eval num_timesteps=576000, episode_reward=-25842.69 +/- 0.00
Episode length: 719.00 +/- 0.00
Eval num_timesteps=594000, episode_reward=-24298.20 +/- 0.00
Episode length: 719.00 +/- 0.00
New best mean reward!
Eval num_timesteps=612000, episode_reward=-21396.81 +/- 0.00
Episode length: 719.00 +/- 0.00
New best mean reward!
Eval num_timesteps=630000, episode_reward=-20081.56 +/- 0.00
Episode length: 719.00 +/- 0.00
New best mean reward!
Eval num_timesteps=648000, episode_reward=-20306.24 +/- 0.00
Episode length: 719.00 +/- 0.00
Eval num_timesteps=666000, episode_reward=-20153.75 +/- 0.00
Episode length: 719.00 +/- 0.00
Eval num_timesteps=684000, episode_reward=-19516.55 +/- 0.00
Episode length: 719.00 +/- 0.00
New best mean reward!
Eval num_timesteps=702000, episode_reward=-18991.74 +/- 0.00
Episode length: 719.00 +/- 0.00
New best mean reward!
Eval num_timesteps=720000, episode_reward=-18480.79 +/- 0.00
Episode length: 719.00 +/- 0.00
New best mean reward!
Eval num_timesteps=738000, episode_reward=-17941.39 +/- 0.00
Episode length: 719.00 +/- 0.00
New best mean reward!
Eval num_timesteps=756000, episode_reward=-17558.93 +/- 0.00
Episode length: 719.00 +/- 0.00
New best mean reward!
Eval num_timesteps=774000, episode_reward=-17302.81 +/- 0.00
Episode length: 719.00 +/- 0.00
New best mean reward!
Eval num_timesteps=792000, episode_reward=-16700.54 +/- 0.00
Episode length: 719.00 +/- 0.00
New best mean reward!
Eval num_timesteps=810000, episode_reward=-16265.78 +/- 0.00
Episode length: 719.00 +/- 0.00
New best mean reward!
Eval num_timesteps=828000, episode_reward=-16279.70 +/- 0.00
Episode length: 719.00 +/- 0.00
Eval num_timesteps=846000, episode_reward=-16502.05 +/- 0.00
Episode length: 719.00 +/- 0.00
Eval num_timesteps=864000, episode_reward=-16092.30 +/- 0.00
Episode length: 719.00 +/- 0.00
New best mean reward!
Eval num_timesteps=882000, episode_reward=-15966.85 +/- 0.00
Episode length: 719.00 +/- 0.00
New best mean reward!
Eval num_timesteps=900000, episode_reward=-15446.05 +/- 0.00
Episode length: 719.00 +/- 0.00
New best mean reward!
Eval num_timesteps=918000, episode_reward=-15233.78 +/- 0.00
Episode length: 719.00 +/- 0.00
New best mean reward!
Eval num_timesteps=936000, episode_reward=-14839.97 +/- 0.00
Episode length: 719.00 +/- 0.00
New best mean reward!
Eval num_timesteps=954000, episode_reward=-14817.57 +/- 0.00
Episode length: 719.00 +/- 0.00
New best mean reward!
Eval num_timesteps=972000, episode_reward=-14743.50 +/- 0.00
Episode length: 719.00 +/- 0.00
New best mean reward!
Eval num_timesteps=990000, episode_reward=-14022.90 +/- 0.00
Episode length: 719.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1008000, episode_reward=-14041.38 +/- 0.00
Episode length: 719.00 +/- 0.00
Eval num_timesteps=1026000, episode_reward=-14296.09 +/- 0.00
Episode length: 719.00 +/- 0.00
Eval num_timesteps=1044000, episode_reward=-13799.66 +/- 0.00
Episode length: 719.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1062000, episode_reward=-13221.98 +/- 0.00
Episode length: 719.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1080000, episode_reward=-12796.27 +/- 0.00
Episode length: 719.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1098000, episode_reward=-12444.82 +/- 0.00
Episode length: 719.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1116000, episode_reward=-12076.74 +/- 0.00
Episode length: 719.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1134000, episode_reward=-11932.53 +/- 0.00
Episode length: 719.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1152000, episode_reward=-11634.13 +/- 0.00
Episode length: 719.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1170000, episode_reward=-11635.64 +/- 0.00
Episode length: 719.00 +/- 0.00
Eval num_timesteps=1188000, episode_reward=-11371.39 +/- 0.00
Episode length: 719.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1206000, episode_reward=-11392.27 +/- 0.00
Episode length: 719.00 +/- 0.00
Eval num_timesteps=1224000, episode_reward=-11158.10 +/- 0.00
Episode length: 719.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1242000, episode_reward=-11105.39 +/- 0.00
Episode length: 719.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1260000, episode_reward=-10691.16 +/- 0.00
Episode length: 719.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1278000, episode_reward=-10543.14 +/- 0.00
Episode length: 719.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1296000, episode_reward=-10456.18 +/- 0.00
Episode length: 719.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1314000, episode_reward=-10369.86 +/- 0.00
Episode length: 719.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1332000, episode_reward=-10360.23 +/- 0.00
Episode length: 719.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1350000, episode_reward=-10159.20 +/- 0.00
Episode length: 719.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1368000, episode_reward=-10074.75 +/- 0.00
Episode length: 719.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1386000, episode_reward=-9932.17 +/- 0.00
Episode length: 719.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1404000, episode_reward=-9743.46 +/- 0.00
Episode length: 719.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1422000, episode_reward=-9591.71 +/- 0.00
Episode length: 719.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1440000, episode_reward=-9400.21 +/- 0.00
Episode length: 719.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1458000, episode_reward=-9374.87 +/- 0.00
Episode length: 719.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1476000, episode_reward=-9340.88 +/- 0.00
Episode length: 719.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1494000, episode_reward=-9262.00 +/- 0.00
Episode length: 719.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1512000, episode_reward=-9198.22 +/- 0.00
Episode length: 719.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1530000, episode_reward=-8976.54 +/- 0.00
Episode length: 719.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1548000, episode_reward=-9042.45 +/- 0.00
Episode length: 719.00 +/- 0.00
Eval num_timesteps=1566000, episode_reward=-9006.70 +/- 0.00
Episode length: 719.00 +/- 0.00
Eval num_timesteps=1584000, episode_reward=-8924.47 +/- 0.00
Episode length: 719.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1602000, episode_reward=-8769.16 +/- 0.00
Episode length: 719.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1620000, episode_reward=-8727.39 +/- 0.00
Episode length: 719.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1638000, episode_reward=-8675.80 +/- 0.00
Episode length: 719.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1656000, episode_reward=-8715.04 +/- 0.00
Episode length: 719.00 +/- 0.00
Eval num_timesteps=1674000, episode_reward=-8650.45 +/- 0.00
Episode length: 719.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1692000, episode_reward=-8491.77 +/- 0.00
Episode length: 719.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1710000, episode_reward=-8410.63 +/- 0.00
Episode length: 719.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1728000, episode_reward=-8281.80 +/- 0.00
Episode length: 719.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1746000, episode_reward=-8147.71 +/- 0.00
Episode length: 719.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1764000, episode_reward=-8179.60 +/- 0.00
Episode length: 719.00 +/- 0.00
Eval num_timesteps=1782000, episode_reward=-8078.89 +/- 0.00
Episode length: 719.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1800000, episode_reward=-8109.72 +/- 0.00
Episode length: 719.00 +/- 0.00
Eval num_timesteps=1818000, episode_reward=-7897.23 +/- 0.00
Episode length: 719.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1836000, episode_reward=-7825.79 +/- 0.00
Episode length: 719.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1854000, episode_reward=-7851.77 +/- 0.00
Episode length: 719.00 +/- 0.00
Eval num_timesteps=1872000, episode_reward=-7786.97 +/- 0.00
Episode length: 719.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1890000, episode_reward=-7756.65 +/- 0.00
Episode length: 719.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1908000, episode_reward=-7640.39 +/- 0.00
Episode length: 719.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1926000, episode_reward=-7631.67 +/- 0.00
Episode length: 719.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1944000, episode_reward=-7656.24 +/- 0.00
Episode length: 719.00 +/- 0.00
Eval num_timesteps=1962000, episode_reward=-7494.44 +/- 0.00
Episode length: 719.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1980000, episode_reward=-7454.64 +/- 0.00
Episode length: 719.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1998000, episode_reward=-7402.16 +/- 0.00
Episode length: 719.00 +/- 0.00
New best mean reward!
Eval num_timesteps=2016000, episode_reward=-7314.17 +/- 0.00
Episode length: 719.00 +/- 0.00
New best mean reward!
Eval num_timesteps=2034000, episode_reward=-7348.51 +/- 0.00
Episode length: 719.00 +/- 0.00
Eval num_timesteps=2052000, episode_reward=-7281.43 +/- 0.00
Episode length: 719.00 +/- 0.00
New best mean reward!
Eval num_timesteps=2070000, episode_reward=-7223.75 +/- 0.00
Episode length: 719.00 +/- 0.00
New best mean reward!
Eval num_timesteps=2088000, episode_reward=-7187.32 +/- 0.00
Episode length: 719.00 +/- 0.00
New best mean reward!
Eval num_timesteps=2106000, episode_reward=-7109.21 +/- 0.00
Episode length: 719.00 +/- 0.00
New best mean reward!
Eval num_timesteps=2124000, episode_reward=-7033.08 +/- 0.00
Episode length: 719.00 +/- 0.00
New best mean reward!
Eval num_timesteps=2142000, episode_reward=-7045.08 +/- 0.00
Episode length: 719.00 +/- 0.00
Eval num_timesteps=2160000, episode_reward=-6970.91 +/- 0.00
Episode length: 719.00 +/- 0.00
New best mean reward!
Eval num_timesteps=2178000, episode_reward=-6915.30 +/- 0.00
Episode length: 719.00 +/- 0.00
New best mean reward!
Eval num_timesteps=2196000, episode_reward=-6871.38 +/- 0.00
Episode length: 719.00 +/- 0.00
New best mean reward!
Eval num_timesteps=2214000, episode_reward=-6774.64 +/- 0.00
Episode length: 719.00 +/- 0.00
New best mean reward!
Eval num_timesteps=2232000, episode_reward=-6816.32 +/- 0.00
Episode length: 719.00 +/- 0.00
Eval num_timesteps=2250000, episode_reward=-6713.18 +/- 0.00
Episode length: 719.00 +/- 0.00
New best mean reward!
Eval num_timesteps=2268000, episode_reward=-6694.05 +/- 0.00
Episode length: 719.00 +/- 0.00
New best mean reward!
Eval num_timesteps=2286000, episode_reward=-6685.78 +/- 0.00
Episode length: 719.00 +/- 0.00
New best mean reward!
Eval num_timesteps=2304000, episode_reward=-6596.40 +/- 0.00
Episode length: 719.00 +/- 0.00
New best mean reward!
Eval num_timesteps=2322000, episode_reward=-6577.40 +/- 0.00
Episode length: 719.00 +/- 0.00
New best mean reward!
Eval num_timesteps=2340000, episode_reward=-6513.29 +/- 0.00
Episode length: 719.00 +/- 0.00
New best mean reward!
Eval num_timesteps=2358000, episode_reward=-6473.40 +/- 0.00
Episode length: 719.00 +/- 0.00
New best mean reward!
Eval num_timesteps=2376000, episode_reward=-6515.49 +/- 0.00
Episode length: 719.00 +/- 0.00
Eval num_timesteps=2394000, episode_reward=-6440.97 +/- 0.00
Episode length: 719.00 +/- 0.00
New best mean reward!
Eval num_timesteps=2412000, episode_reward=-6436.97 +/- 0.00
Episode length: 719.00 +/- 0.00
New best mean reward!
Eval num_timesteps=2430000, episode_reward=-6381.16 +/- 0.00
Episode length: 719.00 +/- 0.00
New best mean reward!
Eval num_timesteps=2448000, episode_reward=-6397.11 +/- 0.00
Episode length: 719.00 +/- 0.00
Eval num_timesteps=2466000, episode_reward=-6355.44 +/- 0.00
Episode length: 719.00 +/- 0.00
New best mean reward!
Eval num_timesteps=2484000, episode_reward=-6296.38 +/- 0.00
Episode length: 719.00 +/- 0.00
New best mean reward!
Eval num_timesteps=2502000, episode_reward=-6286.81 +/- 0.00
Episode length: 719.00 +/- 0.00
New best mean reward!
Eval num_timesteps=2520000, episode_reward=-6262.91 +/- 0.00
Episode length: 719.00 +/- 0.00
New best mean reward!
Eval num_timesteps=2538000, episode_reward=-6263.21 +/- 0.00
Episode length: 719.00 +/- 0.00
Eval num_timesteps=2556000, episode_reward=-6240.62 +/- 0.00
Episode length: 719.00 +/- 0.00
New best mean reward!
Eval num_timesteps=2574000, episode_reward=-6234.46 +/- 0.00
Episode length: 719.00 +/- 0.00
New best mean reward!
Eval num_timesteps=2592000, episode_reward=-6227.48 +/- 0.00
Episode length: 719.00 +/- 0.00
New best mean reward!
Eval num_timesteps=2610000, episode_reward=-6212.63 +/- 0.00
Episode length: 719.00 +/- 0.00
New best mean reward!
Eval num_timesteps=2628000, episode_reward=-6176.65 +/- 0.00
Episode length: 719.00 +/- 0.00
New best mean reward!
Eval num_timesteps=2646000, episode_reward=-6141.61 +/- 0.00
Episode length: 719.00 +/- 0.00
New best mean reward!
Eval num_timesteps=2664000, episode_reward=-6106.98 +/- 0.00
Episode length: 719.00 +/- 0.00
New best mean reward!
Eval num_timesteps=2682000, episode_reward=-6111.00 +/- 0.00
Episode length: 719.00 +/- 0.00
Eval num_timesteps=2700000, episode_reward=-6089.22 +/- 0.00
Episode length: 719.00 +/- 0.00
New best mean reward!
Eval num_timesteps=2718000, episode_reward=-6056.26 +/- 0.00
Episode length: 719.00 +/- 0.00
New best mean reward!
Eval num_timesteps=2736000, episode_reward=-6059.73 +/- 0.00
Episode length: 719.00 +/- 0.00
Eval num_timesteps=2754000, episode_reward=-6019.99 +/- 0.00
Episode length: 719.00 +/- 0.00
New best mean reward!
Eval num_timesteps=2772000, episode_reward=-6026.67 +/- 0.00
Episode length: 719.00 +/- 0.00
Eval num_timesteps=2790000, episode_reward=-5900.29 +/- 0.00
Episode length: 719.00 +/- 0.00
New best mean reward!
Eval num_timesteps=2808000, episode_reward=-5901.14 +/- 0.00
Episode length: 719.00 +/- 0.00
Eval num_timesteps=2826000, episode_reward=-5870.12 +/- 0.00
Episode length: 719.00 +/- 0.00
New best mean reward!
Eval num_timesteps=2844000, episode_reward=-5872.62 +/- 0.00
Episode length: 719.00 +/- 0.00
Eval num_timesteps=2862000, episode_reward=-5869.46 +/- 0.00
Episode length: 719.00 +/- 0.00
New best mean reward!
Eval num_timesteps=2880000, episode_reward=-5864.61 +/- 0.00
Episode length: 719.00 +/- 0.00
New best mean reward!
Eval num_timesteps=2898000, episode_reward=-5831.35 +/- 0.00
Episode length: 719.00 +/- 0.00
New best mean reward!
Eval num_timesteps=2916000, episode_reward=-5820.34 +/- 0.00
Episode length: 719.00 +/- 0.00
New best mean reward!
Eval num_timesteps=2934000, episode_reward=-5823.10 +/- 0.00
Episode length: 719.00 +/- 0.00
Eval num_timesteps=2952000, episode_reward=-5802.91 +/- 0.00
Episode length: 719.00 +/- 0.00
New best mean reward!
Eval num_timesteps=2970000, episode_reward=-5784.29 +/- 0.00
Episode length: 719.00 +/- 0.00
New best mean reward!
Eval num_timesteps=2988000, episode_reward=-5772.78 +/- 0.00
Episode length: 719.00 +/- 0.00
New best mean reward!
Eval num_timesteps=3006000, episode_reward=-5770.21 +/- 0.00
Episode length: 719.00 +/- 0.00
New best mean reward!
Eval num_timesteps=3024000, episode_reward=-5744.33 +/- 0.00
Episode length: 719.00 +/- 0.00
New best mean reward!
Eval num_timesteps=3042000, episode_reward=-5760.00 +/- 0.00
Episode length: 719.00 +/- 0.00
Eval num_timesteps=3060000, episode_reward=-5729.47 +/- 0.00
Episode length: 719.00 +/- 0.00
New best mean reward!
Eval num_timesteps=3078000, episode_reward=-5712.26 +/- 0.00
Episode length: 719.00 +/- 0.00
New best mean reward!
Eval num_timesteps=3096000, episode_reward=-5726.20 +/- 0.00
Episode length: 719.00 +/- 0.00
Eval num_timesteps=3114000, episode_reward=-5672.96 +/- 0.00
Episode length: 719.00 +/- 0.00
New best mean reward!
Eval num_timesteps=3132000, episode_reward=-5673.56 +/- 0.00
Episode length: 719.00 +/- 0.00
Eval num_timesteps=3150000, episode_reward=-5640.63 +/- 0.00
Episode length: 719.00 +/- 0.00
New best mean reward!
Eval num_timesteps=3168000, episode_reward=-5638.48 +/- 0.00
Episode length: 719.00 +/- 0.00
New best mean reward!
Eval num_timesteps=3186000, episode_reward=-5623.97 +/- 0.00
Episode length: 719.00 +/- 0.00
New best mean reward!
Eval num_timesteps=3204000, episode_reward=-5609.84 +/- 0.00
Episode length: 719.00 +/- 0.00
New best mean reward!
Eval num_timesteps=3222000, episode_reward=-5586.24 +/- 0.00
Episode length: 719.00 +/- 0.00
New best mean reward!
Eval num_timesteps=3240000, episode_reward=-5596.42 +/- 0.00
Episode length: 719.00 +/- 0.00
Eval num_timesteps=3258000, episode_reward=-5583.23 +/- 0.00
Episode length: 719.00 +/- 0.00
New best mean reward!
Eval num_timesteps=3276000, episode_reward=-5570.67 +/- 0.00
Episode length: 719.00 +/- 0.00
New best mean reward!
Eval num_timesteps=3294000, episode_reward=-5557.52 +/- 0.00
Episode length: 719.00 +/- 0.00
New best mean reward!
Eval num_timesteps=3312000, episode_reward=-5531.77 +/- 0.00
Episode length: 719.00 +/- 0.00
New best mean reward!
Eval num_timesteps=3330000, episode_reward=-5540.91 +/- 0.00
Episode length: 719.00 +/- 0.00
Eval num_timesteps=3348000, episode_reward=-5532.28 +/- 0.00
Episode length: 719.00 +/- 0.00
Eval num_timesteps=3366000, episode_reward=-5518.93 +/- 0.00
Episode length: 719.00 +/- 0.00
New best mean reward!
Eval num_timesteps=3384000, episode_reward=-5502.12 +/- 0.00
Episode length: 719.00 +/- 0.00
New best mean reward!
Eval num_timesteps=3402000, episode_reward=-5514.32 +/- 0.00
Episode length: 719.00 +/- 0.00
Eval num_timesteps=3420000, episode_reward=-5504.61 +/- 0.00
Episode length: 719.00 +/- 0.00
Eval num_timesteps=3438000, episode_reward=-5495.29 +/- 0.00
Episode length: 719.00 +/- 0.00
New best mean reward!
Eval num_timesteps=3456000, episode_reward=-5507.67 +/- 0.00
Episode length: 719.00 +/- 0.00
Eval num_timesteps=3474000, episode_reward=-5516.54 +/- 0.00
Episode length: 719.00 +/- 0.00
Eval num_timesteps=3492000, episode_reward=-5505.71 +/- 0.00
Episode length: 719.00 +/- 0.00
Eval num_timesteps=3510000, episode_reward=-5488.33 +/- 0.00
Episode length: 719.00 +/- 0.00
New best mean reward!
Eval num_timesteps=3528000, episode_reward=-5456.59 +/- 0.00
Episode length: 719.00 +/- 0.00
New best mean reward!
Eval num_timesteps=3546000, episode_reward=-5462.94 +/- 0.00
Episode length: 719.00 +/- 0.00
Eval num_timesteps=3564000, episode_reward=-5484.88 +/- 0.00
Episode length: 719.00 +/- 0.00
Eval num_timesteps=3582000, episode_reward=-5465.37 +/- 0.00
Episode length: 719.00 +/- 0.00
Eval num_timesteps=3600000, episode_reward=-5434.90 +/- 0.00
Episode length: 719.00 +/- 0.00
New best mean reward!
Eval num_timesteps=3618000, episode_reward=-5449.47 +/- 0.00
Episode length: 719.00 +/- 0.00
Eval num_timesteps=3636000, episode_reward=-5436.40 +/- 0.00
Episode length: 719.00 +/- 0.00
Eval num_timesteps=3654000, episode_reward=-5397.45 +/- 0.00
Episode length: 719.00 +/- 0.00
New best mean reward!
Eval num_timesteps=3672000, episode_reward=-5411.22 +/- 0.00
Episode length: 719.00 +/- 0.00
Eval num_timesteps=3690000, episode_reward=-5383.30 +/- 0.00
Episode length: 719.00 +/- 0.00
New best mean reward!
Eval num_timesteps=3708000, episode_reward=-5386.34 +/- 0.00
Episode length: 719.00 +/- 0.00
Eval num_timesteps=3726000, episode_reward=-5398.71 +/- 0.00
Episode length: 719.00 +/- 0.00
Eval num_timesteps=3744000, episode_reward=-5356.26 +/- 0.00
Episode length: 719.00 +/- 0.00
New best mean reward!
Eval num_timesteps=3762000, episode_reward=-5346.93 +/- 0.00
Episode length: 719.00 +/- 0.00
New best mean reward!
Eval num_timesteps=3780000, episode_reward=-5322.24 +/- 0.00
Episode length: 719.00 +/- 0.00
New best mean reward!
Eval num_timesteps=3798000, episode_reward=-5311.84 +/- 0.00
Episode length: 719.00 +/- 0.00
New best mean reward!
Eval num_timesteps=3816000, episode_reward=-5294.40 +/- 0.00
Episode length: 719.00 +/- 0.00
New best mean reward!
Eval num_timesteps=3834000, episode_reward=-5306.06 +/- 0.00
Episode length: 719.00 +/- 0.00
Eval num_timesteps=3852000, episode_reward=-5307.05 +/- 0.00
Episode length: 719.00 +/- 0.00
Eval num_timesteps=3870000, episode_reward=-5293.35 +/- 0.00
Episode length: 719.00 +/- 0.00
New best mean reward!
Eval num_timesteps=3888000, episode_reward=-5283.40 +/- 0.00
Episode length: 719.00 +/- 0.00
New best mean reward!
Eval num_timesteps=3906000, episode_reward=-5271.19 +/- 0.00
Episode length: 719.00 +/- 0.00
New best mean reward!
Eval num_timesteps=3924000, episode_reward=-5270.90 +/- 0.00
Episode length: 719.00 +/- 0.00
New best mean reward!
Eval num_timesteps=3942000, episode_reward=-5270.72 +/- 0.00
Episode length: 719.00 +/- 0.00
New best mean reward!
Eval num_timesteps=3960000, episode_reward=-5252.28 +/- 0.00
Episode length: 719.00 +/- 0.00
New best mean reward!
Done

###############################################################################
H치br칩k Cluster
Job 7106181 for user s3515249
Finished at: Tue Feb  6 09:56:02 CET 2024

Job details:
============

Job ID              : 7106181
Name                : citylearn_simple_policy_test
User                : s3515249
Partition           : gpulong
Nodes               : v100gpu9
Number of Nodes     : 1
Cores               : 1
Number of Tasks     : 1
State               : COMPLETED
Submit              : 2024-02-03T23:37:23
Start               : 2024-02-04T13:12:46
End                 : 2024-02-06T09:56:02
Reserved walltime   : 2-04:00:00
Used walltime       : 1-20:43:16
Used CPU time       : 1-20:21:15 (efficiency: 99.18%)
% User (Computation): 96.94%
% System (I/O)      :  3.06%
Mem reserved        : 14000M
Max Mem (Node/step) : 11.09G (v100gpu9, per node)
Full Max Mem usage  : 11.09G
Total Disk Read     : 3.78G
Total Disk Write    : 543.84M

Acknowledgements:
=================

Please see this page for information about acknowledging H치br칩k in your publications:

https://wiki.hpc.rug.nl/habrok/introduction/scientific_output

################################################################################
