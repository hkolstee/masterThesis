{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training of a simple policy using the custom reward function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "import math\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.21.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gym\n",
    "gym.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from citylearn.citylearn import CityLearnEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from citylearn.wrappers import NormalizedObservationWrapper, StableBaselines3Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import SAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.schemas.warm_up.custom_reward import CustomReward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function and a wrapper class as given in the local evaluation script provided by the challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WrapperEnv:\n",
    "    \"\"\"\n",
    "    Env to wrap provide Citylearn Env data without providing full env\n",
    "    Preventing attribute access outside of the available functions\n",
    "    \"\"\"\n",
    "    def __init__(self, env_data):\n",
    "        self.observation_names = env_data['observation_names']\n",
    "        self.action_names = env_data['action_names']\n",
    "        self.observation_space = env_data['observation_space']\n",
    "        self.action_space = env_data['action_space']\n",
    "        self.time_steps = env_data['time_steps']\n",
    "        self.seconds_per_time_step = env_data['seconds_per_time_step']\n",
    "        self.random_seed = env_data['random_seed']\n",
    "        self.buildings_metadata = env_data['buildings_metadata']\n",
    "        self.episode_tracker = env_data['episode_tracker']\n",
    "    \n",
    "    def get_metadata(self):\n",
    "        return {'buildings': self.buildings_metadata}\n",
    "\n",
    "def create_citylearn_env(schema_path, reward_function, central_agent):\n",
    "    env = CityLearnEnv(schema=schema_path, reward_function=reward_function, central_agent=central_agent)\n",
    "\n",
    "    env_data = dict(\n",
    "        observation_names = env.observation_names,\n",
    "        action_names = env.action_names,\n",
    "        observation_space = env.observation_space,\n",
    "        action_space = env.action_space,\n",
    "        time_steps = env.time_steps,\n",
    "        random_seed = None,\n",
    "        episode_tracker = None,\n",
    "        seconds_per_time_step = None,\n",
    "        buildings_metadata = env.get_metadata()['buildings']\n",
    "    )\n",
    "\n",
    "    wrapper_env = WrapperEnv(env_data)\n",
    "    return env, wrapper_env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_path = os.path.join(\"./data/\", \"schemas/warm_up/schema_edited.json\")\n",
    "\n",
    "env, wrapper_env = create_citylearn_env(schema_path, CustomReward, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.get_metadata()\n",
    "# env.reward_function.env_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare for SB3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = NormalizedObservationWrapper(env)\n",
    "env = StableBaselines3Wrapper(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create SAC model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hkolstee/.local/lib/python3.10/site-packages/torch/cuda/__init__.py:138: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11040). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n",
      "/home/hkolstee/.local/lib/python3.10/site-packages/stable_baselines3/common/vec_env/patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = SAC(\"MlpPolicy\", env, tensorboard_log=\"./tensorboard_logs/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REWARD 1.0 \n",
      "\n",
      "REWARD 1.499272032714746 \n",
      "\n",
      "REWARD 1.5039870342773698 \n",
      "\n",
      "REWARD 1.836174170435223 \n",
      "\n",
      "REWARD 2.2639087140829735 \n",
      "\n",
      "REWARD 1.1031110343573667 \n",
      "\n",
      "REWARD 1.255072359574174 \n",
      "\n",
      "REWARD 0.7316527933595512 \n",
      "\n",
      "REWARD 0.7186417400580839 \n",
      "\n",
      "REWARD 0.743713514327694 \n",
      "\n",
      "REWARD 0.8140388055482499 \n",
      "\n",
      "REWARD 0.8244180577000697 \n",
      "\n",
      "REWARD 0.871522907977748 \n",
      "\n",
      "REWARD 0.8878689323221514 \n",
      "\n",
      "REWARD 0.9246658191032697 \n",
      "\n",
      "REWARD 0.9742653856887641 \n",
      "\n",
      "REWARD 1.009575574640963 \n",
      "\n",
      "REWARD 1.0035838409765352 \n",
      "\n",
      "REWARD 0.995676088727824 \n",
      "\n",
      "REWARD 1.297814031762374 \n",
      "\n",
      "REWARD 1.2868316203069938 \n",
      "\n",
      "REWARD 1.2731427341459352 \n",
      "\n",
      "REWARD 1.2811821610908947 \n",
      "\n",
      "REWARD 0.0 \n",
      "\n",
      "REWARD 2.1266483453736305 \n",
      "\n",
      "REWARD 1.2366513437216973 \n",
      "\n",
      "REWARD 1.3020502163684098 \n",
      "\n",
      "REWARD 0.9963603231134674 \n",
      "\n",
      "REWARD 0.9670627978731726 \n",
      "\n",
      "REWARD 0.8546953906747745 \n",
      "\n",
      "REWARD 0.7919193586461218 \n",
      "\n",
      "REWARD 0.8000756516506198 \n",
      "\n",
      "REWARD 0.9053514416920196 \n",
      "\n",
      "REWARD 0.7830654615869076 \n",
      "\n",
      "REWARD 0.7595699816443069 \n",
      "\n",
      "REWARD 0.6063021564593154 \n",
      "\n",
      "REWARD 0.652984903462053 \n",
      "\n",
      "REWARD 0.7155774502687205 \n",
      "\n",
      "REWARD 0.7324050645527588 \n",
      "\n",
      "REWARD 0.8051110477046481 \n",
      "\n",
      "REWARD 0.8832834068854989 \n",
      "\n",
      "REWARD 0.8839713833667735 \n",
      "\n",
      "REWARD 0.8684351313258076 \n",
      "\n",
      "REWARD 1.0183961279865767 \n",
      "\n",
      "REWARD 1.0060762997484416 \n",
      "\n",
      "REWARD 1.0282520183525312 \n",
      "\n",
      "REWARD 1.0064159622473925 \n",
      "\n",
      "REWARD 0.0 \n",
      "\n",
      "REWARD 2.354445219414104 \n",
      "\n",
      "REWARD 1.031912713234922 \n",
      "\n",
      "REWARD 1.05309721057053 \n",
      "\n",
      "REWARD 0.8829168549734439 \n",
      "\n",
      "REWARD 0.7343143959199945 \n",
      "\n",
      "REWARD 0.6634505797489646 \n",
      "\n",
      "REWARD 0.5898866027551916 \n",
      "\n",
      "REWARD 0.7476704671765454 \n",
      "\n",
      "REWARD 0.7470861530279783 \n",
      "\n",
      "REWARD 0.7818000034355349 \n",
      "\n",
      "REWARD 0.9598822615549035 \n",
      "\n",
      "REWARD 1.2076513536364069 \n",
      "\n",
      "REWARD 1.0983245090773623 \n",
      "\n",
      "REWARD 1.1339760754979136 \n",
      "\n",
      "REWARD 1.1615604016507017 \n",
      "\n",
      "REWARD 1.229070288548409 \n",
      "\n",
      "REWARD 1.2567502126523913 \n",
      "\n",
      "REWARD 1.2861318511336954 \n",
      "\n",
      "REWARD 1.2350460410228767 \n",
      "\n",
      "REWARD 1.2251422025376215 \n",
      "\n",
      "REWARD 1.2319980887480777 \n",
      "\n",
      "REWARD 1.2020775644391386 \n",
      "\n",
      "REWARD 1.1583189894811867 \n",
      "\n",
      "REWARD 0.0 \n",
      "\n",
      "REWARD 26.878550027257493 \n",
      "\n",
      "REWARD 1.2892114319914614 \n",
      "\n",
      "REWARD 1.0468077260170705 \n",
      "\n",
      "REWARD 0.6625171298240575 \n",
      "\n",
      "REWARD 0.8376539508779344 \n",
      "\n",
      "REWARD 0.5854241389215153 \n",
      "\n",
      "REWARD 0.7369104711345689 \n",
      "\n",
      "REWARD 0.4549988465123114 \n",
      "\n",
      "REWARD 0.5336408742792862 \n",
      "\n",
      "REWARD 0.6389893478533252 \n",
      "\n",
      "REWARD 0.7549204240787319 \n",
      "\n",
      "REWARD 0.7673945457879146 \n",
      "\n",
      "REWARD 0.8555668734052535 \n",
      "\n",
      "REWARD 0.8733374104548753 \n",
      "\n",
      "REWARD 0.8916185432451351 \n",
      "\n",
      "REWARD 0.7561837516652054 \n",
      "\n",
      "REWARD 0.7458860337177923 \n",
      "\n",
      "REWARD 0.7667294005807944 \n",
      "\n",
      "REWARD 0.766097747473522 \n",
      "\n",
      "REWARD 0.7967221397262295 \n",
      "\n",
      "REWARD 0.780800862563613 \n",
      "\n",
      "REWARD 0.9781606049363263 \n",
      "\n",
      "REWARD 0.9981768421763219 \n",
      "\n",
      "REWARD 0.0 \n",
      "\n",
      "REWARD 0.006898073127310919 \n",
      "\n",
      "REWARD 0.2382581456258655 \n",
      "\n",
      "REWARD 1.4732943934166989 \n",
      "\n",
      "REWARD 1.5294664376339453 \n",
      "\n",
      "REWARD 1.2726318557290215 \n",
      "\n",
      "REWARD 1.6250306654915263 \n",
      "\n",
      "REWARD 0.8697986379722237 \n",
      "\n",
      "REWARD 0.9966006769917968 \n",
      "\n",
      "REWARD 1.074774255396596 \n",
      "\n",
      "REWARD 1.2338564629780628 \n",
      "\n",
      "REWARD 1.330032925115935 \n",
      "\n",
      "REWARD 1.204497095475904 \n",
      "\n",
      "REWARD 1.234897672150898 \n",
      "\n",
      "REWARD 1.1468973594014285 \n",
      "\n",
      "REWARD 1.16288935806296 \n",
      "\n",
      "REWARD 1.1807511191402975 \n",
      "\n",
      "REWARD 1.081865533967441 \n",
      "\n",
      "REWARD 1.1275907089362462 \n",
      "\n",
      "REWARD 1.1447251257394464 \n",
      "\n",
      "REWARD 1.104105178876676 \n",
      "\n",
      "REWARD 1.096774049458849 \n",
      "\n",
      "REWARD 1.0828235400212316 \n",
      "\n",
      "REWARD 1.0726274018660424 \n",
      "\n",
      "REWARD 0.0 \n",
      "\n",
      "REWARD 2.391453195344101 \n",
      "\n",
      "REWARD 3.5979204668280413 \n",
      "\n",
      "REWARD 4.777372676251949 \n",
      "\n",
      "REWARD 3.3308971939648386 \n",
      "\n",
      "REWARD 3.2205354722946655 \n",
      "\n",
      "REWARD 0.9860237687932016 \n",
      "\n",
      "REWARD 0.9819741385736719 \n",
      "\n",
      "REWARD 0.9969921585851949 \n",
      "\n",
      "REWARD 1.0388756132553245 \n",
      "\n",
      "REWARD 1.113906219587679 \n",
      "\n",
      "REWARD 1.1377840698370902 \n",
      "\n",
      "REWARD 1.1980998744791893 \n",
      "\n",
      "REWARD 1.2238533638231848 \n",
      "\n",
      "REWARD 1.2803464320815903 \n",
      "\n",
      "REWARD 1.311263189611777 \n",
      "\n",
      "REWARD 1.3198209728624104 \n",
      "\n",
      "REWARD 1.3493792082729206 \n",
      "\n",
      "REWARD 1.3503005654695095 \n",
      "\n",
      "REWARD 1.3196525004899036 \n",
      "\n",
      "REWARD 1.2404106541845965 \n",
      "\n",
      "REWARD 1.246728406482932 \n",
      "\n",
      "REWARD 1.2169494880639824 \n",
      "\n",
      "REWARD 1.1959694020598683 \n",
      "\n",
      "REWARD 0.0 \n",
      "\n",
      "REWARD 11.94586385177931 \n",
      "\n",
      "REWARD 3.342843884103241 \n",
      "\n",
      "REWARD 3.6911950916805547 \n",
      "\n",
      "REWARD 2.425930765005727 \n",
      "\n",
      "REWARD 1.9551467773004332 \n",
      "\n",
      "REWARD 1.5315896253876098 \n",
      "\n",
      "REWARD 1.4012648435487476 \n",
      "\n",
      "REWARD 0.9716676446635543 \n",
      "\n",
      "REWARD 1.0524359244641255 \n",
      "\n",
      "REWARD 1.1728665879735087 \n",
      "\n",
      "REWARD 0.7581699808863229 \n",
      "\n",
      "REWARD 0.86303009920774 \n",
      "\n",
      "REWARD 1.0164083851209529 \n",
      "\n",
      "REWARD 1.1297488107461073 \n",
      "\n",
      "REWARD 1.1937833237012447 \n",
      "\n",
      "REWARD 1.2479642952487224 \n",
      "\n",
      "REWARD 1.266223543170065 \n",
      "\n",
      "REWARD 1.3174613276988467 \n",
      "\n",
      "REWARD 1.2725926234063516 \n",
      "\n",
      "REWARD 1.270113677714694 \n",
      "\n",
      "REWARD 1.222389535269811 \n",
      "\n",
      "REWARD 1.1659390427559013 \n",
      "\n",
      "REWARD 1.1178566613448535 \n",
      "\n",
      "REWARD 0.0 \n",
      "\n",
      "REWARD 0.17861927831080698 \n",
      "\n",
      "REWARD 1.5480396823507878 \n",
      "\n",
      "REWARD 0.9279804306504033 \n",
      "\n",
      "REWARD 0.7965038446159275 \n",
      "\n",
      "REWARD 0.8774949162090768 \n",
      "\n",
      "REWARD 1.043616749137045 \n",
      "\n",
      "REWARD 0.9739771472862572 \n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_metadata\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msimulation_time_steps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/stable_baselines3/sac/sac.py:307\u001b[0m, in \u001b[0;36mSAC.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfSAC,\n\u001b[1;32m    300\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    305\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    306\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfSAC:\n\u001b[0;32m--> 307\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/stable_baselines3/common/off_policy_algorithm.py:347\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    345\u001b[0m         \u001b[38;5;66;03m# Special case when the user passes `gradient_steps=0`\u001b[39;00m\n\u001b[1;32m    346\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m gradient_steps \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 347\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgradient_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    349\u001b[0m callback\u001b[38;5;241m.\u001b[39mon_training_end()\n\u001b[1;32m    351\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/stable_baselines3/sac/sac.py:267\u001b[0m, in \u001b[0;36mSAC.train\u001b[0;34m(self, gradient_steps, batch_size)\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# Optimize the critic\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcritic\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 267\u001b[0m \u001b[43mcritic_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcritic\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    270\u001b[0m \u001b[38;5;66;03m# Compute actor loss\u001b[39;00m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;66;03m# Alternative: actor_loss = th.mean(log_prob - qf1_pi)\u001b[39;00m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;66;03m# Min over all critic networks\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.learn(total_timesteps = env.get_metadata()[\"simulation_time_steps\"] * 20, \n",
    "            log_interval = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate (20 epoch training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# observations = env.reset()\n",
    "\n",
    "# while not env.done:\n",
    "#     actions, _ = model.predict(observations, deterministic=True)\n",
    "#     observations, _, _, _ = env.step(actions)\n",
    "\n",
    "# kpis = env.evaluate()\n",
    "# kpis = kpis.pivot(index='cost_function', columns='name', values='value')\n",
    "# kpis = kpis.dropna(how='all')\n",
    "# display(kpis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train for 1500 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.learn(total_timesteps = env.get_metadata()[\"simulation_time_steps\"] * 1500, \n",
    "#             log_interval = 1)\n",
    "# model.save(\"custom_reward_higher_comfort_SAC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate 500 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# observations = env.reset()\n",
    "\n",
    "# while not env.done:\n",
    "#     actions, _ = model.predict(observations, deterministic=True)\n",
    "#     observations, _, _, _ = env.step(actions)\n",
    "\n",
    "# kpis = env.evaluate()\n",
    "# kpis = kpis.pivot(index='cost_function', columns='name', values='value')\n",
    "# kpis = kpis.dropna(how='all')\n",
    "# display(kpis)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
