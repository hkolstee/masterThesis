{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# from custom_agent.CTCE.sac_agent import Agent\n",
    "from custom_agents.CTCE_algorithms.single_agent_SAC_discrete import SAC\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "# sac_agent = SAC(env, batch_size = 256, buffer_max_size=100000, use_AE = True, AE_reduc_scale = 2)\n",
    "sac_agent = SAC(env, batch_size = 256, buffer_max_size=100000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Episode 50 total reward: 9.000] ~ \n",
      "[-0.1561237  -1.7618388   0.21516146  2.7960207 ]\n",
      "tensor([0.9290, 0.8968])\n",
      "tensor([-0.0053, -0.1628,  0.1798,  2.3508])\n",
      "[Episode 100 total reward: 9.000] ~ \n",
      "[-0.1460567  -1.7604917   0.22502302  2.7876537 ]\n",
      "tensor([0.8967, 0.9997])\n",
      "tensor([-0.0073, -0.4357,  0.1679,  2.4185])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msac_agent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnr_steps\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m50000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarmup_steps\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearn_delay\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearn_freq\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearn_weight\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/masterThesis/masterThesis/custom_agents/core/SAC_single_agent_base.py:326\u001b[0m, in \u001b[0;36mSoftActorCriticCore.train\u001b[0;34m(self, nr_steps, max_episode_len, warmup_steps, learn_delay, learn_freq, learn_weight, checkpoint, save_dir)\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m step \u001b[38;5;241m>\u001b[39m learn_delay \u001b[38;5;129;01mand\u001b[39;00m step \u001b[38;5;241m%\u001b[39m learn_freq \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    324\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(learn_weight):\n\u001b[1;32m    325\u001b[0m         \u001b[38;5;66;03m# learning \u001b[39;00m\n\u001b[0;32m--> 326\u001b[0m         status, loss_actor, loss_critic, alpha, loss_alpha \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m status:\n\u001b[1;32m    329\u001b[0m             \u001b[38;5;66;03m# keep track for logs\u001b[39;00m\n\u001b[1;32m    330\u001b[0m             ep_learn_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/masterThesis/masterThesis/custom_agents/core/SAC_single_agent_base.py:169\u001b[0m, in \u001b[0;36mSoftActorCriticCore.learn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplay_buffer\u001b[38;5;241m.\u001b[39mbuffer_index \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size:\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;66;03m# return status 0\u001b[39;00m\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 169\u001b[0m obs, next_obs, replay_act, rewards, dones \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# get new alpha\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malpha \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mexp(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_alpha\u001b[38;5;241m.\u001b[39mdetach())\n",
      "File \u001b[0;32m~/masterThesis/masterThesis/custom_agents/core/SAC_single_agent_base.py:155\u001b[0m, in \u001b[0;36mSoftActorCriticCore.sample_batch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;66;03m# latent representations if autoencoder is used\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_AE:\n\u001b[0;32m--> 155\u001b[0m     obs, _, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautoencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    156\u001b[0m     next_obs, _, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mautoencoder(next_obs)\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obs, next_obs, replay_act, rewards, dones\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/masterThesis/masterThesis/custom_agents/networks/autoencoder.py:61\u001b[0m, in \u001b[0;36mAutoEncoder.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;66;03m# backward step\u001b[39;00m\n\u001b[1;32m     60\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 61\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m encoded\u001b[38;5;241m.\u001b[39mdetach(), decoded\u001b[38;5;241m.\u001b[39mdetach(), loss\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/optimizer.py:373\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    369\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    370\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    371\u001b[0m             )\n\u001b[0;32m--> 373\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    376\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/adam.py:163\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    152\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    155\u001b[0m         group,\n\u001b[1;32m    156\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    160\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    161\u001b[0m         state_steps)\n\u001b[0;32m--> 163\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/adam.py:311\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    309\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 311\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    324\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[43m     \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    326\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[43m     \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/adam.py:432\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    430\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (max_exp_avg_sqs[i]\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[1;32m    431\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 432\u001b[0m         denom \u001b[38;5;241m=\u001b[39m \u001b[43m(\u001b[49m\u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbias_correction2_sqrt\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_\u001b[49m\u001b[43m(\u001b[49m\u001b[43meps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    434\u001b[0m     param\u001b[38;5;241m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39mstep_size)\n\u001b[1;32m    436\u001b[0m \u001b[38;5;66;03m# Lastly, switch back to complex view\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sac_agent.train(nr_steps = 50000, warmup_steps = 100, learn_delay = 100, learn_freq = 1, learn_weight = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video /home/hkolstee/masterThesis/masterThesis/videos/custom_SAC_cartpole-episode-0.mp4.\n",
      "Moviepy - Writing video /home/hkolstee/masterThesis/masterThesis/videos/custom_SAC_cartpole-episode-0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /home/hkolstee/masterThesis/masterThesis/videos/custom_SAC_cartpole-episode-0.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "from gymnasium.utils.save_video import save_video\n",
    "import gymnasium as gym\n",
    "\n",
    "env2 = gym.make(\"CartPole-v1\", render_mode = \"rgb_array_list\")\n",
    "\n",
    "# test loop\n",
    "obs, info = env2.reset()\n",
    "done = 0\n",
    "truncated = 0\n",
    "step = 0\n",
    "for _ in range(1000):\n",
    "    action = sac_agent.get_action(obs)\n",
    "\n",
    "    next_obs, reward, done, truncated, info = env2.step(action)\n",
    "\n",
    "    obs = next_obs\n",
    "\n",
    "save_video(env2.render(), \"videos\", name_prefix = \"custom_SAC_cartpole\", fps=env.metadata[\"render_fps\"], step_starting_index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hkolstee/.local/lib/python3.10/site-packages/torch/cuda/__init__.py:138: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11040). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Episode 50 total reward: -123.423] ~ \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 17\u001b[0m\n\u001b[1;32m     13\u001b[0m env \u001b[38;5;241m=\u001b[39m gym\u001b[38;5;241m.\u001b[39mmake(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPendulum-v1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     15\u001b[0m sac_agent \u001b[38;5;241m=\u001b[39m SAC(env, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m, buffer_max_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100000\u001b[39m)\n\u001b[0;32m---> 17\u001b[0m \u001b[43msac_agent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnr_steps\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarmup_steps\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearn_delay\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearn_freq\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearn_weight\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/masterThesis/masterThesis/custom_agents/core/SAC_single_agent_base.py:293\u001b[0m, in \u001b[0;36mSoftActorCriticCore.train\u001b[0;34m(self, nr_steps, max_episode_len, warmup_steps, learn_delay, learn_freq, learn_weight, checkpoint, save_dir)\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m step \u001b[38;5;241m>\u001b[39m learn_delay \u001b[38;5;129;01mand\u001b[39;00m step \u001b[38;5;241m%\u001b[39m learn_freq \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    291\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(learn_weight):\n\u001b[1;32m    292\u001b[0m         \u001b[38;5;66;03m# learning \u001b[39;00m\n\u001b[0;32m--> 293\u001b[0m         status, loss_actor, loss_critic, alpha, loss_alpha \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    295\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m status:\n\u001b[1;32m    296\u001b[0m             \u001b[38;5;66;03m# keep track for logs\u001b[39;00m\n\u001b[1;32m    297\u001b[0m             ep_learn_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/masterThesis/masterThesis/custom_agents/core/SAC_single_agent_base.py:156\u001b[0m, in \u001b[0;36mSoftActorCriticCore.learn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malpha \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mexp(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_alpha\u001b[38;5;241m.\u001b[39mdetach())\n\u001b[1;32m    155\u001b[0m \u001b[38;5;66;03m# critic loss\u001b[39;00m\n\u001b[0;32m--> 156\u001b[0m loss_critic \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcritic_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_obs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreplay_act\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrewards\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdones\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;66;03m# backward prop + gradient step\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcritic1\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/masterThesis/masterThesis/custom_agents/CTCE_algorithms/single_agent_SAC_continuous.py:129\u001b[0m, in \u001b[0;36mSAC.critic_loss\u001b[0;34m(self, obs, next_obs, replay_act, rewards, dones)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;66;03m# target q values\u001b[39;00m\n\u001b[1;32m    128\u001b[0m q1_policy_targ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcritic1_targ\u001b[38;5;241m.\u001b[39mforward(next_obs, actions)\n\u001b[0;32m--> 129\u001b[0m q2_policy_targ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcritic2_targ\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnext_obs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;66;03m# Clipped double Q \u001b[39;00m\n\u001b[1;32m    132\u001b[0m min_q_targ \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mminimum(q1_policy_targ, q2_policy_targ)\n",
      "File \u001b[0;32m~/masterThesis/masterThesis/custom_agents/networks/critic.py:50\u001b[0m, in \u001b[0;36mCritic.forward\u001b[0;34m(self, obs, action)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m: \n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([obs, action], dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[0;32m---> 50\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/masterThesis/masterThesis/custom_agents/networks/MLP.py:83\u001b[0m, in \u001b[0;36mMultiLayerPerceptron.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (idx \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnr_layers):\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;66;03m# pass through first and middle layers\u001b[39;00m\n\u001b[1;32m     82\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers[idx](data)    \n\u001b[0;32m---> 83\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;66;03m# last layer(s)\u001b[39;00m\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;66;03m# if multiple last layers\u001b[39;00m\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers[idx], nn\u001b[38;5;241m.\u001b[39mModuleList):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/functional.py:1471\u001b[0m, in \u001b[0;36mrelu\u001b[0;34m(input, inplace)\u001b[0m\n\u001b[1;32m   1469\u001b[0m     result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu_(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m   1470\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1471\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1472\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from custom_agents.CTCE_algorithms.single_agent_SAC_continuous import SAC\n",
    "\n",
    "env = gym.make(\"Pendulum-v1\")\n",
    "\n",
    "sac_agent = SAC(env, batch_size=256, buffer_max_size=100000)\n",
    "\n",
    "sac_agent.train(nr_steps = 20000, warmup_steps = 1000, learn_delay = 100, learn_freq = 1, learn_weight = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hkolstee/.local/lib/python3.10/site-packages/torch/cuda/__init__.py:138: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11040). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 24\u001b[0m\n\u001b[1;32m     19\u001b[0m env \u001b[38;5;241m=\u001b[39m CityLearnWrapper(env)\n\u001b[1;32m     22\u001b[0m sac_agent \u001b[38;5;241m=\u001b[39m SAC(env, batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m256\u001b[39m, buffer_max_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100000\u001b[39m, use_AE \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, AE_reduc_scale \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m, )\n\u001b[0;32m---> 24\u001b[0m \u001b[43msac_agent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnr_steps\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarmup_steps\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearn_delay\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearn_freq\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearn_weight\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/masterThesis/masterThesis/custom_agents/core/SAC_single_agent_base.py:260\u001b[0m, in \u001b[0;36mSoftActorCriticCore.train\u001b[0;34m(self, nr_steps, max_episode_len, warmup_steps, learn_delay, learn_freq, learn_weight, checkpoint, save_dir)\u001b[0m\n\u001b[1;32m    257\u001b[0m         action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_action(obs)\n\u001b[1;32m    259\u001b[0m \u001b[38;5;66;03m# transition\u001b[39;00m\n\u001b[0;32m--> 260\u001b[0m next_obs, reward, done, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;66;03m# step increment \u001b[39;00m\n\u001b[1;32m    263\u001b[0m ep_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/masterThesis/masterThesis/custom_agents/utils/citylearn_wrapper.py:31\u001b[0m, in \u001b[0;36mCityLearnWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[1;32m     27\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;124;03m    Citylearn expects a list of actions, even when using a central agent.\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;124;03m    Also, does not provide truncation information\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m     next_obs, reward, done, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43maction\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m     truncated \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m next_obs[\u001b[38;5;241m0\u001b[39m], reward[\u001b[38;5;241m0\u001b[39m], done, truncated, info\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/citylearn/citylearn.py:816\u001b[0m, in \u001b[0;36mCityLearnEnv.step\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_variables()\n\u001b[1;32m    810\u001b[0m \u001b[38;5;66;03m# NOTE:\u001b[39;00m\n\u001b[1;32m    811\u001b[0m \u001b[38;5;66;03m# This call to retrieve each building's observation dictionary is an expensive call especially since the observations \u001b[39;00m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;66;03m# are retrieved again to send to agent but the observations in dict form is needed for the reward function to easily\u001b[39;00m\n\u001b[1;32m    813\u001b[0m \u001b[38;5;66;03m# extract building-level values. Can't think of a better way to handle this without giving the reward direct access to\u001b[39;00m\n\u001b[1;32m    814\u001b[0m \u001b[38;5;66;03m# env, which is not the best design for competition integrity sake. Will revisit the building.observations() function\u001b[39;00m\n\u001b[1;32m    815\u001b[0m \u001b[38;5;66;03m# to see how it can be optimized.\u001b[39;00m\n\u001b[0;32m--> 816\u001b[0m reward_observations \u001b[38;5;241m=\u001b[39m [b\u001b[38;5;241m.\u001b[39mobservations(include_all\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, normalize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, periodic_normalization\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuildings]\n\u001b[1;32m    817\u001b[0m reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreward_function\u001b[38;5;241m.\u001b[39mcalculate(observations\u001b[38;5;241m=\u001b[39mreward_observations)\n\u001b[1;32m    818\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__rewards\u001b[38;5;241m.\u001b[39mappend(reward)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/citylearn/citylearn.py:816\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_variables()\n\u001b[1;32m    810\u001b[0m \u001b[38;5;66;03m# NOTE:\u001b[39;00m\n\u001b[1;32m    811\u001b[0m \u001b[38;5;66;03m# This call to retrieve each building's observation dictionary is an expensive call especially since the observations \u001b[39;00m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;66;03m# are retrieved again to send to agent but the observations in dict form is needed for the reward function to easily\u001b[39;00m\n\u001b[1;32m    813\u001b[0m \u001b[38;5;66;03m# extract building-level values. Can't think of a better way to handle this without giving the reward direct access to\u001b[39;00m\n\u001b[1;32m    814\u001b[0m \u001b[38;5;66;03m# env, which is not the best design for competition integrity sake. Will revisit the building.observations() function\u001b[39;00m\n\u001b[1;32m    815\u001b[0m \u001b[38;5;66;03m# to see how it can be optimized.\u001b[39;00m\n\u001b[0;32m--> 816\u001b[0m reward_observations \u001b[38;5;241m=\u001b[39m [\u001b[43mb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobservations\u001b[49m\u001b[43m(\u001b[49m\u001b[43minclude_all\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mperiodic_normalization\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuildings]\n\u001b[1;32m    817\u001b[0m reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreward_function\u001b[38;5;241m.\u001b[39mcalculate(observations\u001b[38;5;241m=\u001b[39mreward_observations)\n\u001b[1;32m    818\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__rewards\u001b[38;5;241m.\u001b[39mappend(reward)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/citylearn/building.py:764\u001b[0m, in \u001b[0;36mBuilding.observations\u001b[0;34m(self, include_all, normalize, periodic_normalization)\u001b[0m\n\u001b[1;32m    728\u001b[0m include_all \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m include_all \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m include_all\n\u001b[1;32m    730\u001b[0m observations \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    731\u001b[0m data \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    732\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{\n\u001b[1;32m    733\u001b[0m         k\u001b[38;5;241m.\u001b[39mlstrip(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m): \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menergy_simulation\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattr__\u001b[39m(k\u001b[38;5;241m.\u001b[39mlstrip(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m))[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime_step] \n\u001b[1;32m    734\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mvars\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menergy_simulation)\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v, np\u001b[38;5;241m.\u001b[39mndarray)\n\u001b[1;32m    735\u001b[0m     },\n\u001b[1;32m    736\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{\n\u001b[1;32m    737\u001b[0m         k\u001b[38;5;241m.\u001b[39mlstrip(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m): \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweather\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattr__\u001b[39m(k\u001b[38;5;241m.\u001b[39mlstrip(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m))[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime_step] \n\u001b[1;32m    738\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mvars\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweather)\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v, np\u001b[38;5;241m.\u001b[39mndarray)\n\u001b[1;32m    739\u001b[0m     },\n\u001b[1;32m    740\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{\n\u001b[1;32m    741\u001b[0m         k\u001b[38;5;241m.\u001b[39mlstrip(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m): \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpricing\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattr__\u001b[39m(k\u001b[38;5;241m.\u001b[39mlstrip(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m))[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime_step] \n\u001b[1;32m    742\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mvars\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpricing)\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v, np\u001b[38;5;241m.\u001b[39mndarray)\n\u001b[1;32m    743\u001b[0m     },\n\u001b[1;32m    744\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{\n\u001b[1;32m    745\u001b[0m         k\u001b[38;5;241m.\u001b[39mlstrip(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m): \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcarbon_intensity\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattr__\u001b[39m(k\u001b[38;5;241m.\u001b[39mlstrip(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m))[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime_step] \n\u001b[1;32m    746\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mvars\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcarbon_intensity)\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v, np\u001b[38;5;241m.\u001b[39mndarray)\n\u001b[1;32m    747\u001b[0m     },\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msolar_generation\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;28mabs\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msolar_generation[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime_step]),\n\u001b[1;32m    749\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{\n\u001b[1;32m    750\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcooling_storage_soc\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcooling_storage\u001b[38;5;241m.\u001b[39msoc[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime_step],\n\u001b[1;32m    751\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mheating_storage_soc\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheating_storage\u001b[38;5;241m.\u001b[39msoc[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime_step],\n\u001b[1;32m    752\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdhw_storage_soc\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdhw_storage\u001b[38;5;241m.\u001b[39msoc[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime_step],\n\u001b[1;32m    753\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124melectrical_storage_soc\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39melectrical_storage\u001b[38;5;241m.\u001b[39msoc[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime_step],\n\u001b[1;32m    754\u001b[0m     },\n\u001b[1;32m    755\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcooling_demand\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__energy_from_cooling_device[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime_step] \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mabs\u001b[39m(\u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcooling_storage\u001b[38;5;241m.\u001b[39menergy_balance[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime_step], \u001b[38;5;241m0.0\u001b[39m)),\n\u001b[1;32m    756\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mheating_demand\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__energy_from_heating_device[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime_step] \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mabs\u001b[39m(\u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheating_storage\u001b[38;5;241m.\u001b[39menergy_balance[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime_step], \u001b[38;5;241m0.0\u001b[39m)),\n\u001b[1;32m    757\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdhw_demand\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__energy_from_dhw_device[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime_step] \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mabs\u001b[39m(\u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdhw_storage\u001b[38;5;241m.\u001b[39menergy_balance[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime_step], \u001b[38;5;241m0.0\u001b[39m)),\n\u001b[1;32m    758\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnet_electricity_consumption\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnet_electricity_consumption[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime_step],\n\u001b[1;32m    759\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcooling_electricity_consumption\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcooling_electricity_consumption[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime_step],\n\u001b[1;32m    760\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mheating_electricity_consumption\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheating_electricity_consumption[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime_step],\n\u001b[1;32m    761\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdhw_electricity_consumption\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdhw_electricity_consumption[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime_step],\n\u001b[1;32m    762\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcooling_storage_electricity_consumption\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcooling_storage_electricity_consumption[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime_step],\n\u001b[1;32m    763\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mheating_storage_electricity_consumption\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheating_storage_electricity_consumption[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime_step],\n\u001b[0;32m--> 764\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdhw_storage_electricity_consumption\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdhw_storage_electricity_consumption\u001b[49m[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime_step],\n\u001b[1;32m    765\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124melectrical_storage_electricity_consumption\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39melectrical_storage_electricity_consumption[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime_step],\n\u001b[1;32m    766\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcooling_device_cop\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcooling_device\u001b[38;5;241m.\u001b[39mget_cop(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweather\u001b[38;5;241m.\u001b[39moutdoor_dry_bulb_temperature[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime_step], heating\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m    767\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mheating_device_cop\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheating_device\u001b[38;5;241m.\u001b[39mget_cop(\n\u001b[1;32m    768\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweather\u001b[38;5;241m.\u001b[39moutdoor_dry_bulb_temperature[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime_step], heating\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    769\u001b[0m             ) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheating_device, HeatPump) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheating_device\u001b[38;5;241m.\u001b[39mefficiency,\n\u001b[1;32m    770\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mindoor_dry_bulb_temperature_set_point\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menergy_simulation\u001b[38;5;241m.\u001b[39mindoor_dry_bulb_temperature_set_point[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime_step],\n\u001b[1;32m    771\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mindoor_dry_bulb_temperature_delta\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mabs\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menergy_simulation\u001b[38;5;241m.\u001b[39mindoor_dry_bulb_temperature[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime_step] \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menergy_simulation\u001b[38;5;241m.\u001b[39mindoor_dry_bulb_temperature_set_point[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime_step]),\n\u001b[1;32m    772\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moccupant_count\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menergy_simulation\u001b[38;5;241m.\u001b[39moccupant_count[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime_step],\n\u001b[1;32m    773\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpower_outage\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__power_outage_signal[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime_step],\n\u001b[1;32m    774\u001b[0m }\n\u001b[1;32m    776\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m include_all:\n\u001b[1;32m    777\u001b[0m     valid_observations \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(data\u001b[38;5;241m.\u001b[39mkeys())\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/citylearn/building.py:378\u001b[0m, in \u001b[0;36mBuilding.dhw_storage_electricity_consumption\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    376\u001b[0m     consumption \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdhw_device\u001b[38;5;241m.\u001b[39mget_input_power(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdhw_storage\u001b[38;5;241m.\u001b[39menergy_balance[:\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime_step \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweather\u001b[38;5;241m.\u001b[39moutdoor_dry_bulb_temperature[:\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime_step \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m], \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 378\u001b[0m     consumption \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdhw_device\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_input_power\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdhw_storage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menergy_balance\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtime_step\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m consumption\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from custom_agents.CTCE_algorithms.single_agent_SAC_continuous import SAC\n",
    "from custom_agents.utils.citylearn_wrapper import CityLearnWrapper\n",
    "\n",
    "from citylearn.wrappers import NormalizedSpaceWrapper, StableBaselines3Wrapper\n",
    "from custom_reward.custom_reward import CustomReward\n",
    "\n",
    "from citylearn.citylearn import CityLearnEnv\n",
    "\n",
    "env = CityLearnEnv(schema = \"data/schema.json\", reward_function = CustomReward, central_agent=True)\n",
    "env = CityLearnWrapper(env)\n",
    "\n",
    "\n",
    "sac_agent = SAC(env, batch_size = 256, buffer_max_size = 100000, use_AE = True, AE_reduc_scale = 2, )\n",
    "\n",
    "sac_agent.train(nr_steps = 100000, warmup_steps = 0, learn_delay = 100, learn_freq = 1, learn_weight = 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
