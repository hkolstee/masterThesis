{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SB3 A2C implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import A2C\n",
    "from custom_spider_env.spider_fly_env.envs.grid import SpiderFlyEnv\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import spider_fly_env\n",
    "import gymnasium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = gymnasium.make(\"SpiderFlyGrid-v0\", size = 4, spiders = 4, max_timesteps = 20)\n",
    "env = SpiderFlyEnv(size = 5, spiders = 4, max_timesteps = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hkolstee/.local/lib/python3.10/site-packages/torch/cuda/__init__.py:138: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11040). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "model = A2C(policy = \"MlpPolicy\", env = env, tensorboard_log=\"tensorboard_logs_spiderA2C\")\n",
    "model.learn(150000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# env = gymnasium.make(\"SpiderFlyGrid-v0\", size = 4, spiders = 4, render_mode = \"ascii\")\n",
    "env = SpiderFlyEnv(size = 5, spiders = 4, max_timesteps = 100, render_mode = \"ascii\")\n",
    "\n",
    "done = False\n",
    "obs, _ = env.reset()\n",
    "while not done:\n",
    "    act, _ = model.predict(obs, deterministic = True)\n",
    "    obs, rew, done, _, _ = env.step(act)\n",
    "    time.sleep(.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom discrete SAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Episode 50 total reward: 31.000] ~ \n",
      "[Episode 100 total reward: 260.000] ~ \n"
     ]
    }
   ],
   "source": [
    "from custom_agent.CTCE.sac_agent_discrete import Agent\n",
    "import gymnasium\n",
    "\n",
    "env = gymnasium.make('CartPole-v1')\n",
    "\n",
    "model = Agent(env, batch_size=100, buffer_max_size=100000, layer_sizes = (64, 64))\n",
    "\n",
    "model.train(nr_steps = 20000, warmup_steps = 100, learn_delay = 100, learn_freq = 1, learn_weight = 1, save_dir = \"temp_models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom discrete sequential MA SAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from custom_agent.CTDE.ma_sac_agents_seq_discrete import Agents\n",
    "from spider_fly_env.envs.grid_MA_pettingzoo import SpiderFlyEnvMA\n",
    "from spider_fly_env.envs.pettingzoo_wrapper import PettingZooWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom multi-agent pettinzoo environment\n",
    "env = SpiderFlyEnvMA(size = 3, spiders = 4, max_timesteps = 50)\n",
    "# env = SpiderFlyEnvMA(size = 5, spiders = 4, render_mode = \"ascii\")\n",
    "env = PettingZooWrapper(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Episode 50 total reward: [1. 1. 1. 1.]] ~ \n",
      "[Episode 100 total reward: [-0.065 -0.065 -0.065 -0.065]] ~ \n",
      "[Episode 150 total reward: [-0.113 -0.113 -0.113 -0.113]] ~ \n",
      "[Episode 200 total reward: [1. 1. 1. 1.]] ~ \n",
      "[Episode 250 total reward: [-0.05 -0.05 -0.05 -0.05]] ~ \n",
      "[Episode 300 total reward: [-0.056 -0.056 -0.056 -0.056]] ~ \n",
      "[Episode 350 total reward: [-0.07 -0.07 -0.07 -0.07]] ~ \n",
      "[Episode 400 total reward: [-0.063 -0.063 -0.063 -0.063]] ~ \n",
      "[Episode 450 total reward: [-0.086 -0.086 -0.086 -0.086]] ~ \n",
      "[Episode 500 total reward: [-0.108 -0.108 -0.108 -0.108]] ~ \n",
      "[Episode 550 total reward: [-0.085 -0.085 -0.085 -0.085]] ~ \n",
      "[Episode 600 total reward: [-0.07 -0.07 -0.07 -0.07]] ~ \n",
      "[Episode 650 total reward: [-0.078 -0.078 -0.078 -0.078]] ~ \n",
      "[Episode 700 total reward: [-0.105 -0.105 -0.105 -0.105]] ~ \n",
      "[Episode 750 total reward: [1. 1. 1. 1.]] ~ \n",
      "[Episode 800 total reward: [-0.075 -0.075 -0.075 -0.075]] ~ \n",
      "[Episode 850 total reward: [-0.066 -0.066 -0.066 -0.066]] ~ \n",
      "[Episode 900 total reward: [1. 1. 1. 1.]] ~ \n",
      "[Episode 950 total reward: [-0.07 -0.07 -0.07 -0.07]] ~ \n",
      "[Episode 1000 total reward: [-0.058 -0.058 -0.058 -0.058]] ~ \n",
      "[Episode 1050 total reward: [-0.05 -0.05 -0.05 -0.05]] ~ \n",
      "[Episode 1100 total reward: [-0.117 -0.117 -0.117 -0.117]] ~ \n",
      "[Episode 1150 total reward: [1. 1. 1. 1.]] ~ \n",
      "[Episode 1200 total reward: [1. 1. 1. 1.]] ~ \n",
      "[Episode 1250 total reward: [-0.074 -0.074 -0.074 -0.074]] ~ \n",
      "[Episode 1300 total reward: [-0.067 -0.067 -0.067 -0.067]] ~ \n",
      "[Episode 1350 total reward: [-0.065 -0.065 -0.065 -0.065]] ~ \n",
      "[Episode 1400 total reward: [-0.05 -0.05 -0.05 -0.05]] ~ \n",
      "[Episode 1450 total reward: [-0.081 -0.081 -0.081 -0.081]] ~ \n",
      "[Episode 1500 total reward: [1. 1. 1. 1.]] ~ \n",
      "[Episode 1550 total reward: [1. 1. 1. 1.]] ~ \n",
      "[Episode 1600 total reward: [-0.075 -0.075 -0.075 -0.075]] ~ \n",
      "[Episode 1650 total reward: [-0.05 -0.05 -0.05 -0.05]] ~ \n",
      "[Episode 1700 total reward: [-0.08 -0.08 -0.08 -0.08]] ~ \n",
      "[Episode 1750 total reward: [-0.077 -0.077 -0.077 -0.077]] ~ \n",
      "[Episode 1800 total reward: [-0.106 -0.106 -0.106 -0.106]] ~ \n",
      "[Episode 1850 total reward: [-0.096 -0.096 -0.096 -0.096]] ~ \n",
      "[Episode 1900 total reward: [-0.1 -0.1 -0.1 -0.1]] ~ \n",
      "[Episode 1950 total reward: [-0.085 -0.085 -0.085 -0.085]] ~ \n",
      "[Episode 2000 total reward: [-0.057 -0.057 -0.057 -0.057]] ~ \n",
      "[Episode 2050 total reward: [-0.089 -0.089 -0.089 -0.089]] ~ \n",
      "[Episode 2100 total reward: [-0.086 -0.086 -0.086 -0.086]] ~ \n",
      "[Episode 2150 total reward: [-0.111 -0.111 -0.111 -0.111]] ~ \n",
      "[Episode 2200 total reward: [1. 1. 1. 1.]] ~ \n",
      "[Episode 2250 total reward: [-0.073 -0.073 -0.073 -0.073]] ~ \n",
      "[Episode 2300 total reward: [-0.09 -0.09 -0.09 -0.09]] ~ \n"
     ]
    }
   ],
   "source": [
    "agents = Agents(env, batch_size=100, buffer_max_size=100000, layer_sizes = (64, 64))\n",
    "\n",
    "agents.train(nr_steps = 100000, warmup_steps = 110, learn_delay = 100, learn_freq = 1, learn_weight = 1, save_dir = \"temp_models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observations, infos = env.reset(seed=42)\n",
    "\n",
    "while env.agents:\n",
    "    # this is where you would insert your policy\n",
    "    actions = {agent: env.action_space(agent).sample() for agent in env.agents}\n",
    "\n",
    "    observations, rewards, terminations, truncations, infos = env.step(actions)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
