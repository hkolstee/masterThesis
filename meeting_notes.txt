MEETING NOTES 20/02

Made some changes to my SAC while comparing to SB3, did not see much change.
I think one change they use which might help still is batch normalization for critics.

Added autoencoder, no dropout, the one training run does not proof to work well yet.

Found in literature about centralized critics:
    - MADDPG (https://arxiv.org/abs/1706.02275v4) (2017) uses a centralized critic for each agent in a multi-agent DDPG algorithm
    - Found some applications of centralized critics.
        - https://www.researchgate.net/publication/344036373_Multi-Agent_Reinforcement_Learning_for_the_Energy_Optimization_of_Cyber-Physical_Production_Systems


-> look at MADDPG (baseline) (decide value is factorized (one per agent))
-> meeting on multi-agent reinforcement learning algorithms
-> COMA, MADDPG, VDN, QMIX, 
-> launch benchmarks
-> literature research, classification in attributes of algos, how are they different, pros cons, etc

