{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spider Fly testing environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hkolstee/.local/lib/python3.10/site-packages/gymnasium/envs/registration.py:694: UserWarning: \u001b[33mWARN: Overriding environment SpiderFlyGrid-v0 already in registry.\u001b[0m\n",
      "  logger.warn(f\"Overriding environment {new_spec.id} already in registry.\")\n",
      "/home/hkolstee/.local/lib/python3.10/site-packages/gymnasium/envs/registration.py:694: UserWarning: \u001b[33mWARN: Overriding environment SpiderFlyGridMA-v0 already in registry.\u001b[0m\n",
      "  logger.warn(f\"Overriding environment {new_spec.id} already in registry.\")\n"
     ]
    }
   ],
   "source": [
    "from custom_agent.CTCE.ma_sac_agents_seq_discrete import Agents\n",
    "from custom_spider_env.spider_fly_env.envs.grid_MA_testing import SpiderFlyEnvMA\n",
    "from custom_spider_env.spider_fly_env.envs.pettingzoo_wrapper import PettingZooWrapper\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ' 'X' ' ' 'O' ' ' 'X' 'O']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([6, 1, 5, 5])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### Normal Q-learning (Decentralized with Q-table per agent)\n",
    "env = SpiderFlyEnvMA(render_mode = \"ascii\")\n",
    "env = PettingZooWrapper(env)\n",
    "\n",
    "env.observation_space[0].sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = SpiderFlyEnvMA(max_steps = 200)\n",
    "env = PettingZooWrapper(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hkolstee/.local/lib/python3.10/site-packages/torch/cuda/__init__.py:138: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11040). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "agents = Agents(env, batch_size = 512) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Agents.create_padded_action_input() missing 1 required positional argument: 'setting'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m rewards \u001b[38;5;241m=\u001b[39m \u001b[43magents\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnr_steps\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100000\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/masterThesis/masterThesis/custom_agent/CTCE/ma_sac_agents_seq_discrete.py:428\u001b[0m, in \u001b[0;36mAgents.train\u001b[0;34m(self, nr_steps, max_episode_len, warmup_steps, learn_delay, learn_freq, learn_weight, checkpoint, save_dir)\u001b[0m\n\u001b[1;32m    425\u001b[0m     action \u001b[38;5;241m=\u001b[39m [act_space\u001b[38;5;241m.\u001b[39msample() \u001b[38;5;28;01mfor\u001b[39;00m act_space \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39maction_space]\n\u001b[1;32m    426\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m: \n\u001b[1;32m    427\u001b[0m     \u001b[38;5;66;03m# get action\u001b[39;00m\n\u001b[0;32m--> 428\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    430\u001b[0m \u001b[38;5;66;03m# transition\u001b[39;00m\n\u001b[1;32m    431\u001b[0m next_obs, reward, done, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(action)\n",
      "File \u001b[0;32m~/masterThesis/masterThesis/custom_agent/CTCE/ma_sac_agents_seq_discrete.py:380\u001b[0m, in \u001b[0;36mAgents.get_action\u001b[0;34m(self, observations, reparameterize, deterministic)\u001b[0m\n\u001b[1;32m    376\u001b[0m seq_acts \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mempty((\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m))\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m agent_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnr_agents):\n\u001b[1;32m    378\u001b[0m     \n\u001b[1;32m    379\u001b[0m     \u001b[38;5;66;03m# seq_input is observations plus preceding actor actions\u001b[39;00m\n\u001b[0;32m--> 380\u001b[0m     seq_input \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([global_obs, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_padded_action_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseq_acts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magent_idx\u001b[49m\u001b[43m)\u001b[49m], dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    381\u001b[0m     \u001b[38;5;66;03m# sample action from policy\u001b[39;00m\n\u001b[1;32m    382\u001b[0m     actions, _, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor\u001b[38;5;241m.\u001b[39maction_distr_sample(seq_input, reparameterize, deterministic)\n",
      "\u001b[0;31mTypeError\u001b[0m: Agents.create_padded_action_input() missing 1 required positional argument: 'setting'"
     ]
    }
   ],
   "source": [
    "rewards = agents.train(nr_steps = 100000, warmup_steps = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = np.vstack(rewards)\n",
    "\n",
    "df = pd.DataFrame(data, columns = [\"agent_\" + str(idx) for idx in range(seqTabqlearning.nr_agents)])\n",
    "df[\"Episode\"] = list(range(data.shape[0]))\n",
    "\n",
    "df = df.melt('Episode', var_name='Agent', value_name='Rewards')\n",
    "\n",
    "df[\"Avg_Reward\"] = df[\"Rewards\"].rolling(window = 50, step = 50).mean()\n",
    "df = df[df.Episode > 50]\n",
    "df.dropna()\n",
    "\n",
    "\n",
    "sns.lineplot(data = df, x = \"Episode\", y = \"Avg_Reward\")\n",
    "plt.figure()\n",
    "sns.lineplot(data = df, x = \"Episode\", y = \"Avg_Reward\", hue = \"Agent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST of custom Multi-Agent SAC agent on citylearn env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from citylearn.citylearn import CityLearnEnv\n",
    "from citylearn.wrappers import NormalizedSpaceWrapper, StableBaselines3Wrapper\n",
    "\n",
    "from custom_agent.CTDE.ma_sac_agents_seq2 import Agents\n",
    "\n",
    "from custom_reward.custom_reward import CustomReward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_path = \"data/schema.json\"\n",
    "\n",
    "env = CityLearnEnv(schema = schema_path, reward_function = CustomReward, central_agent=False)\n",
    "\n",
    "# wrap environment for a more workable env\n",
    "env = NormalizedSpaceWrapper(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sac_agent = Agents(env, batch_size=100, buffer_max_size=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warmup_steps = 5000\n",
    "\n",
    "# make agent\n",
    "sac_agent = Agents(env, batch_size=256, buffer_max_size=100000)\n",
    "\n",
    "# training run\n",
    "sac_agent.train(nr_steps = 5500 * 720, warmup_steps = warmup_steps, learn_delay = 100, learn_freq = 1, learn_weight = 1, save_dir = \"temp_models\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
