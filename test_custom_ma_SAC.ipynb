{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST of custom Multi-Agent SAC agent on citylearn env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'custom_agent.custom_SAC.ma_sac_agents2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 12\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcitylearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcitylearn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CityLearnEnv\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcustom_agent\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcustom_SAC\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mma_sac_agents2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Agents\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcustom_reward\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcustom_reward\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CustomReward\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'custom_agent.custom_SAC.ma_sac_agents2'"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from citylearn.citylearn import CityLearnEnv\n",
    "\n",
    "from custom_agent.custom_SAC.ma_sac_agents import Agents\n",
    "\n",
    "from custom_reward.custom_reward import CustomReward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WrapperEnv:\n",
    "    \"\"\"\n",
    "    Env to wrap provide Citylearn Env data without providing full env\n",
    "    Preventing attribute access outside of the available functions\n",
    "    \"\"\"\n",
    "    def __init__(self, env_data):\n",
    "        self.observation_names = env_data['observation_names']\n",
    "        self.action_names = env_data['action_names']\n",
    "        self.observation_space = env_data['observation_space']\n",
    "        self.action_space = env_data['action_space']\n",
    "        self.time_steps = env_data['time_steps']\n",
    "        self.seconds_per_time_step = env_data['seconds_per_time_step']\n",
    "        self.random_seed = env_data['random_seed']\n",
    "        self.buildings_metadata = env_data['buildings_metadata']\n",
    "        self.episode_tracker = env_data['episode_tracker']\n",
    "    \n",
    "    def get_metadata(self):\n",
    "        return {'buildings': self.buildings_metadata}\n",
    "    \n",
    "def makeEnv(schema_path, reward_function):\n",
    "    # create environment\n",
    "    env = CityLearnEnv(schema = schema_path, reward_function = reward_function, central_agent=False)\n",
    "\n",
    "    env_data = dict(\n",
    "        observation_names = env.observation_names,\n",
    "        action_names = env.action_names,\n",
    "        observation_space = env.observation_space,\n",
    "        action_space = env.action_space,\n",
    "        time_steps = env.time_steps,\n",
    "        random_seed = None,\n",
    "        episode_tracker = None,\n",
    "        seconds_per_time_step = None,\n",
    "        buildings_metadata = env.get_metadata()['buildings']\n",
    "    )\n",
    "\n",
    "    wrapper_env = WrapperEnv(env_data)\n",
    "    return env, wrapper_env\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_path = \"data/schema.json\"\n",
    "\n",
    "env, wrapper_env = makeEnv(schema_path, CustomReward)\n",
    "\n",
    "# wrap environment for use in stablebaselines3\n",
    "# env = NormalizedObservationWrapper(env)\n",
    "# env = StableBaselines3Wrapper(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hkolstee/.local/lib/python3.10/site-packages/torch/cuda/__init__.py:138: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11040). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obs 90\n",
      "act 9\n"
     ]
    }
   ],
   "source": [
    "sac_agent = Agents(env, batch_size=100, buffer_max_size=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obs 90\n",
      "act 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Episode 1 mean reward: [-1.05858301 -0.94618482 -0.96865855]] ~ :  51%|█████     | 732/1440 [01:38<01:34,  7.46it/s]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "negative electricity consumption for cooling demand | timestep: 14, building: Building_1, device_output: -9.344993591308594, electricity_consumption: -4.1051105764168625",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m sac_agent \u001b[38;5;241m=\u001b[39m Agents(env, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, buffer_max_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100000\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# training run\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m logs \u001b[38;5;241m=\u001b[39m \u001b[43msac_agent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnr_steps\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m720\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarmup_steps\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m720\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearn_delay\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearn_freq\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearn_weight\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# display(logs)\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# save training logs for this run\u001b[39;00m\n\u001b[1;32m     12\u001b[0m logs_list\u001b[38;5;241m.\u001b[39mappend(logs)\n",
      "File \u001b[0;32m~/masterThesis/masterThesis/custom_agent/custom_SAC/ma_sac_agents2.py:336\u001b[0m, in \u001b[0;36mAgents.train\u001b[0;34m(self, nr_steps, max_episode_len, warmup_steps, learn_delay, learn_freq, learn_weight)\u001b[0m\n\u001b[1;32m    333\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_action(obs)\n\u001b[1;32m    335\u001b[0m \u001b[38;5;66;03m# transition\u001b[39;00m\n\u001b[0;32m--> 336\u001b[0m next_obs, reward, done, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    337\u001b[0m \u001b[38;5;66;03m# print(reward)\u001b[39;00m\n\u001b[1;32m    338\u001b[0m \n\u001b[1;32m    339\u001b[0m \u001b[38;5;66;03m# step increment \u001b[39;00m\n\u001b[1;32m    340\u001b[0m ep_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/citylearn/citylearn.py:806\u001b[0m, in \u001b[0;36mCityLearnEnv.step\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    803\u001b[0m actions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parse_actions(actions)\n\u001b[1;32m    805\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m building, building_actions \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuildings, actions):\n\u001b[0;32m--> 806\u001b[0m     \u001b[43mbuilding\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_actions\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbuilding_actions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_variables()\n\u001b[1;32m    810\u001b[0m \u001b[38;5;66;03m# NOTE:\u001b[39;00m\n\u001b[1;32m    811\u001b[0m \u001b[38;5;66;03m# This call to retrieve each building's observation dictionary is an expensive call especially since the observations \u001b[39;00m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;66;03m# are retrieved again to send to agent but the observations in dict form is needed for the reward function to easily\u001b[39;00m\n\u001b[1;32m    813\u001b[0m \u001b[38;5;66;03m# extract building-level values. Can't think of a better way to handle this without giving the reward direct access to\u001b[39;00m\n\u001b[1;32m    814\u001b[0m \u001b[38;5;66;03m# env, which is not the best design for competition integrity sake. Will revisit the building.observations() function\u001b[39;00m\n\u001b[1;32m    815\u001b[0m \u001b[38;5;66;03m# to see how it can be optimized.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/citylearn/building.py:1829\u001b[0m, in \u001b[0;36mDynamicsBuilding.apply_actions\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m   1828\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_actions\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m-> 1829\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_actions\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1830\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_dynamics_input()\n\u001b[1;32m   1832\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msimulate_dynamics:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/citylearn/building.py:911\u001b[0m, in \u001b[0;36mBuilding.apply_actions\u001b[0;34m(self, cooling_device_action, heating_device_action, cooling_storage_action, heating_storage_action, dhw_storage_action, electrical_storage_action)\u001b[0m\n\u001b[1;32m    908\u001b[0m func, args \u001b[38;5;241m=\u001b[39m actions[k]\n\u001b[1;32m    910\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 911\u001b[0m     \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    912\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m:\n\u001b[1;32m    913\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/citylearn/building.py:935\u001b[0m, in \u001b[0;36mBuilding.update_energy_from_cooling_device\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    933\u001b[0m electricity_consumption \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcooling_device\u001b[38;5;241m.\u001b[39mget_input_power(device_output, temperature, heating\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    934\u001b[0m \u001b[38;5;66;03m# print('timestep:', self.time_step, 'bldg:', self.name, 'demand:', demand, 'temperature:', temperature, 'storage_capacity:', self.cooling_storage.capacity, 'prev_soc:', self.cooling_storage.soc[self.time_step - 1], 'curr_soc:', self.cooling_storage.soc[self.time_step], 'storage_output:', storage_output, 'max_electric_power:', max_electric_power, 'max_device_output:', max_device_output, 'device_output:', device_output, 'consumption:', electricity_consumption)\u001b[39;00m\n\u001b[0;32m--> 935\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m___electricity_consumption_polarity_check\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcooling\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43melectricity_consumption\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    936\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcooling_device\u001b[38;5;241m.\u001b[39mupdate_electricity_consumption(\u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m0.0\u001b[39m, electricity_consumption))\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/citylearn/building.py:1086\u001b[0m, in \u001b[0;36mBuilding.___electricity_consumption_polarity_check\u001b[0;34m(self, end_use, device_output, electricity_consumption)\u001b[0m\n\u001b[1;32m   1084\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m___electricity_consumption_polarity_check\u001b[39m(\u001b[38;5;28mself\u001b[39m, end_use: \u001b[38;5;28mstr\u001b[39m, device_output: \u001b[38;5;28mfloat\u001b[39m, electricity_consumption: \u001b[38;5;28mfloat\u001b[39m):\n\u001b[1;32m   1085\u001b[0m         message \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimestep: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime_step\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, building: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, device_output: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice_output\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, electricity_consumption: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00melectricity_consumption\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m-> 1086\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m electricity_consumption \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mabs\u001b[39m(electricity_consumption) \u001b[38;5;241m<\u001b[39m TOLERANCE,\\\n\u001b[1;32m   1087\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnegative electricity consumption for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mend_use\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m demand | \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmessage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: negative electricity consumption for cooling demand | timestep: 14, building: Building_1, device_output: -9.344993591308594, electricity_consumption: -4.1051105764168625"
     ]
    }
   ],
   "source": [
    "logs_list = []\n",
    "\n",
    "for i in range(3):\n",
    "    # make agent\n",
    "    sac_agent = Agents(env, batch_size=100, buffer_max_size=100000)\n",
    "    \n",
    "    # training run\n",
    "    logs = sac_agent.train(nr_steps = 720*2, warmup_steps = 720, learn_delay = 100, learn_freq = 1, learn_weight = 1)\n",
    "    # display(logs)\n",
    "    \n",
    "    # save training logs for this run\n",
    "    logs_list.append(logs)\n",
    "\n",
    "# print(logs_list.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
