{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST of custom Multi-Agent SAC agent on citylearn env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from citylearn.citylearn import CityLearnEnv\n",
    "from citylearn.wrappers import NormalizedSpaceWrapper, StableBaselines3Wrapper\n",
    "\n",
    "from custom_agent.CTDE.ma_sac_agents import Agents\n",
    "\n",
    "from custom_reward.custom_reward import CustomReward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WrapperEnv:\n",
    "    \"\"\"\n",
    "    Env to wrap provide Citylearn Env data without providing full env\n",
    "    Preventing attribute access outside of the available functions\n",
    "    \"\"\"\n",
    "    def __init__(self, env_data):\n",
    "        self.observation_names = env_data['observation_names']\n",
    "        self.action_names = env_data['action_names']\n",
    "        self.observation_space = env_data['observation_space']\n",
    "        self.action_space = env_data['action_space']\n",
    "        self.time_steps = env_data['time_steps']\n",
    "        self.seconds_per_time_step = env_data['seconds_per_time_step']\n",
    "        self.random_seed = env_data['random_seed']\n",
    "        self.buildings_metadata = env_data['buildings_metadata']\n",
    "        self.episode_tracker = env_data['episode_tracker']\n",
    "    \n",
    "    def get_metadata(self):\n",
    "        return {'buildings': self.buildings_metadata}\n",
    "    \n",
    "def makeEnv(schema_path, reward_function):\n",
    "    # create environment\n",
    "    env = CityLearnEnv(schema = schema_path, reward_function = reward_function, central_agent=False)\n",
    "\n",
    "    env_data = dict(\n",
    "        observation_names = env.observation_names,\n",
    "        action_names = env.action_names,\n",
    "        observation_space = env.observation_space,\n",
    "        action_space = env.action_space,\n",
    "        time_steps = env.time_steps,\n",
    "        random_seed = None,\n",
    "        episode_tracker = None,\n",
    "        seconds_per_time_step = None,\n",
    "        buildings_metadata = env.get_metadata()['buildings']\n",
    "    )\n",
    "\n",
    "    wrapper_env = WrapperEnv(env_data)\n",
    "    return env, wrapper_env\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'month': False,\n",
       " 'day_type': True,\n",
       " 'hour': True,\n",
       " 'daylight_savings_status': False,\n",
       " 'outdoor_dry_bulb_temperature': True,\n",
       " 'outdoor_dry_bulb_temperature_predicted_6h': True,\n",
       " 'outdoor_dry_bulb_temperature_predicted_12h': True,\n",
       " 'outdoor_dry_bulb_temperature_predicted_24h': True,\n",
       " 'outdoor_relative_humidity': False,\n",
       " 'outdoor_relative_humidity_predicted_6h': False,\n",
       " 'outdoor_relative_humidity_predicted_12h': False,\n",
       " 'outdoor_relative_humidity_predicted_24h': False,\n",
       " 'diffuse_solar_irradiance': True,\n",
       " 'diffuse_solar_irradiance_predicted_6h': True,\n",
       " 'diffuse_solar_irradiance_predicted_12h': True,\n",
       " 'diffuse_solar_irradiance_predicted_24h': True,\n",
       " 'direct_solar_irradiance': True,\n",
       " 'direct_solar_irradiance_predicted_6h': True,\n",
       " 'direct_solar_irradiance_predicted_12h': True,\n",
       " 'direct_solar_irradiance_predicted_24h': True,\n",
       " 'carbon_intensity': True,\n",
       " 'indoor_dry_bulb_temperature': True,\n",
       " 'average_unmet_cooling_setpoint_difference': False,\n",
       " 'indoor_relative_humidity': False,\n",
       " 'non_shiftable_load': True,\n",
       " 'solar_generation': True,\n",
       " 'cooling_storage_soc': False,\n",
       " 'heating_storage_soc': False,\n",
       " 'dhw_storage_soc': True,\n",
       " 'electrical_storage_soc': True,\n",
       " 'net_electricity_consumption': True,\n",
       " 'electricity_pricing': True,\n",
       " 'electricity_pricing_predicted_6h': True,\n",
       " 'electricity_pricing_predicted_12h': True,\n",
       " 'electricity_pricing_predicted_24h': True,\n",
       " 'cooling_device_cop': False,\n",
       " 'heating_device_cop': False,\n",
       " 'cooling_demand': True,\n",
       " 'heating_demand': False,\n",
       " 'dhw_demand': True,\n",
       " 'cooling_electricity_consumption': False,\n",
       " 'heating_electricity_consumption': False,\n",
       " 'dhw_electricity_consumption': False,\n",
       " 'occupant_count': True,\n",
       " 'indoor_dry_bulb_temperature_set_point': True,\n",
       " 'indoor_dry_bulb_temperature_delta': False,\n",
       " 'power_outage': True}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "schema_path = \"data/schema.json\"\n",
    "\n",
    "env, wrapper_env = makeEnv(schema_path, CustomReward)\n",
    "\n",
    "display(env.buildings[0].observation_metadata)\n",
    "\n",
    "# wrap environment for a more workable env\n",
    "env = NormalizedSpaceWrapper(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hkolstee/.local/lib/python3.10/site-packages/gym/spaces/box.py:73: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(\n",
      "/home/hkolstee/.local/lib/python3.10/site-packages/torch/cuda/__init__.py:138: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11040). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n",
      "/home/hkolstee/.local/lib/python3.10/site-packages/gym/spaces/box.py:73: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "sac_agent = Agents(env, batch_size=100, buffer_max_size=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Box([0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
       "  0. 0. 0. 0. 0. 0. 0. 0.], [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       "  1. 1. 1. 1. 1. 1. 1. 1.], (32,), float32),\n",
       " Box([0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
       "  0. 0. 0. 0. 0. 0. 0. 0.], [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       "  1. 1. 1. 1. 1. 1. 1. 1.], (32,), float32),\n",
       " Box([0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
       "  0. 0. 0. 0. 0. 0. 0. 0.], [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       "  1. 1. 1. 1. 1. 1. 1. 1.], (32,), float32)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/36000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Episode 1 mean reward: [-1.08041309 -0.96390594 -0.98829467]] ~ :   2%|â–         | 786/36000 [01:17<57:34, 10.19it/s]  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m sac_agent \u001b[38;5;241m=\u001b[39m Agents(env, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, buffer_max_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100000\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# training run\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m logs \u001b[38;5;241m=\u001b[39m \u001b[43msac_agent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnr_steps\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m720\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarmup_steps\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mwarmup_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearn_delay\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearn_freq\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearn_weight\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# save training logs for this run\u001b[39;00m\n\u001b[1;32m     12\u001b[0m logs_list\u001b[38;5;241m.\u001b[39mappend(logs)\n",
      "File \u001b[0;32m~/masterThesis/masterThesis/custom_agent/CTDE/ma_sac_agents.py:392\u001b[0m, in \u001b[0;36mAgents.train\u001b[0;34m(self, nr_steps, max_episode_len, warmup_steps, learn_delay, learn_freq, learn_weight)\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m step \u001b[38;5;241m>\u001b[39m learn_delay \u001b[38;5;129;01mand\u001b[39;00m step \u001b[38;5;241m%\u001b[39m learn_freq \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    390\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(learn_weight):\n\u001b[1;32m    391\u001b[0m         \u001b[38;5;66;03m# learning step\u001b[39;00m\n\u001b[0;32m--> 392\u001b[0m         loss_actor, loss_critic, policy_entropy, alpha, loss_alpha \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    394\u001b[0m         \u001b[38;5;66;03m# keep track for logs\u001b[39;00m\n\u001b[1;32m    395\u001b[0m         ep_learn_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/masterThesis/masterThesis/custom_agent/CTDE/ma_sac_agents.py:256\u001b[0m, in \u001b[0;36mAgents.learn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[38;5;66;03m# critic2\u001b[39;00m\n\u001b[1;32m    255\u001b[0m         p2_targ\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolyak\n\u001b[0;32m--> 256\u001b[0m         p2_targ\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m ((\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolyak) \u001b[38;5;241m*\u001b[39m p2\u001b[38;5;241m.\u001b[39mdata)\n\u001b[1;32m    258\u001b[0m \u001b[38;5;66;03m# log each agent's values\u001b[39;00m\n\u001b[1;32m    259\u001b[0m loss_policy_list\u001b[38;5;241m.\u001b[39mappend(loss_policy\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy())\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "logs_list = []\n",
    "warmup_steps = 5000\n",
    "\n",
    "for i in range(3):\n",
    "    # make agent\n",
    "    sac_agent = Agents(env, batch_size=100, buffer_max_size=100000)\n",
    "    \n",
    "    # training run\n",
    "    logs = sac_agent.train(nr_steps = 50 * 720, warmup_steps = warmup_steps, learn_delay = 100, learn_freq = 1, learn_weight = 1)\n",
    "    \n",
    "    # save training logs for this run\n",
    "    logs_list.append(logs)\n",
    "\n",
    "# print(logs_list.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
